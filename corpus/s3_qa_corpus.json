[
  {
    "question": "What features does Amazon S3 offer to gain visibility into storage usage?",
    "answer": "Amazon S3 offers Amazon S3 Storage Lens, Storage Class Analysis, and S3 Inventory with Inventory reports."
  },
  {
    "question": "What is S3 Storage Lens?",
    "answer": "S3 Storage Lens provides over 60 usage and activity metrics and interactive dashboards to aggregate data for an entire organization, specific accounts, AWS Regions, buckets, or prefixes."
  },
  {
    "question": "What is the purpose of Storage Class Analysis?",
    "answer": "Storage Class Analysis is used to analyze storage access patterns to determine when to move data to a more cost-effective storage class."
  },
  {
    "question": "What are S3 Inventory reports used for?",
    "answer": "S3 Inventory reports are used to audit and report on objects and their corresponding metadata, and to configure other Amazon S3 features to take action."
  },
  {
    "question": "What kind of consistency model does Amazon S3 provide for PUT and DELETE requests of objects?",
    "answer": "Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in all AWS Regions."
  },
  {
    "question": "What is an object in Amazon S3?",
    "answer": "An object is a file and any metadata that describes the file."
  },
  {
    "question": "What is a bucket in Amazon S3?",
    "answer": "A bucket is a container for objects."
  },
  {
    "question": "How do you store data in Amazon S3?",
    "answer": "To store data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region, then upload your data as objects to that bucket."
  },
  {
    "question": "What is an object's key?",
    "answer": "An object's key (or key name) is the unique identifier for the object within the bucket."
  },
  {
    "question": "What is S3 Versioning used for?",
    "answer": "S3 Versioning is used to keep multiple versions of an object in the same bucket, allowing for restoration of accidentally deleted or overwritten objects."
  },
  {
    "question": "How can access permissions be managed for buckets and objects in Amazon S3?",
    "answer": "Access permissions can be managed using bucket policies, AWS Identity and Access Management (IAM) policies, access control lists (ACLs), and S3 Access Points."
  },
  {
    "question": "What are the three types of buckets Amazon S3 supports?",
    "answer": "Amazon S3 supports general purpose buckets, directory buckets, and table buckets."
  },
  {
    "question": "What are general purpose buckets recommended for?",
    "answer": "General purpose buckets are recommended for most use cases and access patterns."
  },
  {
    "question": "Are general purpose buckets private by default?",
    "answer": "Yes, by default, all general purpose buckets are private."
  },
  {
    "question": "What are directory buckets recommended for?",
    "answer": "Directory buckets are recommended for low-latency use cases and data-residency use cases."
  },
  {
    "question": "Can directory buckets have public access?",
    "answer": "No, directory buckets have all public access disabled by default, and this behavior cannot be changed."
  },
  {
    "question": "What are table buckets recommended for?",
    "answer": "Table buckets are recommended for storing tabular data, such as daily purchase transactions, streaming sensor data, or ad impressions."
  },
  {
    "question": "Can table buckets be made public?",
    "answer": "No, all table buckets and tables are private and cannot be made public."
  },
  {
    "question": "What happens after a bucket is created regarding its name and Region?",
    "answer": "After a bucket is created, its name or Region cannot be changed."
  },
  {
    "question": "What do objects in Amazon S3 consist of?",
    "answer": "Objects consist of object data and metadata."
  },
  {
    "question": "How is an object uniquely identified within a bucket?",
    "answer": "An object is uniquely identified within a bucket by a key (name) and a version ID (if S3 Versioning is enabled)."
  },
  {
    "question": "What is a version ID in Amazon S3?",
    "answer": "When S3 Versioning is enabled in a bucket, Amazon S3 generates a unique version ID for each object added to the bucket."
  },
  {
    "question": "What is a bucket policy?",
    "answer": "A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that grants access permissions to a bucket and its objects."
  },
  {
    "question": "What are Amazon S3 Access Points?",
    "answer": "Amazon S3 Access Points are named network endpoints with dedicated access policies that describe how data can be accessed using that endpoint."
  },
  {
    "question": "What are Access Control Lists (ACLs) used for in Amazon S3?",
    "answer": "ACLs are used to grant read and write permissions to authorized users for individual general purpose buckets and objects."
  },
  {
    "question": "What is the recommended setting for ACLs in modern Amazon S3 use cases?",
    "answer": "It is recommended to keep ACLs disabled, as most modern use cases no longer require them."
  },
  {
    "question": "Why would you choose a specific geographical AWS Region for storing buckets?",
    "answer": "You might choose a Region to optimize latency, minimize costs, or address regulatory requirements."
  },
  {
    "question": "Do objects stored in an AWS Region ever leave that Region automatically?",
    "answer": "No, objects stored in an AWS Region never leave the Region unless explicitly transferred or replicated to another Region."
  },
  {
    "question": "What happens if two PUT requests are simultaneously made to the same key in Amazon S3?",
    "answer": "Amazon S3 internally uses last-writer-wins semantics, meaning the request with the latest timestamp wins."
  },
  {
    "question": "What is the consistency model for bucket configurations?",
    "answer": "Bucket configurations have an eventual consistency model."
  },
  {
    "question": "What AWS services can be used with data loaded into Amazon S3?",
    "answer": "You can use Amazon S3 with services like Amazon Elastic Compute Cloud (Amazon EC2), Amazon EMR, AWS Snow Family, and AWS Transfer Family."
  },
  {
    "question": "How do you enable S3 Transfer Acceleration on a bucket using the AWS CLI?",
    "answer": "Use the command: aws s3api put-bucket-accelerate-configuration --bucket BUCKET_NAME --accelerate-configuration Status=Enabled."
  },
  {
    "question": "Which command do you use to suspend Transfer Acceleration on a bucket?",
    "answer": "Set Status=Suspended using the aws s3api put-bucket-accelerate-configuration command."
  },
  {
    "question": "What is the AWS CLI configuration value that directs all S3 requests to the accelerate endpoint?",
    "answer": "The configuration value is use_accelerate_endpoint and should be set to true."
  },
  {
    "question": "What is the accelerate endpoint for Amazon S3?",
    "answer": "The endpoint is s3-accelerate.amazonaws.com."
  },
  {
    "question": "Which requests are not supported by the accelerate endpoint?",
    "answer": "ListBuckets, CreateBucket, and DeleteBucket requests are not supported by the accelerate endpoint."
  },
  {
    "question": "How can you enable the accelerate endpoint for specific AWS CLI commands?",
    "answer": "You can set --endpoint-url to https://s3-accelerate.amazonaws.com for individual commands or use different AWS CLI profiles with use_accelerate_endpoint set accordingly."
  },
  {
    "question": "How do you upload an object to a Transfer Acceleration-enabled bucket using the AWS CLI and accelerate endpoint?",
    "answer": "Run aws s3 cp file.txt s3://bucket-name/key-name --region region --endpoint-url https://s3-accelerate.amazonaws.com."
  },
  {
    "question": "When using the AWS SDK for Java, how do you configure a client to use acceleration?",
    "answer": "Use AmazonS3ClientBuilder with enableAccelerateMode, then set and verify the bucket's accelerate configuration before uploading an object."
  },
  {
    "question": "What steps are performed in the Java SDK example for using transfer acceleration?",
    "answer": "Create an accelerate-enabled S3 client, enable acceleration on the bucket, verify acceleration is enabled, then upload an object using the accelerate endpoint."
  },
  {
    "question": "How do you enable Transfer Acceleration on a bucket using the AWS SDK for .NET?",
    "answer": "Set BucketAccelerateStatus.Enabled with PutBucketAccelerateConfigurationRequest and verify with GetBucketAccelerateConfigurationAsync."
  },
  {
    "question": "How do you configure a .NET SDK client to use the accelerate endpoint for uploads?",
    "answer": "Set UseAccelerateEndpoint = true in the AmazonS3Config when instantiating the AmazonS3Client."
  },
  {
    "question": "Which SDK methods should you use to enable Transfer Acceleration in JavaScript and Python?",
    "answer": "Use PutBucketAccelerateConfiguration in JavaScript and put_bucket_accelerate_configuration in Python (Boto3)."
  },
  {
    "question": "What REST API operation is used to enable accelerate configuration on an existing bucket?",
    "answer": "Use PutBucketAccelerateConfiguration."
  },
  {
    "question": "What tool helps compare accelerated and non-accelerated upload speeds to different S3 regions?",
    "answer": "The Amazon S3 Transfer Acceleration Speed Comparison tool."
  },
  {
    "question": "How can you access the Amazon S3 Transfer Acceleration Speed Comparison tool in a browser?",
    "answer": "By using the URL https://s3-accelerate-speedtest.s3-accelerate.amazonaws.com/en/accelerate-speed-comparsion.html?region=region&origBucketName=bucket."
  },
  {
    "question": "Who pays the data transfer and request costs in a Requester Pays bucket?",
    "answer": "The requester pays the request and data download costs; the bucket owner pays storage costs."
  },
  {
    "question": "What is a typical use case for Requester Pays buckets?",
    "answer": "Sharing large datasets publicly without incurring download costs as the bucket owner."
  },
  {
    "question": "Is anonymous access allowed on Requester Pays buckets?",
    "answer": "No, anonymous access is not allowed; all requests must be authenticated."
  },
  {
    "question": "How must requesters confirm they accept Requester Pays charges when accessing a bucket?",
    "answer": "By including the x-amz-request-payer header in API requests or the --request-payer parameter in CLI requests."
  },
  {
    "question": "What does a Requester Pays bucket owner always pay for?",
    "answer": "The bucket owner always pays for the cost of storing data."
  },
  {
    "question": "What types of requests are NOT supported by Requester Pays buckets?",
    "answer": "Anonymous requests, SOAP requests, and end-user logging with a Requester Pays bucket as the target."
  },
  {
    "question": "Under what conditions is the bucket owner charged for a request in a Requester Pays bucket?",
    "answer": "If AccessDenied (403) is returned and the request originates within the owner’s AWS account or organization, or if it is a SOAP request."
  },
  {
    "question": "How do you enable Requester Pays for a bucket using the S3 console?",
    "answer": "In Properties > Requester pays, choose Edit, select Enable, then Save changes."
  },
  {
    "question": "What REST API request enables Requester Pays for a bucket?",
    "answer": "A PUT ?requestPayment HTTP/1.1 request with a <Payer>Requester</Payer> XML body."
  },
  {
    "question": "Can you configure Requester Pays at the object level?",
    "answer": "No, Requester Pays can be set only at the bucket level, not at the object level."
  },
  {
    "question": "Can you switch a bucket between Requester Pays and BucketOwner payer modes at any time?",
    "answer": "Yes, you can change the payer mode at any time, but it may take a few minutes for changes to take effect."
  },
  {
    "question": "What must you do to delete an object in a Requester Pays bucket using the API?",
    "answer": "Include the x-amz-request-payer: RequestPayer header in the DeleteObjectVersion request."
  },
  {
    "question": "When restoring objects from Requester Pays buckets, who pays for retrieval charges?",
    "answer": "The bucket owner pays for retrieval charges, but the requester pays for the request cost."
  },
  {
    "question": "What should bucket owners consider before enabling Requester Pays on a bucket when sharing presigned URLs?",
    "answer": "They should consider that the bucket owner is charged each time a requester uses a presigned URL with the owner's credentials, particularly if the URL has a long lifetime."
  },
  {
    "question": "How can you retrieve the RequestPayment configuration for a bucket using the REST API?",
    "answer": "Make a GET request to ?requestPayment for the bucket endpoint. The response includes the Payer value, which can be 'Requester' or 'BucketOwner'."
  },
  {
    "question": "How do you know from an API response if Requester Pays is enabled?",
    "answer": "You will receive an XML body with <Payer>Requester</Payer> in the RequestPaymentConfiguration response."
  },
  {
    "question": "What HTTP header or parameter must be included to access objects from Requester Pays buckets?",
    "answer": "The x-amz-request-payer header with the value 'requester' for most API calls, or x-amz-request-payer=requester in the presigned URL/query string."
  },
  {
    "question": "What happens if you access a Requester Pays bucket without including x-amz-request-payer?",
    "answer": "Amazon S3 returns a 403 error and charges the bucket owner for the request."
  },
  {
    "question": "Does the bucket owner need to include x-amz-request-payer to access their own bucket?",
    "answer": "No, the bucket owner does not need to include x-amz-request-payer in their requests."
  },
  {
    "question": "What additional step is needed in signature calculation when using x-amz-request-payer?",
    "answer": "The x-amz-request-payer parameter and value must be included in the request signature calculation."
  },
  {
    "question": "How do you download an object from a Requester Pays bucket using the REST API?",
    "answer": "Use a GET request to the object's URI with the x-amz-request-payer: requester header."
  },
  {
    "question": "If an object download succeeds from a Requester Pays bucket, which header indicates the requester was charged?",
    "answer": "The response will include the header x-amz-request-charged: requester."
  },
  {
    "question": "What error might you receive when accessing a Requester Pays bucket and what does it indicate?",
    "answer": "You may receive an Access Denied error, indicating a permissions issue, not just billing."
  },
  {
    "question": "How do you download objects from a Requester Pays bucket using the AWS CLI?",
    "answer": "Use the --request-payer requester argument as part of your get-object request."
  },
  {
    "question": "What is the basic S3 data model for organizing information?",
    "answer": "Buckets serve as containers for objects, and objects consist of data (value) and metadata."
  },
  {
    "question": "What is an S3 object and what are its main components?",
    "answer": "An S3 object is a file and associated metadata stored in a bucket. Its key components are key (name), version ID (if versioned), value (data), and metadata."
  },
  {
    "question": "What is the maximum size of an individual S3 object?",
    "answer": "5 TB."
  },
  {
    "question": "What is an S3 object key?",
    "answer": "It is the unique name you assign to an object within a bucket; it is used to retrieve the object."
  },
  {
    "question": "What is S3 object versioning and when does a version ID exist?",
    "answer": "When versioning is enabled on a bucket, each object has a unique version ID along with its key, allowing multiple versions of the same key."
  },
  {
    "question": "What kinds of metadata does Amazon S3 support for objects?",
    "answer": "S3 supports user-defined metadata (set by the user) and system-metadata (used by S3 for object management)."
  },
  {
    "question": "What is an S3 object subresource?",
    "answer": "A subresource is additional information subordinate to an object, such as an ACL, associated with an object or bucket."
  },
  {
    "question": "What types of access control does Amazon S3 support for objects?",
    "answer": "Both resource-based (such as ACLs and bucket policies) and user-based control."
  },
  {
    "question": "How are S3 resources (buckets and objects) protected by default?",
    "answer": "They are private by default and require explicit permission grants for access."
  },
  {
    "question": "What are object tags used for in Amazon S3?",
    "answer": "Tags can be used for categorizing objects, controlling access, and cost allocation."
  },
  {
    "question": "What is the function of the 'acl' subresource for an S3 object?",
    "answer": "It lists grants (grantees and their permissions). The object creator/owner has full control by default and can update the ACL by replacing it."
  },
  {
    "question": "How long can an S3 object key name be?",
    "answer": "Up to 1,024 bytes."
  },
  {
    "question": "Are S3 object key names case sensitive?",
    "answer": "Yes, object key names are case sensitive."
  },
  {
    "question": "How does the S3 console display logical hierarchy for objects?",
    "answer": "By using prefixes and delimiters in key names (e.g., 'folder/object'), the console and SDKs can present a folder structure, though the underlying data model is flat."
  },
  {
    "question": "How is a folder created in the S3 console?",
    "answer": "By creating a zero-byte object with the folder prefix and delimiter as the key; these folder objects are not visible but behave like regular objects."
  },
  {
    "question": "Which special restriction applies to object key names containing the string 'soap'?",
    "answer": "Keys with the value 'soap' are not supported for virtual-hosted-style requests and must use path-style URLs instead."
  },
  {
    "question": "What characters are generally safe for use in S3 object key names?",
    "answer": "Alphanumerics (0-9, a-z, A-Z), and special characters such as !, -, _, ., *, ', (, )"
  },
  {
    "question": "Give three examples of valid S3 object key names.",
    "answer": "Examples: '4my-organization', 'my.great_photos-2014/jan/myvacation.jpg', 'videos/2014/birthday/video1.wmv'"
  },
  {
    "question": "What is the effect of a key name ending with periods (.) when downloaded via the S3 console?",
    "answer": "The periods are removed from the ends of key names; to preserve them, use AWS CLI, SDKs, or REST API."
  },
  {
    "question": "What naming limitation applies to objects with a prefix of './'?",
    "answer": "Such objects must be uploaded/downloaded using AWS CLI, SDKs, or REST API; the S3 console does not support them."
  },
  {
    "question": "What is the rule for valid use of relative path elements (e.g., '../') in key names?",
    "answer": "A key name with relative paths is valid if, when parsed left-to-right, the cumulative count of relative segments does not exceed the count of non-relative segments."
  },
  {
    "question": "Name three characters that may require special code handling or URL encoding in S3 key names.",
    "answer": "Examples: Ampersand (&), dollar sign ($), at symbol (@)."
  },
  {
    "question": "Why should you avoid using backslash (\\), braces ({ }), non-printable ASCII chars (128–255), caret (^), or percent (%) in key names?",
    "answer": "Because handling of these characters is not consistent across applications, and can lead to errors."
  },
  {
    "question": "Which characters must be replaced with XML entity codes when sending S3 object keys in XML requests?",
    "answer": "Characters like apostrophe ('), quotation mark (\") , ampersand (&), less than (<), greater than (>), carriage return (\\r), and newline (\\n) must be replaced with &apos;, &quot;, &amp;, &lt;, &gt;, &#13; or &#x0D;, and &#10; or &#x0A;, respectively."
  },
  {
    "question": "What does S3 Metadata provide by default?",
    "answer": "S3 Metadata provides system-defined object metadata such as creation time and storage class, as well as custom metadata such as tags and user-defined metadata."
  },
  {
    "question": "How does S3 Metadata accelerate data discovery?",
    "answer": "It automatically captures metadata and stores it in read-only Apache Iceberg tables, which are automatically refreshed as objects are added, changed, or deleted."
  },
  {
    "question": "Where are S3 Metadata tables stored and how can they be queried?",
    "answer": "They are stored in S3 table buckets, and you can query them using analytics services like Amazon Athena, Redshift, or QuickSight."
  },
  {
    "question": "Name the two categories of system-defined metadata in Amazon S3.",
    "answer": "System controlled and user controlled."
  },
  {
    "question": "Give an example of user-controlled system metadata.",
    "answer": "Storage class of the object or whether server-side encryption is enabled."
  },
  {
    "question": "What is the limit on the size of system-defined metadata in a PUT request header?",
    "answer": "2 KB within the PUT request header, and the header itself is limited to 8 KB."
  },
  {
    "question": "Which system-defined metadata fields can a user NOT modify?",
    "answer": "Date, Content-Length, Last-Modified, and ETag."
  },
  {
    "question": "According to the table on pages 4 and 5, which key enables you to control request redirection for an S3 object?",
    "answer": "The header x-amz-website-redirect-location."
  },
  {
    "question": "When using the S3 REST API, what prefix must user-defined metadata names begin with?",
    "answer": "They must begin with x-amz-meta-."
  },
  {
    "question": "How does Amazon S3 handle user-defined metadata keys?",
    "answer": "It stores them in lowercase."
  },
  {
    "question": "Are arbitrary Unicode characters supported in S3 user-defined metadata values?",
    "answer": "Yes, but using US-ASCII for REST and UTF-8 for SOAP/POST is recommended to avoid presentation issues."
  },
  {
    "question": "What happens to non-ASCII user metadata values in REST when retrieved?",
    "answer": "They are encoded and decoded as per RFC 2047 to ensure mail-safety."
  },
  {
    "question": "What does the x-amz-missing-meta header indicate in REST API responses?",
    "answer": "It shows how many metadata entries contained unprintable characters and were not returned."
  },
  {
    "question": "How can you edit metadata for an existing S3 object in the console?",
    "answer": "Use the Copy action to copy the object to itself, specifying new metadata to replace the old."
  },
  {
    "question": "If you change an object’s metadata and bucket versioning is enabled, what happens?",
    "answer": "A new version of the object is created with the updated metadata; the old version remains."
  },
  {
    "question": "What is the minimum size limit for using S3 multipart upload to replace object metadata?",
    "answer": "Objects greater than 5 GB require multipart upload for this operation."
  },
  {
    "question": "What metadata cannot be modified after upload in S3?",
    "answer": "System-defined metadata like Content-Length and Last-Modified."
  },
  {
    "question": "Which objects cannot be copied using the console when replacing metadata?",
    "answer": "Objects encrypted with customer-provided encryption keys (SSE-C)."
  },
  {
    "question": "What is a key consideration when replacing metadata for a folder in Amazon S3?",
    "answer": "Wait for the Copy action to finish before adding new objects to avoid unintended edits."
  },
  {
    "question": "When copying objects with the console, what could cause the error 'Copied metadata can't be verified'?",
    "answer": "If your network or browser modifies headers, unintended metadata (like changed Cache-Control) may be written."
  },
  {
    "question": "Where do you specify system-defined metadata when replacing it via the S3 console?",
    "answer": "In the Metadata section, after choosing 'Replace all metadata' and setting Type to 'System-defined.'"
  },
  {
    "question": "What is the process for replacing system-defined metadata on an object in S3 via the console?",
    "answer": "Select objects, choose Copy, set destination, enable versioning if desired, specify metadata to replace, and confirm the copy."
  },
  {
    "question": "Which characters must be replaced with XML entity codes when sending S3 object keys in XML requests?",
    "answer": "Characters like apostrophe ('), quotation mark (\") , ampersand (&), less than (<), greater than (>), carriage return (\\r), and newline (\\n) must be replaced with &apos;, &quot;, &amp;, &lt;, &gt;, &#13; or &#x0D;, and &#10; or &#x0A;, respectively."
  },
  {
    "question": "What does S3 Metadata provide by default?",
    "answer": "S3 Metadata provides system-defined object metadata such as creation time and storage class, as well as custom metadata such as tags and user-defined metadata."
  },
  {
    "question": "How does S3 Metadata accelerate data discovery?",
    "answer": "It automatically captures metadata and stores it in read-only Apache Iceberg tables, which are automatically refreshed as objects are added, changed, or deleted."
  },
  {
    "question": "Where are S3 Metadata tables stored and how can they be queried?",
    "answer": "They are stored in S3 table buckets, and you can query them using analytics services like Amazon Athena, Redshift, or QuickSight."
  },
  {
    "question": "Name the two categories of system-defined metadata in Amazon S3.",
    "answer": "System controlled and user controlled."
  },
  {
    "question": "Give an example of user-controlled system metadata.",
    "answer": "Storage class of the object or whether server-side encryption is enabled."
  },
  {
    "question": "What is the limit on the size of system-defined metadata in a PUT request header?",
    "answer": "2 KB within the PUT request header, and the header itself is limited to 8 KB."
  },
  {
    "question": "Which system-defined metadata fields can a user NOT modify?",
    "answer": "Date, Content-Length, Last-Modified, and ETag."
  },
  {
    "question": "According to the table on pages 4 and 5, which key enables you to control request redirection for an S3 object?",
    "answer": "The header x-amz-website-redirect-location."
  },
  {
    "question": "When using the S3 REST API, what prefix must user-defined metadata names begin with?",
    "answer": "They must begin with x-amz-meta-."
  },
  {
    "question": "How does Amazon S3 handle user-defined metadata keys?",
    "answer": "It stores them in lowercase."
  },
  {
    "question": "Are arbitrary Unicode characters supported in S3 user-defined metadata values?",
    "answer": "Yes, but using US-ASCII for REST and UTF-8 for SOAP/POST is recommended to avoid presentation issues."
  },
  {
    "question": "What happens to non-ASCII user metadata values in REST when retrieved?",
    "answer": "They are encoded and decoded as per RFC 2047 to ensure mail-safety."
  },
  {
    "question": "What does the x-amz-missing-meta header indicate in REST API responses?",
    "answer": "It shows how many metadata entries contained unprintable characters and were not returned."
  },
  {
    "question": "How can you edit metadata for an existing S3 object in the console?",
    "answer": "Use the Copy action to copy the object to itself, specifying new metadata to replace the old."
  },
  {
    "question": "If you change an object’s metadata and bucket versioning is enabled, what happens?",
    "answer": "A new version of the object is created with the updated metadata; the old version remains."
  },
  {
    "question": "What is the minimum size limit for using S3 multipart upload to replace object metadata?",
    "answer": "Objects greater than 5 GB require multipart upload for this operation."
  },
  {
    "question": "What metadata cannot be modified after upload in S3?",
    "answer": "System-defined metadata like Content-Length and Last-Modified."
  },
  {
    "question": "Which objects cannot be copied using the console when replacing metadata?",
    "answer": "Objects encrypted with customer-provided encryption keys (SSE-C)."
  },
  {
    "question": "What is a key consideration when replacing metadata for a folder in Amazon S3?",
    "answer": "Wait for the Copy action to finish before adding new objects to avoid unintended edits."
  },
  {
    "question": "When copying objects with the console, what could cause the error 'Copied metadata can't be verified'?",
    "answer": "If your network or browser modifies headers, unintended metadata (like changed Cache-Control) may be written."
  },
  {
    "question": "Where do you specify system-defined metadata when replacing it via the S3 console?",
    "answer": "In the Metadata section, after choosing 'Replace all metadata' and setting Type to 'System-defined.'"
  },
  {
    "question": "What is the process for replacing system-defined metadata on an object in S3 via the console?",
    "answer": "Select objects, choose Copy, set destination, enable versioning if desired, specify metadata to replace, and confirm the copy."
  },
  {
    "question": "What does the 'version_id' column represent in Amazon S3 metadata?",
    "answer": "The 'version_id' column contains the object's version ID. When versioning is enabled on a bucket, Amazon S3 assigns a version number to each object added to the bucket. Objects stored in the bucket before versioning is enabled have a version ID of null."
  },
  {
    "question": "Is the 'version_id' column required for S3 metadata, and what data type is it?",
    "answer": "No, the 'version_id' column is not required. Its data type is String."
  },
  {
    "question": "What does the 'is_delete_marker' column indicate about an object in Amazon S3?",
    "answer": "The 'is_delete_marker' column indicates the delete marker status of the object. If the object is a delete marker, the value is True. Otherwise, it's False."
  },
  {
    "question": "What is special about rows added for delete markers in S3 metadata tables?",
    "answer": "Rows added for delete markers have a record_type value of DELETE, not UPDATE_METADATA. If the delete marker is created as the result of an S3 Lifecycle expiration, the requester value is s3.amazonaws.com."
  },
  {
    "question": "What does the 'size' column show in S3 metadata tables?",
    "answer": "The 'size' column shows the object's size in bytes, not including the size of incomplete multipart uploads or object metadata. If 'is_delete_marker' is True, the size is 0."
  },
  {
    "question": "What is the 'last_modified_date' column in S3 metadata and its data type?",
    "answer": "The 'last_modified_date' column shows the object creation date or the last modified date, whichever is latest. The data type is Timestamp NTZ (no time zone). For multipart uploads, it shows the date when the upload is initiated."
  },
  {
    "question": "What does the 'e_tag' column store in S3 metadata?",
    "answer": "The 'e_tag' column stores the entity tag (ETag), which is a hash of the object’s content. The ETag reflects changes only to the contents, not the metadata. The ETag can be an MD5 digest, depending on upload and encryption method."
  },
  {
    "question": "What does the 'storage_class' column specify and what are example values?",
    "answer": "The 'storage_class' column specifies the storage class used for storing the object. Example values include STANDARD, REDUCED_REDUNDANCY, STANDARD_IA, ONEZONE_IA, INTELLIGENT_TIERING, GLACIER, DEEP_ARCHIVE, and GLACIER_IR."
  },
  {
    "question": "What does the 'is_multipart' column in S3 metadata reflect?",
    "answer": "The 'is_multipart' column reflects the object's upload type. If the object was uploaded as a multipart upload, this value is True; otherwise, it is False."
  },
  {
    "question": "What types of server-side encryption does the 'encryption_status' column reflect?",
    "answer": "The 'encryption_status' column reflects the object's server-side encryption status, such as SSE-S3, SSE-KMS, DSSE-KMS, SSE-C, or null if the object is unencrypted."
  },
  {
    "question": "What does the 'is_bucket_key_enabled' column indicate?",
    "answer": "The 'is_bucket_key_enabled' column indicates whether the object uses an S3 Bucket Key for SSE-KMS encryption. If enabled, the value is True; otherwise, it's False."
  },
  {
    "question": "What does the 'kms_key_arn' column store in S3 metadata?",
    "answer": "The 'kms_key_arn' column stores the Amazon Resource Name (ARN) for the KMS key with which the object is encrypted, for rows where 'encryption_status' is SSE-KMS or DSSE-KMS. If not encrypted with these methods, the value is null."
  },
  {
    "question": "In what case will the 'kms_key_arn' column have a null value even if 'encryption_status' is SSE-KMS or DSSE-KMS?",
    "answer": "If a row represents an object version that no longer existed when a delete or overwrite event was processed, 'kms_key_arn' contains a null value even if 'encryption_status' is SSE-KMS or DSSE-KMS."
  },
  {
    "question": "What is the purpose of the 'checksum_algorithm' column in S3 metadata tables?",
    "answer": "The 'checksum_algorithm' column specifies the algorithm used to create the checksum for the object. Possible values are CRC64NVME, CRC32, CRC32C, SHA1, SHA256, or null if no checksum is present."
  },
  {
    "question": "What does the 'object_tags' column in S3 metadata tables store?",
    "answer": "The 'object_tags' column stores the object tags as a map of key-value pairs associated with the object. If the object has no tags, an empty map ({}) is stored."
  },
  {
    "question": "What happens to the 'object_tags' column when the record_type value is DELETE?",
    "answer": "When the record_type value is DELETE, the 'object_tags' column contains a null value."
  },
  {
    "question": "What data type does the 'object_tags' column use?",
    "answer": "The 'object_tags' column uses the data type Map<String, String>."
  },
  {
    "question": "Which section provides more information about handling delete markers in S3?",
    "answer": "More information can be found in the section called 'Working with delete markers.'"
  },
  {
    "question": "Which section should you refer to for details about storage classes in Amazon S3?",
    "answer": "You should refer to the section called 'Understanding and managing storage classes.'"
  },
  {
    "question": "Where can you learn more about system-defined object metadata in Amazon S3?",
    "answer": "More information is available in the section called 'System-defined object metadata.'"
  },
  {
    "question": "Where is additional information provided for the 'e_tag' column?",
    "answer": "For more information about the ETag, see the 'Object in the Amazon S3 API Reference.'"
  },
  {
    "question": "What is the default value of 'size' if the object is a delete marker?",
    "answer": "The value is 0 if 'is_delete_marker' is True."
  },
  {
    "question": "What is the impact of incomplete multipart uploads on the 'size' column value?",
    "answer": "Incomplete multipart uploads do not contribute to the size value in the 'size' column."
  },
  {
    "question": "What happens to the object_tags column when an S3 object is deleted or overwritten?",
    "answer": "If an object is deleted or overwritten, and the version no longer exists at the time the event is processed, the object_tags column will contain a null value."
  },
  {
    "question": "What is stored in the user_metadata column of S3 metadata tables?",
    "answer": "The user_metadata column stores user metadata associated with the object as a map of key-value pairs. If there is no user metadata, an empty map ({}) is stored."
  },
  {
    "question": "When does the user_metadata column contain a null value?",
    "answer": "The user_metadata column contains a null value if the record_type is DELETE, or if the object version no longer exists when a delete or overwrite event is processed."
  },
  {
    "question": "What information does the requester column in S3 metadata contain?",
    "answer": "The requester column contains the AWS account ID of the requester or the AWS service principal that made the request."
  },
  {
    "question": "Under what circumstances is the source_ip_address column null in S3 metadata?",
    "answer": "The source_ip_address column is null for actions taken by Amazon S3 or another AWS service on behalf of the user. It contains the actual IP address only for user-generated requests."
  },
  {
    "question": "What does the request_id column represent in S3 metadata?",
    "answer": "The request_id column represents the unique request ID associated with the S3 operation."
  },
  {
    "question": "How does Amazon S3 Metadata accelerate data discovery?",
    "answer": "Amazon S3 Metadata accelerates data discovery by automatically capturing object metadata and storing it in read-only, fully managed Apache Iceberg tables that can be queried."
  },
  {
    "question": "What are metadata tables in Amazon S3?",
    "answer": "Metadata tables are read-only tables managed by S3, storing metadata about objects in general purpose buckets, and are always kept updated as objects are added, updated, or deleted."
  },
  {
    "question": "For what purpose can you use S3 Metadata?",
    "answer": "S3 Metadata allows you to quickly find, store, and query metadata for S3 objects, facilitating analytics, business intelligence, and AI/ML model training."
  },
  {
    "question": "What is required to create a metadata table configuration in S3?",
    "answer": "To create a metadata table configuration, you need the necessary IAM permissions and a chosen S3 table bucket in the same AWS Region and account as your general purpose bucket."
  },
  {
    "question": "How can administrators monitor configuration changes to S3 metadata tables?",
    "answer": "Administrators can use AWS CloudTrail to monitor changes to metadata table configurations."
  },
  {
    "question": "Which AWS permissions are necessary to configure S3 metadata tables?",
    "answer": "Necessary permissions include s3:CreateBucketMetadataTableConfiguration, s3:GetBucketMetadataTableConfiguration, s3:DeleteBucketMetadataTableConfiguration, and several s3tables permissions like CreateNamespace, GetTable, CreateTable, and PutTablePolicy."
  },
  {
    "question": "What extra permissions are needed for integrating S3 metadata tables with analytics services?",
    "answer": "Additional permissions are required to integrate the table bucket with AWS analytics services. The specifics are detailed in the S3 documentation for analytics integration."
  },
  {
    "question": "Which permissions are needed to use server-side encryption with AWS KMS keys on S3 tables?",
    "answer": "You need s3tables:PutTableEncryption, s3tables:PutTableBucketEncryption, and kms:DescribeKey for the IAM user or role, as well as kms:GenerateDataKey, kms:Decrypt, and kms:DescribeKey on the KMS key's resource policy."
  },
  {
    "question": "What does the example S3 policy on page 7 illustrate?",
    "answer": "It shows an example IAM policy allowing creation, retrieval, and deletion of metadata table configurations, as well as all s3tables actions, for a specific source bucket and table bucket."
  },
  {
    "question": "What is the Amazon Resource Name (ARN) format for a metadata table?",
    "answer": "The ARN format is: arn:aws:s3tables:region-code:account-id:bucket/table-bucket-name/table/metadata_table_name."
  },
  {
    "question": "Where are metadata tables stored within S3 table buckets?",
    "answer": "Fully managed metadata tables are stored in the aws_s3_metadata namespace in your table bucket."
  },
  {
    "question": "How can you create a metadata table configuration in S3?",
    "answer": "You can create a metadata table configuration using the S3 console, AWS CLI, SDKs, or S3 REST API."
  },
  {
    "question": "What are the prerequisites for creating an S3 metadata table configuration?",
    "answer": "You must have the necessary IAM permissions, an S3 table bucket in the same region and account, and optionally integrate the table bucket with AWS Glue Data Catalog."
  },
  {
    "question": "List the step-by-step process for creating a metadata table configuration via the AWS Management Console.",
    "answer": "1. Sign in and open S3. 2. Choose General purpose buckets. 3. Select the bucket. 4. Go to Metadata tab. 5. Click Create metadata configuration. 6. Specify a destination table bucket. 7. Set the metadata table name. 8. Choose Create."
  },
  {
    "question": "What requirements exist for table bucket and metadata table names?",
    "answer": "Table bucket names must be 3-63 characters, unique within your AWS account in the region, and contain only lowercase letters, numbers, and hyphens. Metadata table names must be 1-255 characters, unique in the namespace, and contain only lowercase letters, numbers, and underscores."
  },
  {
    "question": "How do you use the AWS CLI to create an S3 metadata table configuration?",
    "answer": "1. Create a table bucket with aws s3tables create-table-bucket. 2. Verify it with aws s3tables list-table-buckets. 3. Create a JSON config file. 4. Apply it with aws s3api create-bucket-metadata-table-configuration."
  },
  {
    "question": "What is the structure of the JSON configuration file for an S3 metadata table?",
    "answer": "The JSON must specify S3TablesDestination with the TableBucketArn and the TableName to be created or used."
  },
  {
    "question": "Can you monitor metadata table configuration changes using the AWS CLI?",
    "answer": "Yes, all changes are tracked and can be monitored via AWS CloudTrail."
  },
  {
    "question": "Which AWS CLI command do you use to verify that a metadata table configuration was created in an S3 bucket?",
    "answer": "You use the command: aws s3api get-bucket-metadata-table-configuration --bucket <bucket-name> --region <region>."
  },
  {
    "question": "How can you monitor updates to your S3 metadata table configuration?",
    "answer": "You can monitor updates using AWS CloudTrail, which tracks bucket-level actions."
  },
  {
    "question": "What AWS service allows you to send REST requests to create a metadata table configuration?",
    "answer": "You can use the S3 REST API to send requests such as CreateBucketMetadataTableConfiguration."
  },
  {
    "question": "How can you control access to your Amazon S3 metadata tables at both the bucket and table level?",
    "answer": "Access can be controlled using AWS Identity and Access Management (IAM) resource-based policies attached to your table bucket and metadata table."
  },
  {
    "question": "What is the recommendation regarding permissions for Amazon S3 writing to your metadata table and table bucket?",
    "answer": "Do not restrict Amazon S3 from writing to your table bucket or metadata table, or you will need to delete and recreate your metadata table configuration."
  },
  {
    "question": "What other AWS service allows for row and column-level access control in metadata tables?",
    "answer": "AWS Lake Formation enables row and column-level access control and cell-level security in metadata tables."
  },
  {
    "question": "What happens if you delete a metadata table configuration from a bucket?",
    "answer": "Only the configuration is deleted; the table bucket and metadata table still exist, but the metadata table will no longer be updated."
  },
  {
    "question": "What are the methods you can use to delete a metadata table configuration in S3?",
    "answer": "You can delete a metadata table configuration using the S3 console, AWS CLI, AWS SDKs, or the S3 REST API."
  },
  {
    "question": "Describe the steps to delete a metadata table configuration using the S3 Management Console.",
    "answer": "1. Sign in to the AWS Management Console and open S3, 2. Choose General purpose buckets, 3. Choose the relevant bucket, 4. Go to the Metadata tab, 5. Click Delete, 6. Enter 'confirm' and click Delete."
  },
  {
    "question": "How do you delete a metadata table configuration using the AWS CLI?",
    "answer": "Use the command: aws s3api delete-bucket-metadata-table-configuration --bucket <bucket-name> --region <region>."
  },
  {
    "question": "Can you use AWS CloudShell for running CLI commands described in the guide?",
    "answer": "Yes, AWS CloudShell can be used as a browser-based, pre-authenticated shell to run AWS CLI commands."
  },
  {
    "question": "What is the final verification step after deleting a metadata table configuration?",
    "answer": "Run aws s3api get-bucket-metadata-table-configuration to ensure the configuration is deleted."
  },
  {
    "question": "How do you delete a metadata table itself (not just the configuration) from your S3 table bucket?",
    "answer": "Use the CLI command: aws s3tables delete-table --table-bucket-arn <arn> --namespace <namespace> --name <table-name> --region <region>."
  },
  {
    "question": "What is recommended before deleting a metadata table from S3?",
    "answer": "It is recommended to first delete the associated metadata table configuration from your general purpose bucket."
  },
  {
    "question": "How can you verify that a metadata table has been deleted using the CLI?",
    "answer": "Run aws s3tables get-table with the appropriate parameters to check if the table has been deleted."
  },
  {
    "question": "What query engines does Amazon S3 Metadata support for querying managed metadata tables?",
    "answer": "Any query engine supporting the Apache Iceberg format, such as Amazon Athena, Amazon Redshift, Amazon EMR, Apache Spark, Trino, etc."
  },
  {
    "question": "List four use cases for querying S3 managed metadata tables.",
    "answer": "1. Discover storage usage patterns and trends, 2. Audit AWS KMS encryption key usage, 3. Search for objects by user metadata and tags, 4. Track object metadata changes over time."
  },
  {
    "question": "Can you join S3 managed metadata tables with custom metadata tables?",
    "answer": "Yes, you can join S3 managed metadata tables and custom metadata tables if they are stored in the same table bucket."
  },
  {
    "question": "What AWS service can be used to visualize query results from metadata tables?",
    "answer": "Amazon QuickSight can be used to build interactive dashboards from metadata table queries."
  },
  {
    "question": "Are there additional costs associated with querying S3 metadata tables?",
    "answer": "Yes, additional pricing applies depending on the query engine used."
  },
  {
    "question": "How do you specify a managed metadata table catalog and database when querying in Amazon Athena?",
    "answer": "Use 's3tablescatalog' for the catalog and 'aws_s3_metadata' for the database."
  },
  {
    "question": "How does Amazon Redshift access S3 metadata tables for querying?",
    "answer": "By creating a resource link to the 'aws_s3_metadata' namespace and querying the tables from the Amazon Redshift console."
  },
  {
    "question": "Describe the process of querying metadata tables using Amazon EMR.",
    "answer": "Create an EMR cluster configured for Apache Iceberg, and connect to metadata tables using Apache Spark."
  },
  {
    "question": "What do you need to use open-source query engines such as Spark to query S3 managed metadata tables?",
    "answer": "You need the Amazon S3 Tables Catalog for Apache Iceberg client catalog, available as an open-source library from AWS Labs."
  },
  {
    "question": "How can you optimize the performance and cost of S3 Metadata queries?",
    "answer": "Use specific time ranges in queries, for example, filtering with the 'record_timestamp' column to scan only relevant data."
  },
  {
    "question": "Give an example of a SQL query to find objects by file extension in Amazon Athena.",
    "answer": "SELECT key FROM \"s3tablescatalog/amzn-s3-demo-bucket\".\"aws_s3_metadata\".\"my_metadata_table\" WHERE key LIKE '%.jpg' AND record_type = 'CREATE';"
  },
  {
    "question": "How can you list object deletions from an S3 metadata table using SQL?",
    "answer": "SELECT DISTINCT bucket, key, sequence_number, record_type, record_timestamp, requester, source_ip_address, version_id FROM \"s3tablescatalog/amzn-s3-demo-bucket\".\"aws_s3_metadata\".\"my_metadata_table\" WHERE record_type = 'DELETE';"
  },
  {
    "question": "Which SQL query lists the ARNs of AWS KMS keys encrypting your S3 objects?",
    "answer": "SELECT DISTINCT kms_key_arn FROM \"s3tablescatalog/amzn-s3-demo-bucket\".\"aws_s3_metadata\".\"my_metadata_table\";"
  },
  {
    "question": "How can you find objects not encrypted with AWS KMS keys using a metadata table query?",
    "answer": "SELECT DISTINCT kms_key_arn FROM \"s3tablescatalog/amzn-s3-demo-bucket\".\"aws_s3_metadata\".\"my_metadata_table\" WHERE encryption_status NOT IN ('SSE-KMS', 'DSSE-KMS') AND record_type = 'CREATE';"
  },
  {
    "question": "How can you identify objects uploaded to a bucket by Amazon Bedrock?",
    "answer": "Query the user_metadata column for records where user_metadata['content-source'] = 'AmazonBedrock'."
  },
  {
    "question": "What is an example of a complex SQL query for finding the latest active version of each object in a metadata table?",
    "answer": "The query uses common table expressions (CTEs) to select records, determine version stacks, pick current versions, exclude deleted objects, and identify the latest version for each object based on sequence numbers."
  },
  {
    "question": "What does the sequence_number column represent in S3 metadata tables?",
    "answer": "It is an ordinal value for versions and events of an object, used to order records chronologically within a bucket and key."
  },
  {
    "question": "How are rows describing deleted or overwritten objects handled in S3 metadata tables?",
    "answer": "They remain in the metadata table, but certain columns like object_tags or user_metadata may become null to indicate the data is no longer accessible."
  },
  {
    "question": "How can you join S3 managed metadata tables with your own custom metadata tables?",
    "answer": "By using a standard SQL JOIN operator, you can query data from both S3 managed metadata tables and your own self-managed metadata tables. For example, you can join on bucket, key, and version_id columns and filter events like CREATE."
  },
  {
    "question": "What advantages does Amazon QuickSight provide when analyzing S3 metadata tables?",
    "answer": "Amazon QuickSight allows you to create interactive dashboards, monitor statistics, track changes, and gain operational insights about your S3 metadata tables by visualizing SQL query results."
  },
  {
    "question": "What types of analyses might a QuickSight dashboard for S3 metadata tables show?",
    "answer": "It can show the distribution of objects in storage classes, the percentage of small versus large objects, object types, and the percentage of uploads versus deletions."
  },
  {
    "question": "What are Amazon S3 objects composed of?",
    "answer": "Objects are composed of the file data and metadata that describes the object. Metadata can be system-defined or user-defined."
  },
  {
    "question": "What permissions are necessary before uploading files to an S3 bucket?",
    "answer": "You need write permissions for the bucket to upload files."
  },
  {
    "question": "What is the maximum size file you can upload using the Amazon S3 console?",
    "answer": "The maximum file size is 160 GB via the console. For larger files, use the AWS CLI, SDKs, or REST API."
  },
  {
    "question": "What happens when you upload an object with a key name that already exists in a versioning-enabled S3 bucket?",
    "answer": "Amazon S3 creates a new version of the object; it does not replace the existing object."
  },
  {
    "question": "In Amazon S3, what are your options for uploading data depending on file size?",
    "answer": "You can upload a single object up to 5 GB in one operation (SDK, CLI, API), use the console up to 160 GB, or use multipart upload (SDK, CLI, API) up to 5 TB."
  },
  {
    "question": "What is the multipart upload API operation designed for?",
    "answer": "It is designed to improve the upload experience for larger objects by allowing parts of the object to be uploaded independently, in any order, and in parallel."
  },
  {
    "question": "How is encryption handled by default when you upload an object to S3?",
    "answer": "Objects are automatically encrypted using server-side encryption with Amazon S3 managed keys (SSE-S3) by default."
  },
  {
    "question": "How can you use AWS KMS keys for encrypting S3 objects?",
    "answer": "Specify server-side encryption with AWS KMS keys (SSE-KMS) in the PUT request or set the default encryption configuration for the bucket to use SSE-KMS."
  },
  {
    "question": "What should you do if you want to use a KMS key owned by a different AWS account for S3 encryption?",
    "answer": "You must have permission to use the key and enter the KMS key ARN in the configuration."
  },
  {
    "question": "What happens if you rename an S3 object or change its properties?",
    "answer": "A new object (or new version if versioning is enabled) is created to replace the old one, and the modifying role becomes the owner."
  },
  {
    "question": "How does Amazon S3 display objects in folders in the console?",
    "answer": "Objects in folders are displayed showing only the filename after the last '/', but the key name includes the folder as a prefix."
  },
  {
    "question": "How are permissions and storage class configured during an S3 upload via the console?",
    "answer": "Object permissions can be set via the Access Control List (ACL); you can also change the storage class and encryption settings under Properties."
  },
  {
    "question": "How many tags and what size limitations exist for S3 object tagging?",
    "answer": "You can add up to 10 tags per object, with key up to 128 and value up to 255 Unicode characters."
  },
  {
    "question": "What are the two types of S3 metadata, and how do you specify user-defined metadata?",
    "answer": "System-defined metadata (like HTTP headers) and user-defined metadata (which must start with the x-amz-meta- prefix and adhere to US-ASCII)."
  },
  {
    "question": "What are two ways to upload objects to S3 using code and what are key points from the provided code examples?",
    "answer": "You can use the AWS SDK for .NET (C# example with PutObjectRequest for string or file, specifying ContentType and user metadata) or the AWS SDK for Java (using putObject and PutObjectRequest, also demonstrating how to set ContentType and custom metadata using ObjectMetadata)."
  },
  {
    "question": "What are important considerations regarding KMS keys when setting encryption in the S3 Console?",
    "answer": "Only KMS keys from the same region as the bucket are available; only symmetric keys are supported. If not listed, the ARN must be entered manually."
  },
  {
    "question": "What does enabling additional checksums during upload in S3 accomplish?",
    "answer": "It allows you to specify a checksum algorithm to help verify data integrity. If you provide a precalculated value, S3 will compare it to its computed value and error out if they do not match."
  },
  {
    "question": "What is the recommended public access setting for uploaded objects?",
    "answer": "It is recommended not to change the default public read access setting, except in specific cases such as hosting a public website."
  },
  {
    "question": "What must be done if you want to specify or replace existing user-defined metadata for an uploaded file?",
    "answer": "You must upload a new version with the new metadata, as updating metadata or object properties replaces the existing object (or adds a version if versioning is enabled)."
  },
  {
    "question": "How is uploading a folder to S3 represented in object key names?",
    "answer": "Each file in the folder is assigned an object key name based on the folder and filename, with the folder as a key prefix."
  },
  {
    "question": "List the order of steps to upload files and folders to S3 using the Console.",
    "answer": "1. Sign in and open the S3 Console, 2. Choose Buckets and select your target bucket, 3. Click Upload, 4. Add files/folders, 5. Optionally enable versioning, 6. Click Upload."
  },
  {
    "question": "How do you grant public read access to objects during upload?",
    "answer": "Adjust the Access Control List (ACL) under permissions, though this is only recommended for specific use cases. Public access can also be changed after upload."
  },
  {
    "question": "How do you upload an object to an S3 bucket in Java using the AWS SDK?",
    "answer": "By creating a PutObjectRequest with the bucket name, key, and file or data to upload, and then calling s3Client.putObject(request)."
  },
  {
    "question": "In the provided JavaScript example, what happens if an object is too large to upload in a single operation?",
    "answer": "If the object is larger than 5 GB, the upload fails with an 'EntityTooLarge' error. You should use the S3 Console (up to 160GB) or the multipart upload API (up to 5TB)."
  },
  {
    "question": "How can you handle S3 upload errors in JavaScript when using the AWS SDK?",
    "answer": "Catch S3ServiceException for known API errors and handle by name. For unknown errors, rethrow or log."
  },
  {
    "question": "How do you upload an object using the AWS SDK for PHP?",
    "answer": "Create an S3Client, and call putObject with a configuration array that specifies the bucket, key, body, and optional ACL or metadata."
  },
  {
    "question": "How can you upload a file of any size using the AWS SDK for Ruby version 3?",
    "answer": "Create an Aws::S3::Resource instance, reference the target object by bucket and key, then call #upload_file on the object."
  },
  {
    "question": "What alternative Ruby method allows uploading object data from non-file sources?",
    "answer": "Use the #put method on Aws::S3::Object, passing the string or IO object as the body."
  },
  {
    "question": "How do you check if an object with a specific key already exists before uploading to S3?",
    "answer": "Use a conditional write (like If-None-Match or similar) on the upload operation, or query the object first to prevent overwrites."
  },
  {
    "question": "What is multipart upload in Amazon S3?",
    "answer": "A process that allows uploading a single object as a set of parts, which can be uploaded independently and in any order, then S3 assembles the complete object."
  },
  {
    "question": "Why should you use multipart upload for objects larger than 100 MB?",
    "answer": "It provides improved throughput, allows pausing and resuming uploads, easier recovery from network issues, and lets you upload objects before knowing the total size."
  },
  {
    "question": "List two key advantages of multipart upload in S3.",
    "answer": "Parallel part uploads (better throughput), and resiliency against network errors by only retrying failed parts."
  },
  {
    "question": "Describe the three steps of the multipart upload process.",
    "answer": "1. Initiate upload (get upload ID); 2. Upload parts (specifying upload ID and part number); 3. Complete multipart upload (provide upload ID, list of part numbers and ETag values)."
  },
  {
    "question": "What must you record after uploading each part during multipart upload?",
    "answer": "You must record the part number and ETag value for each part to include in the final complete-multipart-upload request."
  },
  {
    "question": "What occurs if you upload a new part using the same part number as a previous part in a multipart upload?",
    "answer": "The previously uploaded part with that number is overwritten."
  },
  {
    "question": "What is the minimum and maximum allowed part number for S3 multipart upload parts?",
    "answer": "Part numbers can be any integer between 1 and 10,000."
  },
  {
    "question": "How does S3 calculate object integrity in multipart upload?",
    "answer": "AWS SDKs calculate a checksum for each part; after upload completion S3 computes a single ETag (checksum of checksums) for the full object."
  },
  {
    "question": "Why is it important to complete or stop a multipart upload after uploading parts?",
    "answer": "Because S3 will continue to bill for storage of the uploaded parts until you explicitly complete or abort the upload."
  },
  {
    "question": "What information do you need to complete a multipart upload in S3?",
    "answer": "The upload ID, a list of uploaded part numbers, and their ETag values."
  },
  {
    "question": "What happens if the full-object checksum value does not match after a multipart upload with a client-specified checksum?",
    "answer": "S3 rejects the upload with a BadDigest error."
  },
  {
    "question": "How do you list parts of an in-progress multipart upload using the S3 API?",
    "answer": "Use the ListParts operation to return part information for that upload."
  },
  {
    "question": "What is the maximum number of parts the ListParts operation will return in a single response?",
    "answer": "Up to 1,000 parts per request."
  },
  {
    "question": "If a multipart upload has more than 1,000 parts, how can you list all the parts?",
    "answer": "Send multiple ListParts requests with pagination."
  },
  {
    "question": "Why shouldn't you use the result of ListParts when sending a CompleteMultipartUpload request?",
    "answer": "Because ListParts may not include parts that are still being uploaded; you should maintain your own record of uploaded parts and ETags."
  },
  {
    "question": "How do checksums work with multipart uploads?",
    "answer": "S3 calculates and stores checksums for individual parts and for the combined object. You can choose the checksum algorithm; CRC-64/NVME is the default if you do not specify one."
  },
  {
    "question": "What requirement is imposed when using checksums with multipart upload?",
    "answer": "All part numbers must be consecutive and start with 1, or S3 will return an HTTP 500 error if you try to complete the upload otherwise."
  },
  {
    "question": "How can you retrieve checksum values for an S3 multipart upload?",
    "answer": "For individual parts, use GetObject or HeadObject; for the full object, use PutObject. During upload, ListParts can show checksums of uploaded parts."
  },
  {
    "question": "What happens in a distributed environment if several multipart uploads use the same object key with versioning enabled?",
    "answer": "Completing a multipart upload always creates a new version; the current object version is determined by the most recent multipart upload start (by createdDate)."
  },
  {
    "question": "What is the maximum object size supported by S3's multipart upload?",
    "answer": "5 TB is the maximum object size that can be uploaded using multipart upload."
  },
  {
    "question": "How big can a single part in a multipart upload be?",
    "answer": "Individual parts can range from 5 MB to 5 GB, except for the last part, which can be smaller."
  },
  {
    "question": "Can anonymous users initiate multipart uploads in Amazon S3?",
    "answer": "No, only authenticated users can initiate multipart uploads."
  },
  {
    "question": "For what kind of objects is it best practice to use multipart upload?",
    "answer": "For objects larger than 100 MB, multipart upload is recommended for better performance and reliability."
  },
  {
    "question": "What determines which version of an object becomes the current version when multiple multipart uploads are started for the same key?",
    "answer": "The multipart upload request submitted most recently determines the current version, regardless of the order in which the uploads are completed."
  },
  {
    "question": "What can happen if another operation deletes an S3 object key after you initiate a multipart upload but before you complete it?",
    "answer": "The complete multipart upload response may indicate a successful creation, but you may never see the object if the key was deleted during the process."
  },
  {
    "question": "How can you prevent uploading objects with identical key names during multipart upload in S3?",
    "answer": "You can check for the existence of an object using a conditional write (like PutObject or CompleteMultipartUpload with preconditions) to prevent overwrites."
  },
  {
    "question": "What is the purpose of using conditional writes when uploading to S3?",
    "answer": "Conditional writes prevent overwriting existing data by checking if an object with the same key name already exists before uploading."
  },
  {
    "question": "How are storage, bandwidth, and request charges billed during a multipart upload?",
    "answer": "You are billed for all storage, bandwidth, and requests for the multipart upload and its parts until you either complete or abort the upload."
  },
  {
    "question": "How are in-progress multipart parts billed if using S3 Glacier Flexible Retrieval or Deep Archive storage classes?",
    "answer": "In-progress parts are billed at S3 Standard rates as staging storage until the upload completes, after which the CompleteMultipartUpload request is billed at Glacier or Deep Archive rates."
  },
  {
    "question": "Are there early delete charges for aborting incomplete multipart uploads in S3?",
    "answer": "No, there are no early delete charges when aborting incomplete multipart uploads, regardless of storage class."
  },
  {
    "question": "What AWS S3 action is recommended to minimize storage costs due to incomplete multipart uploads?",
    "answer": "Configure a lifecycle rule using the AbortIncompleteMultipartUpload action to delete incomplete multipart uploads after a set number of days."
  },
  {
    "question": "After you stop a multipart upload, what happens to the uploaded parts and storage charges?",
    "answer": "All uploaded parts and artifacts are deleted, and you are not billed for them."
  },
  {
    "question": "Which S3 API operations are part of the multipart upload workflow according to the user guide?",
    "answer": "CreateMultipartUpload, UploadPart, UploadPartCopy, CompleteMultipartUpload, AbortMultipartUpload, ListParts, ListMultipartUploads."
  },
  {
    "question": "List multipart upload-related operations supported by the AWS CLI.",
    "answer": "InitiateMultipartUpload, UploadPart, UploadPartCopy, CompleteMultipartUpload, AbortMultipartUpload, ListParts, ListMultipartUploads."
  },
  {
    "question": "What is required to allow a principal to create a multipart upload in S3 using bucket or user policy?",
    "answer": "The principal must be allowed to perform the s3:PutObject action."
  },
  {
    "question": "What permission must the initiator of a multipart upload have in order to upload or copy parts?",
    "answer": "The initiator must have permission to perform s3:PutObject on the object, and in the case of UploadPartCopy, also s3:GetObject on the source object."
  },
  {
    "question": "What's the role of the 'Initiator' container in S3 multipart upload operations?",
    "answer": "It identifies who initiated the multipart upload, showing account or IAM user details."
  },
  {
    "question": "Which extra permission is required for the initiator to upload a part from an existing object (UploadPartCopy)?",
    "answer": "s3:GetObject permission on the source object."
  },
  {
    "question": "What permission is needed to stop or abort a multipart upload?",
    "answer": "s3:AbortMultipartUpload permission; the bucket owner and initiator are allowed by default."
  },
  {
    "question": "Who is allowed to list parts of a multipart upload by default?",
    "answer": "The bucket owner, the initiator of the multipart upload, and the AWS account controlling the IAM user if applicable."
  },
  {
    "question": "What permission is required to list multipart uploads in progress for a bucket?",
    "answer": "s3:ListBucketMultipartUploads on the bucket."
  },
  {
    "question": "What additional permissions are required to use SSE-KMS with multipart uploads?",
    "answer": "kms:GenerateDataKey (for CreateMultipartUpload), kms:Decrypt (for UploadPart and UploadPartCopy), and coverage in both the key and IAM policy if cross-account."
  },
  {
    "question": "What happens if you use CompleteMultipartUpload without proper KMS permissions?",
    "answer": "The object is created without a checksum value."
  },
  {
    "question": "What must you provide when using SSE-C for multipart uploads when completing the upload?",
    "answer": "You must provide the corresponding encryption key (SSE-C) or the object will be created without a checksum."
  },
  {
    "question": "Which three Amazon S3 APIs require checksum headers in a multipart upload?",
    "answer": "CreateMultipartUpload, UploadPart, and CompleteMultipartUpload."
  },
  {
    "question": "Which checksum headers are required for the CRC-64/NVME algorithm when initiating a multipart upload?",
    "answer": "x-amz-checksum-algorithm is required for CreateMultipartUpload. x-amz-checksum-crc64nvme is optional for UploadPart and CompleteMultipartUpload."
  },
  {
    "question": "For composite checksums (e.g., SHA-256), which headers must be in the CompleteMultipartUpload request?",
    "answer": "All part-level checksums must be included in the CompleteMultipartUpload request."
  },
  {
    "question": "What table on page 7 of the PDF outlines API header requirements for multipart upload and checksums?",
    "answer": "The table outlines which headers and algorithms (CRC-64, CRC32, SHA-1, SHA-256) must be provided or are optional for each multipart upload API action."
  },
  {
    "question": "What is the recommended way to minimize S3 storage costs from incomplete multipart uploads?",
    "answer": "Configure a lifecycle rule to automatically abort incomplete multipart uploads after a specified number of days."
  },
  {
    "question": "What is the effect of an AbortIncompleteMultipartUpload lifecycle rule?",
    "answer": "Multipart uploads that aren't completed within the time frame become eligible for abort, which deletes all parts; completed uploads are unaffected."
  },
  {
    "question": "Provide an example of an XML lifecycle configuration that deletes incomplete multipart uploads after 7 days.",
    "answer": "<LifecycleConfiguration><Rule><ID>sample-rule</ID><Prefix></Prefix><Status>Enabled</Status><AbortIncompleteMultipartUpload><DaysAfterInitiation>7</DaysAfterInitiation></AbortIncompleteMultipartUpload></Rule></LifecycleConfiguration>"
  },
  {
    "question": "Does deleting incomplete multipart upload parts incur S3 early delete charges?",
    "answer": "No, there are no early delete charges for removing incomplete multipart upload artifacts."
  },
  {
    "question": "How do you add a lifecycle rule in the S3 Console to delete incomplete multipart uploads after 7 days?",
    "answer": "Go to the S3 Console, choose your bucket, select the Management tab, create a lifecycle rule, specify the rule applies to all objects or use filters, and select 'Delete incomplete multipart uploads' under lifecycle rule actions."
  },
  {
    "question": "Does AbortIncompleteMultipartUpload lifecycle action delete objects?",
    "answer": "No, it only deletes multipart upload parts that were not completed; it does not delete objects."
  },
  {
    "question": "Name at least three SDKs or tools supported by S3 for multipart upload.",
    "answer": "AWS SDKs (e.g., Java, .NET, Python), AWS CLI, and REST API."
  },
  {
    "question": "Is there an S3 API walkthrough example that combines Lambda and multipart upload?",
    "answer": "Yes, the documentation references a walkthrough using multipart upload and transfer acceleration with AWS Lambda."
  },
  {
    "question": "Where do you find details on how to add preconditions to S3 operations with conditional requests?",
    "answer": "See the section 'Add preconditions to S3 operations with conditional requests' in the documentation."
  },
  {
    "question": "Can the bucket owner deny principals the ability to abort multipart uploads?",
    "answer": "Yes, the bucket owner can deny any principal the s3:AbortMultipartUpload permission."
  },
  {
    "question": "Where do you find more information about mapping of ACL permissions and access policy permissions?",
    "answer": "The documentation refers to the section 'Mapping of ACL permissions and access policy permissions.'"
  },
  {
    "question": "What must your IAM user or role have if it belongs to a different account than the KMS key used for SSE-KMS?",
    "answer": "Permissions on both the key policy (resource policy) and your IAM user/role must include necessary KMS actions."
  },
  {
    "question": "How do you configure S3 to automatically remove incomplete multipart uploads?",
    "answer": "Set a lifecycle rule with 'AbortIncompleteMultipartUpload', specifying the number of days after initiation. This deletes all parts of incomplete uploads after the configured period."
  },
  {
    "question": "Which AWS CLI command adds a lifecycle configuration to delete incomplete multipart uploads?",
    "answer": "Use 'aws s3api put-bucket-lifecycle-configuration --bucket <bucket-name> --lifecycle-configuration <filename>' to set the lifecycle rule."
  },
  {
    "question": "What does the sample JSON lifecycle configuration to abort incomplete multipart uploads look like?",
    "answer": "It sets 'Rules' with 'AbortIncompleteMultipartUpload' where 'DaysAfterInitiation' is 7, a blank prefix means it matches all objects, and 'Status' is 'Enabled'."
  },
  {
    "question": "How do you verify that a lifecycle configuration has been added to your S3 bucket via AWS CLI?",
    "answer": "Run 'aws s3api get-bucket-lifecycle --bucket <bucket-name>'."
  },
  {
    "question": "Which AWS CLI command deletes the lifecycle configuration from a bucket?",
    "answer": "Run 'aws s3api delete-bucket-lifecycle --bucket <bucket-name>'."
  },
  {
    "question": "What is multipart upload in Amazon S3 and why is it useful?",
    "answer": "Multipart upload lets you upload a single object as a set of parts, independently and in any order, making large uploads more reliable and resumable."
  },
  {
    "question": "Who can initiate multipart uploads in S3?",
    "answer": "Only authenticated users; anonymous users cannot initiate multipart uploads."
  },
  {
    "question": "Why might you need to use multipart upload for files over 160GB?",
    "answer": "Because the S3 console upload limit is 160GB; larger files require AWS CLI, SDKs, or REST API with multipart upload."
  },
  {
    "question": "What are the key multipart upload operations provided by the AWS CLI?",
    "answer": "Initiate Multipart Upload, Upload Part, Upload Part (Copy), Complete Multipart Upload, Abort Multipart Upload, List Parts, and List Multipart Uploads."
  },
  {
    "question": "What is the order and independence of parts uploaded via multipart upload?",
    "answer": "Parts can be uploaded independently and in any order; failures only require retransmitting the failed part."
  },
  {
    "question": "Which AWS documentation section covers CLI setup for multipart upload?",
    "answer": "See 'Developing with Amazon S3 using the AWS CLI' in the Amazon S3 API Reference."
  },
  {
    "question": "What does the .NET SDK's TransferUtility class do for multipart uploads?",
    "answer": "It simplifies multipart uploads and handles file or stream sources, offering overloads to manage file paths, streams, metadata, storage class, ACLs, and parallelism."
  },
  {
    "question": "If you set TransferUtility to read from a stream, does it upload parts in parallel?",
    "answer": "No, when the source is a stream, the TransferUtility does not perform concurrent uploads."
  },
  {
    "question": "What happens when you use TransferUtility.Upload multiple times for the same key?",
    "answer": "Each successive call replaces the previously uploaded object."
  },
  {
    "question": "In the .NET example, how do you specify metadata and ACL for the uploaded file?",
    "answer": "Use TransferUtilityUploadRequest to set Metadata and CannedACL properties."
  },
  {
    "question": "What are the options demonstrated in the C# TransferUtility multipart upload example?",
    "answer": "Upload from file (implicit key), from file with explicit key, from stream, and from file with advanced options (storage class, part size, ACL, metadata)."
  },
  {
    "question": "How do you handle errors in a C# TransferUtility multipart upload?",
    "answer": "Catch AmazonS3Exception for S3 errors and Exception for unknown server errors; log the error messages."
  },
  {
    "question": "In the JavaScript example, what does the Upload class from '@aws-sdk/lib-storage' enable?",
    "answer": "It provides a high-level multipart upload, managing part size and progress, and auto-uploads a large buffer in multiple parts."
  },
  {
    "question": "How is the upload progress visualized in the JavaScript multipart upload sample?",
    "answer": "A ProgressBar is updated with loaded and total bytes during the upload using the 'httpUploadProgress' event."
  },
  {
    "question": "What types of errors are explicitly handled in the JS multipart upload sample?",
    "answer": "AbortError (for user/interruption aborts) as well as general errors thrown by the Upload operation."
  },
  {
    "question": "How does the JavaScript example download large objects in ranges?",
    "answer": "It sends repeated GetObjectCommand requests with 'Range' headers, writes each chunk to a file, and updates a write stream until the download completes."
  },
  {
    "question": "What is the function of the isComplete helper in the JS download example?",
    "answer": "It returns true when the end byte index matches length-1, meaning the full object has been downloaded."
  },
  {
    "question": "How do you handle a NoSuchKey error in the JavaScript download chunk example?",
    "answer": "Log the error and remove the local file that was being written."
  },
  {
    "question": "Where is a Go multipart upload example referenced?",
    "answer": "See the linked section 'Upload or download large files to and from Amazon S3 using an AWS SDK' in the PDF."
  },
  {
    "question": "In the Go sample, what does the upload manager do?",
    "answer": "It uses manager.NewUploader to split large data into parts and upload them concurrently, allowing efficient multipart uploads."
  },
  {
    "question": "How do you set the part size in the Go upload example?",
    "answer": "By supplying a function in NewUploader options to set 'u.PartSize' (in this example, to 10 MiB: partMiBs * 1024 * 1024)."
  },
  {
    "question": "What struct encapsulates S3 operations in the Go multipart example?",
    "answer": "The 'BucketBasics' struct, which contains an S3 client, organizes all S3 bucket and object actions."
  },
  {
    "question": "What are the main analytics and insights features offered by Amazon S3?",
    "answer": "Amazon S3 offers three main analytics and insights features: Amazon S3 Storage Lens (provides 60+ usage and activity metrics and interactive dashboards), Storage Class Analysis (analyzes storage access patterns to decide when to move data to more cost-effective storage classes), and S3 Inventory with Inventory reports (audits and reports on objects and their metadata)."
  },
  {
    "question": "What type of consistency does Amazon S3 provide for PUT and DELETE operations?",
    "answer": "Amazon S3 provides strong read-after-write consistency for PUT and DELETE requests of objects in your Amazon S3 bucket in all AWS Regions. This applies to both writes of new objects and PUT requests that overwrite existing objects and DELETE requests."
  },
  {
    "question": "What is an object in Amazon S3?",
    "answer": "An object in Amazon S3 is a file and any metadata that describes the file. Objects are the fundamental entities stored in Amazon S3 and consist of object data and metadata, which is a set of name-value pairs that describe the object."
  },
  {
    "question": "What is a bucket in Amazon S3?",
    "answer": "A bucket is a container for objects in Amazon S3. To store data in Amazon S3, you first create a bucket and specify a bucket name and AWS Region, then upload your data to that bucket as objects."
  },
  {
    "question": "What are the three types of buckets supported by Amazon S3?",
    "answer": "Amazon S3 supports three types of buckets: general purpose buckets (recommended for most use cases), directory buckets (recommended for low-latency and data-residency use cases), and table buckets (recommended for storing tabular data like daily purchase transactions or streaming sensor data)."
  },
  {
    "question": "What is the default limit for directory buckets in an AWS account?",
    "answer": "By default, you can create up to 100 directory buckets in your AWS account, with no limit on the number of objects that you can store in a directory bucket."
  },
  {
    "question": "What storage class is supported by directory buckets in Availability Zones?",
    "answer": "Directory buckets in Availability Zones support the S3 Express One Zone storage class, which provides single-digit millisecond PUT and GET latencies and is recommended for performance-sensitive applications."
  },
  {
    "question": "What is the default limit for table buckets per AWS account?",
    "answer": "By default, you can create up to 10 table buckets per AWS account per AWS Region and up to 10,000 tables per table bucket."
  },
  {
    "question": "What is an object key in Amazon S3?",
    "answer": "An object key (or key name) is the unique identifier for an object within a bucket. Every object in a bucket has exactly one key. The combination of a bucket, object key, and optionally version ID uniquely identifies each object."
  },
  {
    "question": "What is S3 Versioning?",
    "answer": "S3 Versioning is a feature that allows you to keep multiple variants of an object in the same bucket. With S3 Versioning, you can preserve, retrieve, and restore every version of every object stored in your buckets, helping you recover from unintended user actions and application failures."
  },
  {
    "question": "What is a bucket policy in Amazon S3?",
    "answer": "A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that you can use to grant access permissions to your bucket and the objects in it. Only the bucket owner can associate a policy with a bucket, and bucket policies are limited to 20 KB in size."
  },
  {
    "question": "What are S3 Access Points?",
    "answer": "Amazon S3 Access Points are named network endpoints with dedicated access policies that describe how data can be accessed using that endpoint. Access Points are attached to buckets and can be used to perform S3 object operations like GetObject and PutObject, simplifying data access management at scale."
  },
  {
    "question": "What are Access Control Lists (ACLs) in Amazon S3?",
    "answer": "ACLs are an access control mechanism that can be used to grant read and write permissions to authorized users for individual general purpose buckets and objects. Each general purpose bucket and object has an ACL attached to it as a subresource that defines which AWS accounts or groups are granted access and the type of access."
  },
  {
    "question": "What is the recommended approach for ACLs in modern Amazon S3 use cases?",
    "answer": "A majority of modern use cases in Amazon S3 no longer require the use of ACLs. Amazon recommends keeping ACLs disabled, except in unusual circumstances where you need to control access for each object individually. With ACLs disabled, you can use policies to control access to all objects in your bucket."
  },
  {
    "question": "Can objects stored in an AWS Region automatically move to another Region?",
    "answer": "No, objects stored in an AWS Region never leave the Region unless you explicitly transfer or replicate them to another Region. For example, objects stored in the Europe (Ireland) Region never leave it."
  },
  {
    "question": "What happens when multiple PUT requests are made to the same key simultaneously?",
    "answer": "Amazon S3 does not support object locking for concurrent writers. If two PUT requests are simultaneously made to the same key, the request with the latest timestamp wins. If this is an issue, you must build an object-locking mechanism into your application."
  },
  {
    "question": "What AWS services can be used with data loaded into Amazon S3?",
    "answer": "You can use Amazon S3 with services like Amazon Elastic Compute Cloud (Amazon EC2), Amazon EMR, AWS Snow Family, and AWS Transfer Family."
  },
  {
    "question": "What are the different ways to access Amazon S3?",
    "answer": "You can access Amazon S3 through: AWS Management Console (web-based interface), AWS Command Line Interface (CLI), AWS SDKs (software development kits for various programming languages), and Amazon S3 REST API (HTTP interface)."
  },
  {
    "question": "How does Amazon S3 pricing work?",
    "answer": "Amazon S3 charges you only for what you actually use, with no hidden fees and no overage charges. This gives you a variable-cost service that can grow with your business while providing the cost advantages of AWS infrastructure."
  },
  {
    "question": "What is PCI DSS compliance in relation to Amazon S3?",
    "answer": "Amazon S3 supports the processing, storage, and transmission of credit card data by a merchant or service provider, and has been validated as being compliant with Payment Card Industry (PCI) Data Security Standard (DSS)."
  },
  {
    "question": "What are the bucket naming requirements for general purpose buckets?",
    "answer": "Bucket names must be unique within a partition, be between 3 and 63 characters long, consist only of lowercase letters, numbers, periods (.), and hyphens (-), and begin and end with a letter or number. For best compatibility, avoid using periods in bucket names except for static website hosting."
  },
  {
    "question": "What is the default Object Ownership setting for new buckets?",
    "answer": "The default Object Ownership setting is 'Bucket owner enforced', where ACLs are disabled and the bucket owner automatically owns and has full control over every object in the bucket. The bucket uses policies exclusively to define access control."
  },
  {
    "question": "What are the default Block Public Access settings for new buckets?",
    "answer": "By default, all four Block Public Access settings are enabled for new buckets. Amazon recommends keeping all settings enabled unless you know you need to turn off one or more of them for your specific use case."
  },
  {
    "question": "What encryption options are available for Amazon S3 buckets?",
    "answer": "Amazon S3 offers three encryption options: Server-side encryption with Amazon S3 managed keys (SSE-S3), Server-side encryption with AWS Key Management Service keys (SSE-KMS), and Dual-layer server-side encryption with AWS KMS keys (DSSE-KMS)."
  },
  {
    "question": "What happens when you enable versioning on a bucket for the first time?",
    "answer": "When you enable versioning on a bucket for the first time, it might take a short amount of time for the change to be fully propagated. Amazon recommends waiting for 15 minutes after enabling versioning before issuing write operations (PUT or DELETE requests) on objects in the bucket."
  },
  {
    "question": "What is the difference between directory buckets and general purpose buckets in terms of structure?",
    "answer": "Directory buckets organize objects into hierarchical directories (prefixes) instead of the flat storage structure of general purpose buckets. This bucket type has no prefix limits and individual directories can scale horizontally."
  },
  {
    "question": "What is the Apache Iceberg format used for in Amazon S3?",
    "answer": "S3 Tables are purpose-built for storing tabular data in the Apache Iceberg format. You can query tabular data in S3 Tables with popular query engines, including Amazon Athena, Amazon Redshift, and Apache Spark."
  },
  {
    "question": "What are the different namespaces used by the three bucket types?",
    "answer": "General purpose buckets use the 'S3' namespace, directory buckets use the 's3express' namespace, and table buckets use the 's3tables' namespace."
  },
  {
    "question": "What is the maximum size limit for bucket policies?",
    "answer": "Bucket policies are limited to 20 KB in size."
  },
  {
    "question": "What are the prerequisites for getting started with Amazon S3?",
    "answer": "Before you begin with Amazon S3, you need to complete the steps in Setting up Amazon S3, which includes signing up for an AWS account and creating a user with administrative access."
  },
  {
    "question": "What is the maximum object size for a single PUT operation in S3?",
    "answer": "The maximum object size for a single PUT operation is 5 GB using AWS SDK or CLI, but the multipart upload API supports objects up to 5 TB."
  },
  {
    "question": "Why might an S3 upload return an 'EntityTooLarge' error?",
    "answer": "An 'EntityTooLarge' error occurs if you attempt to upload an object larger than the maximum allowed size for a single PUT (5 GB); use multipart upload for larger files."
  },
  {
    "question": "How can you check that a recently uploaded object now exists in S3?",
    "answer": "Call HeadObject using the s3.NewObjectExistsWaiter utility to confirm the object's existence."
  },
  {
    "question": "What is a download manager and how is it used in S3?",
    "answer": "A download manager breaks a large object into parts, downloads them concurrently, and writes them to a buffer until all data is received."
  },
  {
    "question": "How do you set up multipart upload with the AWS SDK for PHP?",
    "answer": "Instantiate a MultipartUploader with a S3Client, target file path, bucket name, and key; handle upload and potential exceptions using try/catch."
  },
  {
    "question": "How does the Python boto3 TransferCallback class help with multipart uploads?",
    "answer": "It provides a callback to display progress, keeping track of bytes transferred and the percent complete during uploads or downloads."
  },
  {
    "question": "How do you upload an S3 object in Python with custom chunk size and metadata?",
    "answer": "Use UploadFile with extra TransferConfig for multipart_chunksize, and additional ExtraArgs for metadata key-value pairs."
  },
  {
    "question": "How do you upload a file but ensure it is not uploaded as multipart in Python?",
    "answer": "Set TransferConfig's multipart_threshold to a value larger than your file size, forcing a standard upload."
  },
  {
    "question": "How do you upload a file using server-side encryption (SSE-C) in Python and boto3?",
    "answer": "Add ExtraArgs with keys SSECustomerAlgorithm and SSECustomerKey, providing the algorithm and the customer-provided key."
  },
  {
    "question": "How do you perform a single-threaded download of a large S3 file in Python?",
    "answer": "Configure TransferConfig with use_threads=False and use download_file() with the config."
  },
  {
    "question": "How is a multipart threshold used during downloads from S3 with Python?",
    "answer": "Setting TransferConfig's multipart_threshold to a value higher than the file disables multipart download for files below the threshold."
  },
  {
    "question": "How do you use a customer-provided key to download an encrypted S3 object with Python?",
    "answer": "Set ExtraArgs with SSECustomerAlgorithm and the correct SSECustomerKey on the download_file call."
  },
  {
    "question": "What key advantage does the low-level AWS SDK for S3 provide for multipart upload?",
    "answer": "It enables granular control such as pausing, resuming, varying part sizes, or handling unknown data size at the start."
  },
  {
    "question": "List the sequence of steps for a multipart upload in Java using the AWS SDK.",
    "answer": "1) Initiate multipart upload and get the upload ID. 2) Upload each part with UploadPart, saving each part's ETag. 3) After all parts are uploaded, call CompleteMultipartUpload with the list of ETags."
  },
  {
    "question": "How do you handle the final part in AWS Java SDK multipart upload if it is smaller than partSize?",
    "answer": "Adjust partSize as Math.min(partSize, contentLength - filePosition) for the last part."
  },
  {
    "question": "What should you always do after uploading all parts in a multipart upload?",
    "answer": "Call CompleteMultipartUpload so S3 assembles all parts and finalizes the object."
  },
  {
    "question": "If an error occurs during Java multipart upload, what is best practice for cleanup?",
    "answer": "Catch AmazonServiceException or SdkClientException and consider calling AbortMultipartUpload to avoid orphaned data and storage costs."
  },
  {
    "question": "What does the UploadPartRequest in the Java example configure?",
    "answer": "It specifies the bucket, key, upload ID, part number, file offset, file reference, and part size."
  },
  {
    "question": "Is it possible to vary the size and number of parts in S3 multipart upload?",
    "answer": "Yes, each part can have a different size except that all (except possibly the last) must be at least 5 MB."
  },
  {
    "question": "What happens if you do not call CompleteMultipartUpload after uploading all parts?",
    "answer": "The parts remain as incomplete artifacts, incur storage costs, and do not constitute a downloadable object."
  },
  {
    "question": "What programming languages are covered by the S3 multipart upload user guide you provided?",
    "answer": "Go, PHP, Python, Java, JavaScript, and C# (.NET) are all covered with language-specific multipart upload/download examples."
  },
  {
    "question": "What is the main purpose of using multipart upload in Amazon S3?",
    "answer": "The main purpose of using multipart upload in Amazon S3 is to upload large objects more efficiently by splitting them into smaller parts, uploading each part separately, and then combining them into a single object."
  },
  {
    "question": "How can you set an explicit timeout for uploading large files using the AWS SDK for .NET?",
    "answer": "You can set an explicit timeout for uploading large files using the UploadPartRequest in the AWS SDK for .NET API."
  },
  {
    "question": "What AWS SDK namespaces are used in the .NET code sample for uploading a file with multipart upload?",
    "answer": "The namespaces used are Amazon, Amazon.Runtime, Amazon.S3, Amazon.S3.Model, System, System.Collections.Generic, System.IO, and System.Threading.Tasks."
  },
  {
    "question": "In the provided C# code, what method is responsible for tracking the progress of each uploaded part?",
    "answer": "The 'UploadPartProgressEventCallback' method is responsible for tracking the progress of each uploaded part."
  },
  {
    "question": "What happens in the C# example if an exception occurs during the multipart upload?",
    "answer": "If an exception occurs during the process, the multipart upload is aborted using 'AbortMultipartUploadRequest' to prevent incomplete uploads."
  },
  {
    "question": "In the PHP example, what method is used to initiate the multipart upload?",
    "answer": "The 'createMultipartUpload' method of the S3Client class is used to initiate the multipart upload in the PHP example."
  },
  {
    "question": "How are the parts of a file read and uploaded in the PHP multipart upload example?",
    "answer": "The file is opened and read in 5 MB chunks with 'fread', then each chunk is uploaded as a part using the 'uploadPart' method."
  },
  {
    "question": "What should you do if a multipart upload fails in the PHP code sample?",
    "answer": "If the upload fails, you should call 'abortMultipartUpload' to stop and clean up the multipart upload."
  },
  {
    "question": "According to the guide, what are the two ways the AWS SDK for Ruby version 3 supports Amazon S3 multipart uploads?",
    "answer": "The two ways are: 1) using managed file uploads, which handle multipart uploads automatically, and 2) using low-level multipart upload operations directly (such as 'create_multipart_upload', 'upload_part', and 'complete_multipart_upload')."
  },
  {
    "question": "What are the benefits of using managed file uploads with the AWS SDK for Ruby?",
    "answer": "The benefits of managed file uploads include automatic multipart upload management for objects larger than 15MB, correct binary mode file opening, and using multiple threads for parallel uploads."
  },
  {
    "question": "How can you upload an entire directory to S3 using the .NET TransferUtility class?",
    "answer": "You can use the 'UploadDirectoryAsync' method of the TransferUtility class to upload an entire directory, and you can specify search patterns and options to filter or recursively upload files from subdirectories."
  },
  {
    "question": "How does Amazon S3 construct key names when uploading a directory using TransferUtility?",
    "answer": "Amazon S3 constructs key names based on the original file path of each uploaded file within the directory structure."
  },
  {
    "question": "What is the purpose of specifying a filter expression (such as '*.pdf') when using TransferUtility.UploadDirectory?",
    "answer": "The filter expression specifies which files to select within the directory, allowing uploads to be limited to files matching the pattern."
  },
  {
    "question": "Which classes and methods would you use in .NET to list all in-progress multipart uploads on an S3 bucket?",
    "answer": "You would use the 'ListMultipartUploadsRequest' class and the AmazonS3Client's 'ListMultipartUploads' method to list all in-progress multipart uploads."
  },
  {
    "question": "What does an in-progress multipart upload mean in the context of Amazon S3?",
    "answer": "An in-progress multipart upload is a multipart upload that has been initiated but not yet completed or stopped."
  },
  {
    "question": "What AWS CLI commands can be used to list multipart uploads and their parts?",
    "answer": "You can use the 'list-parts' and 'list-multipart-uploads' commands in AWS CLI to list the uploaded parts for a specific multipart upload and all in-progress multipart uploads, respectively."
  },
  {
    "question": "What are the main REST API operations for listing multipart uploads and their parts in Amazon S3?",
    "answer": "The main REST API operations are 'ListParts' to list uploaded parts of a specific upload, and 'ListMultipartUploads' to list all in-progress multipart uploads."
  },
  {
    "question": "Describe the Java process for listing multipart uploads in a bucket using the low-level API as shown in the guide.",
    "answer": "Create an instance of 'ListMultipartUploadsRequest' with the bucket name, run 'AmazonS3Client.listMultipartUploads', and receive a 'MultipartUploadListing' instance with in-progress uploads information."
  },
  {
    "question": "How can you list all in-progress multipart uploads in an Amazon S3 bucket using the AWS SDK for PHP?",
    "answer": "You can list all in-progress multipart uploads in a bucket by calling the 'listMultipartUploads' method on the S3Client instance and providing the bucket name."
  },
  {
    "question": "What PHP class is used to interact with Amazon S3 for multipart upload operations?",
    "answer": "The Aws\\S3\\S3Client class is used to interact with Amazon S3 for multipart upload operations in PHP."
  },
  {
    "question": "What is the role of a ProgressListener in high-level multipart uploads?",
    "answer": "The ProgressListener interface provides periodic progress events, notifying listeners about the number of bytes transferred during an upload."
  },
  {
    "question": "What Java class is used to manage file transfers and track multipart upload progress in S3?",
    "answer": "The TransferManager class is used to manage file transfers and track multipart upload progress in Amazon S3."
  },
  {
    "question": "Which event handler is needed in Java to receive notifications when bytes are transferred during a multipart upload?",
    "answer": "A ProgressListener must be set, and its 'progressChanged' method receives notifications with ProgressEvent indicating transferred bytes."
  },
  {
    "question": "How do you wait for a multipart upload to finish in Java using the TransferManager API?",
    "answer": "Call the 'waitForCompletion()' method on the Upload object returned by TransferManager.upload()."
  },
  {
    "question": "What exception is caught in Java to handle errors during multipart upload, and what message is printed?",
    "answer": "The AmazonClientException is caught, and the message 'Unable to upload file, upload aborted.' is printed along with the stack trace."
  },
  {
    "question": "How do you track upload progress in .NET when using the TransferUtility class?",
    "answer": "You create a TransferUtilityUploadRequest, subscribe to the UploadProgressEvent, and handle progress updates in an event handler."
  },
  {
    "question": "In the C# example, what method is subscribed to for progress updates during upload?",
    "answer": "The method 'uploadRequest_UploadPartProgressEvent' is subscribed to the UploadProgressEvent for progress updates."
  },
  {
    "question": "What happens to uploaded parts if you do not complete a multipart upload in Amazon S3?",
    "answer": "Amazon S3 does not assemble the object and any uploaded parts remain stored, incurring storage charges until they are removed or the upload is aborted."
  },
  {
    "question": "How can you ensure leftover uploaded parts from incomplete uploads are cleaned up automatically?",
    "answer": "Configure a bucket lifecycle configuration to delete incomplete multipart uploads after a specified period."
  },
  {
    "question": "Which methods can be used to abort a multipart upload in Amazon S3?",
    "answer": "You can use the AWS CLI, the REST API (AbortMultipartUpload operation), or SDK-specific methods such as abortMultipartUploads in TransferManager (Java) or AbortMultipartUploadsAsync in TransferUtility (.NET)."
  },
  {
    "question": "How do you abort all multipart uploads initiated before a given date using Java's high-level API?",
    "answer": "Instantiate TransferManager and call 'abortMultipartUploads' with the bucket name and the Date object representing the cutoff."
  },
  {
    "question": "How can you abort multipart uploads in .NET that started over a week ago?",
    "answer": "Use TransferUtility's AbortMultipartUploadsAsync method, passing the bucket name and a DateTime specifying seven days ago."
  },
  {
    "question": "After aborting a multipart upload, can you use the same upload ID to continue uploading more parts?",
    "answer": "No, once a multipart upload is aborted, the upload ID cannot be used to upload additional parts."
  },
  {
    "question": "What parameters are required to abort a specific multipart upload using the SDKs?",
    "answer": "You must provide the bucket name, key name, and the upload ID associated with the multipart upload."
  },
  {
    "question": "How does the AWS SDK for PHP abort a multipart upload?",
    "answer": "Call the 'abortMultipartUpload' method on the S3Client, providing the bucket name, object key, and upload ID."
  },
  {
    "question": "What REST API call is used to abort a multipart upload?",
    "answer": "The AbortMultipartUpload operation in the Amazon Simple Storage Service REST API is used to abort a multipart upload."
  },
  {
    "question": "What AWS CLI command can you use to abort a multipart upload?",
    "answer": "You can use the 'abort-multipart-upload' command in the AWS CLI to abort a multipart upload."
  },
  {
    "question": "Why is it recommended to always complete or abort a multipart upload that was started?",
    "answer": "Because you are billed for the storage consumed by all uploaded parts, and failing to complete or abort leaves unused data in storage, incurring charges."
  },
  {
    "question": "What should you consult for more information about uploading and pricing in Amazon S3?",
    "answer": "Refer to the section 'Multipart upload and pricing' in the Amazon Simple Storage Service User Guide."
  },
  {
    "question": "What does the Java method s3Client.abortMultipartUpload do?",
    "answer": "It deletes all uploaded parts corresponding to the specified upload ID, bucket name, and key name, aborting the multipart upload."
  },
  {
    "question": "According to the manual, what is the best way to stop old incomplete multipart uploads programmatically?",
    "answer": "Use the high-level SDK APIs (such as TransferManager in Java or TransferUtility in .NET) to abort all uploads initiated before a given time."
  },
  {
    "question": "What is the purpose of Amazon S3 multipart upload?",
    "answer": "Multipart upload in Amazon S3 allows uploading large objects as a set of parts, enabling efficient and reliable upload operations, especially for files over 100 MB in size."
  },
  {
    "question": "How can you copy an S3 object larger than 5 GB using multipart upload?",
    "answer": "To copy an S3 object larger than 5 GB, use the multipart upload API to break the object into parts, copy each part with the Upload Part (Copy) API, and combine them at the destination, specifying the x-amz-copy-source header."
  },
  {
    "question": "What are the main REST API operations involved in a multipart upload?",
    "answer": "The main REST API operations include: Initiate Multipart Upload, Upload Part, Upload Part (Copy), Complete Multipart Upload, Abort Multipart Upload, List Parts, and List Multipart Uploads."
  },
  {
    "question": "Which header is used in REST API to specify the source object for copying during multipart upload?",
    "answer": "The x-amz-copy-source request header is used to specify the source object when copying with multipart upload."
  },
  {
    "question": "What steps are involved in copying an object with Amazon S3's Java SDK using the low-level API?",
    "answer": "Steps: Initiate the multipart upload, save the upload ID, copy all parts with CopyPartRequest, store each part's ETag and number, and then complete the multipart upload using all collected ETags."
  },
  {
    "question": "Why do you need to keep track of ETags during a multipart upload?",
    "answer": "ETags are used to uniquely identify each part uploaded; they are required to assemble the complete object during the CompleteMultipartUpload step."
  },
  {
    "question": "In the Java code example, what does the getETags function do?",
    "answer": "The getETags function constructs and returns a list of PartETag objects for all the uploaded parts, each initialized with the part number and its corresponding ETag."
  },
  {
    "question": "How is error handling implemented in the Java Amazon S3 multipart copy example?",
    "answer": "Error handling is implemented by catching AmazonServiceException for AWS errors, and SdkClientException for client-side issues or communication problems."
  },
  {
    "question": "Describe the overall flow of the .NET C# example for multipart copy.",
    "answer": "The flow is: initiate the upload, get the source object's metadata and size, copy the object in 5MB chunks using CopyPartRequest, store ETags, and call CompleteMultipartUpload with collected ETags."
  },
  {
    "question": "In the .NET example, how is the last byte of each part determined?",
    "answer": "The last byte is set using: LastByte = (bytePosition + partSize - 1 >= objectSize) ? objectSize - 1 : bytePosition + partSize - 1, ensuring it does not exceed the object size."
  },
  {
    "question": "Why is it important to check if the last byte exceeds object size during part copying?",
    "answer": "To avoid trying to copy beyond the actual data of the object and ensure the final part is the exact remainder."
  },
  {
    "question": "When should you consider using multipart upload in Amazon S3 according to the tutorial?",
    "answer": "You should consider multipart upload when an object's size exceeds 100 MB."
  },
  {
    "question": "What are the durability best practices for verifying files uploaded to S3?",
    "answer": "Best practices include using checksums (SHA-1, SHA-256, CRC32, or CRC32C) to verify that every byte is transferred without alteration."
  },
  {
    "question": "Which algorithms does Amazon S3 support for checksums?",
    "answer": "Amazon S3 supports SHA-1, SHA-256, CRC32, and CRC32C algorithms for checksums."
  },
  {
    "question": "What steps are involved in the multipart upload integrity verification tutorial?",
    "answer": "Steps: create a large file, split the file, create multipart upload with checksum, upload the parts, list all parts, complete the upload, confirm the object's presence, verify integrity with MD5 and additional checksums, and clean up resources."
  },
  {
    "question": "What is required before starting the multipart upload and integrity verification tutorial?",
    "answer": "You need an S3 bucket to upload to, and the AWS CLI installed and configured (or use AWS CloudShell as an alternative)."
  },
  {
    "question": "How can you create a 15 MB file on Linux or macOS for multipart upload testing?",
    "answer": "Run: dd if=/dev/urandom of=census-data.bin bs=1M count=15, to create a 15 MB binary file filled with random data."
  },
  {
    "question": "How do you create a 15 MB file on Windows for testing multipart upload?",
    "answer": "Run: fsutil file createnew census-data.bin 15728640, which creates a 15 MB file with arbitrary data."
  },
  {
    "question": "Why do you need to split a large file before a multipart upload?",
    "answer": "Splitting enables uploading different parts independently as required by the multipart upload process."
  },
  {
    "question": "How do you split a file into 5 MB chunks on Linux or macOS?",
    "answer": "Run: split -b 5M -d census-data.bin census-part, which will split census-data.bin into 5 MB files with numeric suffixes."
  },
  {
    "question": "How can you split a file into 5 MB parts using PowerShell on Windows?",
    "answer": "Use a script that reads 5MB chunks from the source file and writes them to new files with incrementing numeric suffixes."
  },
  {
    "question": "What should you do if any part of a multipart upload fails to transfer?",
    "answer": "Retransmit only the failed part; successfully transferred parts will not be affected."
  },
  {
    "question": "How does Amazon S3 assemble a multipart-uploaded object?",
    "answer": "After successfully uploading all parts, S3 assembles them in order to create the final complete object."
  },
  {
    "question": "What happens if you leave a multipart upload incomplete in Amazon S3?",
    "answer": "Uploaded parts remain stored and incur charges until you either complete the upload or abort it to remove the parts."
  },
  {
    "question": "What is the purpose of a PowerShell script that reads a file in 5 MB chunks for Amazon S3 multipart upload?",
    "answer": "It splits a large file into 5 MB chunks, saving each as a separate file so they can be uploaded as parts in a multipart upload process."
  },
  {
    "question": "After executing a file splitting PowerShell script, how will the created file parts be named?",
    "answer": "Each part will have a numeric suffix, such as census-part00, census-part01, census-part02, and so on."
  },
  {
    "question": "What is the first step to create a multipart upload with checksum verification in AWS CLI?",
    "answer": "Use the create-multipart-upload command, specifying the bucket, key, and checksum algorithm such as SHA-256."
  },
  {
    "question": "What information is returned by Amazon S3 after initiating a multipart upload?",
    "answer": "Amazon S3 returns a JSON response including ServerSideEncryption, ChecksumAlgorithm, Bucket, Key, and a unique UploadId."
  },
  {
    "question": "Why is it important to keep the upload ID during a multipart upload process?",
    "answer": "The upload ID uniquely identifies the multipart upload and is required for uploading parts, listing parts, completing, or aborting the upload."
  },
  {
    "question": "What can happen if you use nonconsecutive part numbers when uploading parts with additional checksums in S3?",
    "answer": "The complete-multipart-upload request may result in an HTTP 500 Internal Server Error."
  },
  {
    "question": "Which AWS CLI command is used to upload a part in a multipart upload?",
    "answer": "aws s3api upload-part is used, specifying --bucket, --key, --part-number, --body, --upload-id, and --checksum-algorithm."
  },
  {
    "question": "What response do you receive after successfully uploading a part in S3 with checksum verification?",
    "answer": "You receive a JSON output including ServerSideEncryption, ETag, and ChecksumSHA256."
  },
  {
    "question": "How do you determine which files are associated with each part number during multipart upload?",
    "answer": "The file parts are named with a numeric suffix matching their part number, such as census-part00 for part 1."
  },
  {
    "question": "How do you create a JSON file with the details of all uploaded parts in AWS CLI?",
    "answer": "Use the list-parts command with --query to select PartNumber, ETag, and ChecksumSHA256, outputting to a file such as parts.json."
  },
  {
    "question": "What is the purpose of the parts.json file in the multipart upload process?",
    "answer": "It contains the part numbers and their corresponding ETag and checksum values needed to complete the multipart upload."
  },
  {
    "question": "Which AWS CLI command completes a multipart upload using the collected parts information?",
    "answer": "aws s3api complete-multipart-upload with --multipart-upload file://parts.json, --bucket, --key, and --upload-id."
  },
  {
    "question": "What should you NOT do immediately after completing a multipart upload and why?",
    "answer": "You should not delete the individual part files before verifying checksums to ensure the integrity of the merged object."
  },
  {
    "question": "How can you confirm that an uploaded object exists in your S3 bucket?",
    "answer": "Use the aws s3api list-objects-v2 command and check for the file (key) in the returned list."
  },
  {
    "question": "Why is the ETag of a multipart-uploaded object followed by a dash and a number, like ETag: \"...-3\"?",
    "answer": "The dash and number indicate that the object was uploaded in multiple parts (the number equals the part count)."
  },
  {
    "question": "How do you calculate the MD5 checksum for each part file on Linux/macOS?",
    "answer": "By running md5sum census-part* in the terminal, which prints an MD5 hash for each file."
  },
  {
    "question": "How do you verify that the multipart object in S3 has not been altered during upload, using MD5?",
    "answer": "Combine all part MD5 hashes into one string, convert the string to binary and calculate its MD5 hash. It should match the ETag shown by S3."
  },
  {
    "question": "What CLI command can retrieve the ETag of an S3 object for integrity checking?",
    "answer": "aws s3api head-object --bucket <bucket-name> --key <file-key>"
  },
  {
    "question": "Why is the ETag not a checksum for the entire object after multipart upload?",
    "answer": "Because it is a composite of the individual part checksums used to assemble the final object in S3."
  },
  {
    "question": "How can you view the ChecksumSHA256 value of a multipart-uploaded object with AWS CLI?",
    "answer": "By running aws s3api head-object --bucket <bucket> --key <key> --checksum-mode enabled."
  },
  {
    "question": "How do you manually validate the SHA256 checksum of a multipart-uploaded object using base64 and sha256sum?",
    "answer": "Decode each part's base64 ChecksumSHA256 value to binary, concatenate, run sha256sum on the outfile, and convert the result to base64 to compare to the S3-reported ChecksumSHA256."
  },
  {
    "question": "What must you do if the SHA256 checksum validation does not match when verifying integrity?",
    "answer": "Suspect data corruption or transfer error; the object should not be trusted unless the checksums match."
  },
  {
    "question": "What happens if you upload a new part with the same part number as an existing uploaded part?",
    "answer": "The previously uploaded part is overwritten in S3 with the new data."
  },
  {
    "question": "How can you retrieve the checksum values for the individual parts of a multipart upload still in progress?",
    "answer": "Use the list-parts command for your multipart upload ID."
  },
  {
    "question": "What command can be used to clean up all local files created during a multipart upload demo?",
    "answer": "rm census-data.bin census-part* outfile parts.json"
  },
  {
    "question": "At what object size does Amazon recommend considering multipart upload rather than a single operation?",
    "answer": "When your object size reaches 100 MB."
  },
  {
    "question": "Does Amazon S3 have a minimum size requirement for the last part of a multipart upload?",
    "answer": "No, there is no minimum size for the last part, but all other parts except the last must be at least 5 MB."
  },
  {
    "question": "What is the maximum object size supported by Amazon S3 for an individual object?",
    "answer": "The maximum object size supported by Amazon S3 for an individual object is 5 TiB."
  },
  {
    "question": "What is the maximum number of parts you can use in a single Amazon S3 multipart upload?",
    "answer": "The maximum number of parts per multipart upload is 10,000."
  },
  {
    "question": "What is the allowed size range for each part in an Amazon S3 multipart upload?",
    "answer": "Each part must be between 5 MiB and 5 GiB, except the last part, which has no minimum size limit."
  },
  {
    "question": "How many parts can Amazon S3 return in response to a list parts request?",
    "answer": "Amazon S3 can return up to 1,000 parts in a single list parts request."
  },
  {
    "question": "What is a conditional request in Amazon S3?",
    "answer": "A conditional request in Amazon S3 is an API operation with an additional header specifying a condition. If the condition is not met, the S3 operation fails."
  },
  {
    "question": "For which request types does Amazon S3 support conditional reads?",
    "answer": "Conditional reads are supported for GET, HEAD, and COPY requests."
  },
  {
    "question": "Give an example of how object metadata can be used in a conditional read operation.",
    "answer": "You can use ETag or Last-Modified metadata to return or copy an object only if it matches a certain version or modification date."
  },
  {
    "question": "What is a common use of conditional writes in Amazon S3?",
    "answer": "A common use is to ensure there is no existing object with the same key name before uploading, preventing unintentional overwrites."
  },
  {
    "question": "What precondition can be added to a PUT request to prevent overwrite if the object already exists?",
    "answer": "Add the If-None-Match header with a value of * to the PUT request."
  },
  {
    "question": "What Amazon S3 APIs support using conditional reads with headers?",
    "answer": "GetObject, HeadObject, and CopyObject APIs support conditional reads."
  },
  {
    "question": "Which headers can you use with GetObject and HeadObject for conditional reads?",
    "answer": "If-Match, If-Modified-Since, If-None-Match, and If-Unmodified-Since."
  },
  {
    "question": "Which headers are available to CopyObject for conditional copying based on ETag or Last-Modified?",
    "answer": "x-amz-copy-source-if-match, x-amz-copy-source-if-modified-since, x-amz-copy-source-if-none-match, x-amz-copy-source-if-unmodified-since."
  },
  {
    "question": "What is the behavior of the If-None-Match header with PutObject?",
    "answer": "The write operation is successful only if there is no existing object with the same key name; otherwise, it fails with a 412 Precondition Failed response."
  },
  {
    "question": "What permission is required for callers to perform a conditional write with If-None-Match?",
    "answer": "The s3:PutObject permission."
  },
  {
    "question": "How do you perform a conditional write using the AWS CLI to check if the file does not already exist?",
    "answer": "Use --if-none-match '*' as a parameter in the aws s3api put-object command."
  },
  {
    "question": "Which header allows you to ensure an object's content has not changed before overwriting it in S3?",
    "answer": "The If-Match header with the expected ETag value."
  },
  {
    "question": "To use the If-Match header in a PUT operation, what permissions do you need?",
    "answer": "s3:PutObject and s3:GetObject permissions."
  },
  {
    "question": "What AWS CLI command demonstrates a conditional write requiring the object's ETag to match a given value?",
    "answer": "aws s3api put-object --bucket <bucket> --key <key> --body <file> --if-match \"<etag>\""
  },
  {
    "question": "What is the response if multiple concurrent PUT operations use If-None-Match and the key does not exist at first?",
    "answer": "The first completed write operation succeeds; others fail with 412 Precondition Failed."
  },
  {
    "question": "How does Amazon S3 handle a delete request that is received concurrently with conditional write operations?",
    "answer": "The delete takes precedence. Conditional writes may receive a 409 Conflict or 404 Not Found response."
  },
  {
    "question": "What response does S3 give if a conditional PUT with If-None-Match is made on a versioned bucket with a delete marker?",
    "answer": "The write operation succeeds."
  },
  {
    "question": "What status code is returned if a conditional PUT with a non-matching ETag is attempted with If-Match?",
    "answer": "412 Precondition Failed."
  },
  {
    "question": "What is the effect of a failed multipart upload due to a concurrent delete, as described in the diagram on page 8?",
    "answer": "You must initiate a new multipart upload to proceed."
  },
  {
    "question": "How can you minimize costs associated with incomplete multipart uploads?",
    "answer": "Configure a bucket lifecycle rule using AbortIncompleteMultipartUpload to delete incomplete multipart uploads after a certain number of days."
  },
  {
    "question": "How can you enforce conditional write policies on an S3 bucket?",
    "answer": "By configuring a bucket policy using condition keys s3:if-match or s3:if-none-match."
  },
  {
    "question": "What additional condition key is required in bucket policies to support multipart upload exemptions?",
    "answer": "s3:ObjectCreationOperation must be specified to exempt CreateMultipartUpload, UploadPart, and UploadPartCopy from conditional header requirements."
  },
  {
    "question": "What is the impact on CopyObject operations if you enforce conditional write headers through a bucket policy?",
    "answer": "CopyObject requests without the required headers (If-None-Match or If-Match) fail with 403 Access Denied, and those with the headers fail with 501 Not Implemented."
  },
  {
    "question": "What approach is shown in Example 1 in the document for enforcing conditional writes?",
    "answer": "Allowing uploads using PutObject and CompleteMultipartUpload only if the if-none-match header is present, ensuring the key is new."
  },
  {
    "question": "Who can associate a policy with a bucket to enforce conditional writes?",
    "answer": "Only the bucket owner."
  },
  {
    "question": "What IAM policy condition key prevents object overwrite unless s3:if-none-match is present?",
    "answer": "The condition key 's3:if-none-match' must be set to false to allow object creation only if that header is present."
  },
  {
    "question": "In the given policy, who is allowed to upload objects to the bucket?",
    "answer": "Only the AWS IAM user 'Alice' in account 111122223333 is allowed, as specified by the 'Principal' field."
  },
  {
    "question": "When does the 'AllowConditionalPutwithMPUs' statement let Alice upload objects?",
    "answer": "Only if 's3:ObjectCreationOperation' is false, allowing multipart uploads with their own conditions."
  },
  {
    "question": "What header must be present in PutObject or CompleteMultipartUpload for uploads to succeed, as in Example 2?",
    "answer": "The 'if-match' header must be present, and its ETag value must match the object's ETag."
  },
  {
    "question": "What does the 's3:ObjectCreationOperation' condition key enable in policies?",
    "answer": "It lets multipart uploads via CreateMultipartUpload, UploadPart, and UploadPartCopy bypass certain conditional header requirements."
  },
  {
    "question": "What is the role of the 'AllowPutObject' statement in the shown policy?",
    "answer": "It allows Alice to perform s3:PutObject on the bucket, granting basic upload privileges."
  },
  {
    "question": "How does the 'BlockNonConditionalObjectCreation' statement enforce conditional writes?",
    "answer": "It denies PutObject operations unless both 's3:if-match' is true and 's3:ObjectCreationOperation' is true, forcing conditional headers on single PUTs."
  },
  {
    "question": "What principal is specified in the given policy statements?",
    "answer": "The user Alice in account 111122223333 ('arn:aws:iam::111122223333:user/Alice')."
  },
  {
    "question": "When can Alice overwrite an existing object using the policy in Example 3?",
    "answer": "If the 'if-match' header matches the current ETag of the object or if the 'if-none-match' header is present and the key does not exist."
  },
  {
    "question": "What is the general purpose of using 'if-match' or 'if-none-match' headers in S3 upload requests?",
    "answer": "To ensure that writes are conditional: to avoid overwriting existing objects unintentionally or only overwrite if the ETag matches."
  },
  {
    "question": "What Amazon S3 operation allows you to duplicate or rename an object?",
    "answer": "The CopyObject operation, which creates a new copy of an object, optionally in a different bucket or with a new key."
  },
  {
    "question": "Do you need multipart upload for copying objects larger than 5 GB?",
    "answer": "Yes, objects larger than 5 GB must be copied with multipart upload using AWS CLI or SDKs."
  },
  {
    "question": "List three things you can do using the CopyObject operation.",
    "answer": "You can create copies, rename objects (by copying and deleting the original), and move objects between buckets or AWS Regions."
  },
  {
    "question": "How do you change an S3 object's metadata after upload?",
    "answer": "Copy the object onto itself with new metadata settings, as metadata can't be changed in place."
  },
  {
    "question": "If you copy an object, what happens to user-set and system metadata?",
    "answer": "User-controlled metadata is copied; system-controlled metadata like creation date is reset."
  },
  {
    "question": "What is the impact of network modifications on metadata when copying via the S3 console?",
    "answer": "Network or browser modifications may change headers, leading to unintended metadata in the copied object."
  },
  {
    "question": "How do you copy an object stored in S3 Glacier Flexible Retrieval using the console?",
    "answer": "You cannot use the console. You must restore it first, then copy it using AWS CLI, SDK, or REST API."
  },
  {
    "question": "What level of encryption is set by default for new copied objects in S3?",
    "answer": "Server-side encryption with Amazon S3 managed keys (SSE-S3)."
  },
  {
    "question": "How do you copy an S3 object with a specific type of encryption?",
    "answer": "Specify the desired encryption (SSE-S3, SSE-KMS, or SSE-C) in the copy request, which overrides the target bucket's default."
  },
  {
    "question": "What is the default checksum for copied S3 objects without a user-specified algorithm?",
    "answer": "CRC-64NVME checksum is set by default."
  },
  {
    "question": "How can you copy multiple S3 objects in one job?",
    "answer": "Use S3 Batch Operations with a list of objects and a defined operation, which can affect billions of objects in one job."
  },
  {
    "question": "Name one restriction when copying objects using the S3 console.",
    "answer": "Objects must be less than 5 GB; larger objects require AWS CLI/SDK and multipart upload."
  },
  {
    "question": "Can you copy objects with customer-managed encryption keys (SSE-C) using the S3 console?",
    "answer": "No, you must use AWS CLI, SDK, or REST API."
  },
  {
    "question": "What happens to Object Lock settings when copying objects in S3?",
    "answer": "Object Lock settings are not retained in the copied objects."
  },
  {
    "question": "What must you do if copying between buckets with enforced Object Ownership settings?",
    "answer": "Ensure both buckets have the same enforced setting, or remove object ACL grants to other AWS accounts."
  },
  {
    "question": "List the steps to copy an object using the AWS Management Console.",
    "answer": "1. Open S3 Console, 2. Select bucket and object, 3. Choose Copy from Actions, 4. Configure destination, 5. Adjust copy settings, 6. Confirm copy."
  },
  {
    "question": "What should you do when copying objects if bucket versioning is disabled?",
    "answer": "Enable bucket versioning to protect against accidental overwrites or deletions."
  },
  {
    "question": "How can you copy objects using AWS SDKs?",
    "answer": "Use the copyObject method (e.g., in SDK for Java) to copy up to 5 GB in a single operation; use multipart upload for larger files."
  },
  {
    "question": "Show a code example of instantiating and using an S3 CopyObjectRequest in Java.",
    "answer": "CopyObjectRequest copyObjRequest = new CopyObjectRequest(bucketName, sourceKey, bucketName, destinationKey); s3Client.copyObject(copyObjRequest);"
  },
  {
    "question": "What is the maximum object size that the .NET CopyObject example can copy in a single operation?",
    "answer": "The maximum size is 5 GB for a single CopyObject operation in the .NET example."
  },
  {
    "question": "What should you use to copy S3 objects larger than 5 GB?",
    "answer": "You should use the multipart upload copy example for objects larger than 5 GB."
  },
  {
    "question": "What are the four pieces of information you must configure in the .NET CopyObject example before executing?",
    "answer": "Source bucket name, destination bucket name, source object key, and destination object key."
  },
  {
    "question": "What is the method used to copy the object asynchronously in the .NET example?",
    "answer": "The method is CopyingObjectAsync()."
  },
  {
    "question": "In the .NET example, what happens if an AmazonS3Exception occurs during copy?",
    "answer": "An error message including the exception is printed to the console."
  },
  {
    "question": "Which PHP AWS SDK class is used to create an S3 client for object copy operations?",
    "answer": "The Aws\\S3\\S3Client class."
  },
  {
    "question": "How do you make multiple copies of an object in the PHP AWS SDK example?",
    "answer": "By creating a batch of CopyObject commands and executing them with CommandPool::batch."
  },
  {
    "question": "What does the php copyObject() method do?",
    "answer": "It copies a single object from a source key in a bucket to a target key in a destination bucket in S3."
  },
  {
    "question": "How can you handle AWS-specific errors in the PHP command batch?",
    "answer": "By checking if the result is an instance of AwsException and handling it accordingly."
  },
  {
    "question": "In the Python example, what AWS SDK is used to interact with S3 objects?",
    "answer": "The boto3 SDK is used."
  },
  {
    "question": "Explain briefly what the ObjectWrapper class in the Python example does.",
    "answer": "It encapsulates S3 object actions and provides a copy method to copy the object to another bucket."
  },
  {
    "question": "When using copy_from in boto3, what parameter specifies the source to copy from?",
    "answer": "The CopySource dictionary specifies the source bucket and key."
  },
  {
    "question": "What happens after dest_object.copy_from in the Python example?",
    "answer": "The method waits until the new object exists and logs that the copy succeeded."
  },
  {
    "question": "In the Ruby AWS SDK, which method is used to copy an object within buckets?",
    "answer": "The #copy_to method is used."
  },
  {
    "question": "How do you handle errors in a Ruby S3 copy operation?",
    "answer": "By rescuing Aws::Errors::ServiceError and printing the error message."
  },
  {
    "question": "What does the Ruby example print when a copy is successful?",
    "answer": "It prints a message indicating the source file has been copied to the target bucket and key."
  },
  {
    "question": "What REST API HTTP method is used to copy an object in S3?",
    "answer": "The PUT method is used to copy an object."
  },
  {
    "question": "Which header specifies the source object when using PUT for S3 object copy via REST API?",
    "answer": "The x-amz-copy-source header."
  },
  {
    "question": "In the REST API example, what important information does Amazon S3 return after copying?",
    "answer": "It returns the ETag and LastModified information in the XML response."
  },
  {
    "question": "What AWS CLI command is referenced for copying objects in S3?",
    "answer": "The aws s3api copy-object command."
  },
  {
    "question": "How can you move an object in S3 using the Console?",
    "answer": "Select your object, use the Actions menu to choose Move, specify the destination path, and confirm."
  },
  {
    "question": "Can you use the S3 console to move objects larger than 5 GB?",
    "answer": "No, for objects over 5 GB you must use the AWS CLI or SDKs."
  },
  {
    "question": "What must not be used for source or destination when moving objects with aliases?",
    "answer": "S3 access point aliases must not be used as source or destination in the Amazon S3 console."
  },
  {
    "question": "Why does the console warn about bucket versioning during a move operation?",
    "answer": "To help protect against unintentionally overwriting or deleting objects."
  },
  {
    "question": "When moving an object, what three copy settings options are available?",
    "answer": "Copy source settings, Don’t specify settings, Specify settings."
  },
  {
    "question": "After moving an object in the console, what happens to the object in the source location?",
    "answer": "Amazon S3 moves your objects to the destination, removing them from the source."
  },
  {
    "question": "How do you move an object using the AWS CLI?",
    "answer": "Use the aws s3 mv command."
  },
  {
    "question": "Can you rename objects with the S3 console if they are encrypted with SSE-C?",
    "answer": "No, to rename SSE-C encrypted objects, use the AWS CLI, SDKs, or REST API."
  },
  {
    "question": "What happens under the hood when you rename an S3 object?",
    "answer": "S3 creates a copy with the new name, assigns a new last-modified date, and adds a delete marker to the original."
  },
  {
    "question": "Does default encryption apply automatically to an unencrypted object that is renamed?",
    "answer": "Yes, default encryption settings are automatically applied to the new object."
  },
  {
    "question": "What happens to object ACLs when renaming objects in buckets with Object Ownership enforcement?",
    "answer": "Object ACLs will not be copied."
  },
  {
    "question": "List the steps to rename an object via the S3 Console.",
    "answer": "1. Open S3 console. 2. Select the object. 3. Choose Rename object. 4. Enter a new name. 5. Select copy settings. 6. Save changes."
  },
  {
    "question": "What is the maximum size for a single object you can store in Amazon S3?",
    "answer": "A single object in Amazon S3 can be up to 5 TB in size."
  },
  {
    "question": "Can you access S3 archived objects in real time?",
    "answer": "No, archived objects must first be restored before they can be downloaded."
  },
  {
    "question": "Name four methods to download a single object from S3.",
    "answer": "Amazon S3 Console, AWS Command Line Interface (CLI), AWS SDKs, Amazon S3 REST API."
  },
  {
    "question": "If you want to download multiple objects at once from S3, which tools can you use?",
    "answer": "AWS CloudShell, AWS CLI, AWS SDKs."
  },
  {
    "question": "How can you download only part of an S3 object?",
    "answer": "Use extra parameters with the AWS CLI or REST API to specify the specific byte range you want to download."
  },
  {
    "question": "What is a presigned URL in the context of S3 downloads?",
    "answer": "It is a time-limited URL generated by the object owner that allows downloads of objects by others."
  },
  {
    "question": "When do data transfer fees apply during S3 downloads?",
    "answer": "Fees apply when downloading objects outside of the AWS network."
  },
  {
    "question": "If you download an object using the S3 console and its object key ends with a period, what happens?",
    "answer": "The period is removed from the downloaded object's name; to preserve it, use the CLI, SDK, or API."
  },
  {
    "question": "How can you download a specific version of an object in the S3 console?",
    "answer": "Enable 'Show versions', select the version, and choose 'Download'."
  },
  {
    "question": "Which AWS CLI command downloads a single object from S3?",
    "answer": "aws s3api get-object --bucket <bucket> --key <key> <outfile>"
  },
  {
    "question": "Why might you use the 'sync' command in AWS CloudShell for downloads?",
    "answer": "To synchronize all objects in your S3 bucket to your CloudShell directory."
  },
  {
    "question": "What is one limitation of using AWS CloudShell with S3 sync?",
    "answer": "CloudShell has a storage limit of 1 GB per AWS Region."
  },
  {
    "question": "How do you download an entire S3 bucket recursively using the AWS CLI?",
    "answer": "aws s3 cp s3://<bucket> . --recursive"
  },
  {
    "question": "How can you filter the files to be downloaded using AWS CLI from S3?",
    "answer": "Use the --exclude and --include parameters to match specific files."
  },
  {
    "question": "Explain how to download a partial S3 object using the AWS CLI.",
    "answer": "Use the --range parameter with aws s3api get-object to specify start and end bytes."
  },
  {
    "question": "Can S3 REST API retrieve multiple ranges in a single GET request?",
    "answer": "No, Amazon S3 doesn't support retrieving multiple ranges in a single GET request."
  },
  {
    "question": "How do you generate a presigned download URL for an S3 object using the console?",
    "answer": "On the object, choose 'Share with a presigned URL', set expiry, and copy the generated URL."
  },
  {
    "question": "What is the maximum expiration time for an S3 presigned URL generated in the console?",
    "answer": "12 hours from the time of creation."
  },
  {
    "question": "Before downloading an archived S3 object, what must you do?",
    "answer": "First restore the archived object."
  },
  {
    "question": "What is a conditional read request in S3?",
    "answer": "A request that returns an object only if it meets certain metadata conditions, such as ETag or last-modified date."
  },
  {
    "question": "A 403 Access Denied error when downloading S3 objects is often caused by what?",
    "answer": "Insufficient permissions or IAM policy issues."
  },
  {
    "question": "If you get a 404 NoSuchKey error when downloading from S3, what does it mean?",
    "answer": "The object does not exist at the specified key."
  },
  {
    "question": "What primary mechanism does S3 use to verify data integrity during uploads and downloads?",
    "answer": "Checksum values."
  },
  {
    "question": "What is the default checksum algorithm used by Amazon S3 if none is specified?",
    "answer": "CRC-64/NVME (CRC64NVME)."
  },
  {
    "question": "Name three other checksum algorithms supported by Amazon S3.",
    "answer": "CRC-32C (CRC32C), SHA-1 (SHA1), SHA-256 (SHA256)."
  },
  {
    "question": "What happens if the uploaded object checksum and provided checksum do not match in S3?",
    "answer": "Amazon S3 generates an error and does not store the object."
  },
  {
    "question": "What is the difference between a full object and a composite checksum in S3?",
    "answer": "A full object checksum covers all data in an object; a composite checksum aggregates the checksums of each part in a multipart upload."
  },
  {
    "question": "If you use multipart upload, what might the object's ETag in S3 represent?",
    "answer": "A composite MD5 digest of all part digests, not a simple MD5 of the object."
  },
  {
    "question": "What happens if you try to complete a multipart upload with missing or non-sequential part numbers?",
    "answer": "Amazon S3 generates an HTTP 500 Internal Server error."
  },
  {
    "question": "Which checksum algorithms does Amazon S3 support for full object and composite checksums?",
    "answer": "CRC-64/NVME (CRC64NVME), CRC-32 (CRC32), CRC-32C (CRC32C), SHA-1 (SHA1), and SHA-256 (SHA256)."
  },
  {
    "question": "What must you use when uploading a multipart object with SHA-1 or SHA-256 in Amazon S3?",
    "answer": "You must use the composite algorithm type for SHA-1 or SHA-256 in multipart uploads."
  },
  {
    "question": "When using single part upload with SHA-1 or SHA-256 in S3, which checksum type is supported?",
    "answer": "Only the full object checksum type is supported."
  },
  {
    "question": "Where can you specify the checksum algorithm during upload using AWS SDKs?",
    "answer": "You can specify the checksum algorithm in the upload or multipart upload requests, such as PutObject or MultipartUpload API."
  },
  {
    "question": "What does Amazon S3 do if you upload an object without providing a checksum?",
    "answer": "Amazon S3 automatically attaches the recommended full object CRC-64/NVME (CRC64NVME) checksum algorithm."
  },
  {
    "question": "Which checksum algorithms are available for full object checksums in multipart upload?",
    "answer": "CRC-64/NVME (CRC64NVME), CRC-32 (CRC32), and CRC-32C (CRC32C)."
  },
  {
    "question": "Can SHA-1 and SHA-256 be used for full object checksums in multipart uploads?",
    "answer": "No, they can only be used as composite checksums in multipart uploads."
  },
  {
    "question": "What error does Amazon S3 return if provided and computed checksum values do not match?",
    "answer": "Amazon S3 fails the request with a BadDigest error."
  },
  {
    "question": "How can Amazon S3 use part-level checksums to validate full object integrity?",
    "answer": "S3 uses stored part-level checksums to calculate and compare the full object checksum internally."
  },
  {
    "question": "What information can GetObjectAttributes return about multipart objects?",
    "answer": "It can return the number of parts, their sizes, and each part's checksum value."
  },
  {
    "question": "How can you obtain checksums for completed object parts post-upload?",
    "answer": "By using GetObject or HeadObject with a part number or aligned byte range."
  },
  {
    "question": "What AWS CLI command lists the uploaded parts of an in-progress multipart upload?",
    "answer": "aws s3api list-parts --bucket <bucket> --key <key> --upload-id <upload_id>"
  },
  {
    "question": "What happens to a multipart-uploaded object's checksum value if you copy it?",
    "answer": "The checksum changes, even if the data does not, because S3 computes it over the whole object upon copy."
  },
  {
    "question": "If you use the S3 Console for copy and the object is larger than 16 MB, which upload mode does it use?",
    "answer": "The S3 Console uses multipart upload."
  },
  {
    "question": "What Java AWS SDK class is used to build the S3 client for multipart upload and validation?",
    "answer": "S3Client from software.amazon.awssdk.services.s3."
  },
  {
    "question": "What Java SDK method uploads a file in parts with SHA-256 checksum validation?",
    "answer": "You split the file, upload each part with UploadPartRequest, and track checksums using MessageDigest."
  },
  {
    "question": "What is the role of the CompleteMultipartUploadRequest in the Java example?",
    "answer": "It completes the multipart upload, combining the parts and their checksum values."
  },
  {
    "question": "How does the Java example handle checksum mismatches after upload?",
    "answer": "It aborts the multipart upload with AbortMultipartUploadRequest if the calculated checksum does not match the expected value."
  },
  {
    "question": "In the Java download validation example, what is checked for each part retrieved?",
    "answer": "The computed part checksum is compared with S3's reported checksum for that part."
  },
  {
    "question": "How does the Java code verify the checksum of the entire downloaded object?",
    "answer": "It calculates the SHA256 of the full download and compares it to the value stored or retrieved from S3 tags/attributes."
  },
  {
    "question": "On download, what happens if the checksum of checksums does not match the value from S3 attributes?",
    "answer": "An error is raised indicating failed checksum validation for the full object checksum of checksums."
  },
  {
    "question": "What method in the Java example can validate an existing file on disk against S3 multipart part-level checksums?",
    "answer": "validateExistingFileAgainstS3Checksum."
  },
  {
    "question": "How does the Java code handle reading part boundaries for part-level validation?",
    "answer": "It checks the running total of bytes read and computes each part's checksum at the correct offsets."
  },
  {
    "question": "If a part checksum does not match the value stored in S3 object attributes, what happens in the code?",
    "answer": "An IOException is thrown indicating part checksum mismatch."
  },
  {
    "question": "What is the base level encryption configuration for new S3 buckets and objects?",
    "answer": "Buckets and new objects are encrypted by using server-side encryption with Amazon S3 managed keys (SSE-S3) as the base level of encryption configuration."
  },
  {
    "question": "What are the key requirements for using AWS KMS keys with Amazon S3 encryption?",
    "answer": "You can use only KMS keys that are available in the same AWS Region as the bucket. When you use an AWS KMS key for server-side encryption in Amazon S3, you must choose a symmetric encryption KMS key. Amazon S3 supports only symmetric encryption KMS keys and not asymmetric KMS keys."
  },
  {
    "question": "What happens when you enable S3 Object Lock on a bucket?",
    "answer": "Enabling Object Lock automatically enables versioning for the bucket. After you've enabled and successfully created the bucket, you must also configure the Object Lock default retention and legal hold settings on the bucket's Properties tab."
  },
  {
    "question": "What permissions are required to create an Object Lock enabled bucket?",
    "answer": "To create an Object Lock enabled bucket, you must have the following permissions: s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration."
  },
  {
    "question": "What is the purpose of S3 Bucket Keys?",
    "answer": "S3 Bucket Keys lower the cost of encryption by decreasing request traffic from Amazon S3 to AWS KMS. By default, S3 Bucket Keys are enabled in the Amazon S3 console to help lower costs."
  },
  {
    "question": "How many objects can you download at once using the S3 console?",
    "answer": "You can download only one object at a time using the Amazon S3 console."
  },
  {
    "question": "What limitation exists when downloading objects with key names ending with a period?",
    "answer": "If you use the Amazon S3 console to download an object whose key name ends with a period (.), the period is removed from the key name of the downloaded object. To retain the period at the end of the name, you must use the AWS CLI, AWS SDKs, or Amazon S3 REST API."
  },
  {
    "question": "What are the main steps to copy an object to a folder in S3?",
    "answer": "To copy an object to a folder: 1) Create a folder in your bucket, 2) Navigate to the bucket containing the objects you want to copy, 3) Select the objects and choose Copy, 4) Choose the destination folder by browsing S3, and 5) Complete the copy operation."
  },
  {
    "question": "What should you do before deleting an S3 bucket?",
    "answer": "Before you delete your bucket, you must first empty the bucket or delete the objects in the bucket. After you delete your objects and bucket, they are no longer available."
  },
  {
    "question": "Why might you want to keep a bucket instead of deleting it?",
    "answer": "If you want to continue to use the same bucket name, it's recommended to delete the objects or empty the bucket, but don't delete the bucket. After you delete a bucket, the name becomes available to reuse, but another AWS account might create a bucket with the same name before you have a chance to reuse it."
  },
  {
    "question": "What are the four main use cases for Amazon S3?",
    "answer": "The four main use cases for Amazon S3 are: 1) Backup and storage - using storage management features to manage costs and meet regulatory requirements, 2) Application hosting - deploying reliable, highly scalable, and low-cost web applications, 3) Media hosting - building infrastructure for video, photo, or music uploads and downloads, and 4) Software delivery - hosting software applications for customer downloads."
  },
  {
    "question": "What is the default state of S3 buckets and objects regarding public access?",
    "answer": "By default, S3 buckets and the objects in them are private. You have access only to the S3 resources that you create."
  },
  {
    "question": "What are the main access control mechanisms available in Amazon S3?",
    "answer": "Amazon S3 provides several access control mechanisms: S3 Block Public Access, IAM identities, bucket policies, access control lists (ACLs), S3 Object Ownership, and IAM Access Analyzer for S3."
  },
  {
    "question": "What is recommended instead of using ACLs for access control in S3?",
    "answer": "As a general rule, it's recommended to use S3 resource-based policies (bucket policies and access point policies) or IAM user policies for access control instead of ACLs. Policies are a simplified and more flexible access-control option."
  },
  {
    "question": "What tools can be used for monitoring Amazon S3 storage?",
    "answer": "You can monitor Amazon S3 storage using Amazon S3 Storage Lens to understand and analyze storage usage, Storage Class Analysis to analyze storage access patterns, and S3 Lifecycle to manage costs."
  },
  {
    "question": "What are the two tiers of AWS CLI commands for accessing Amazon S3?",
    "answer": "The AWS CLI provides two tiers of commands: High-level (s3) commands that simplify common tasks like creating and manipulating objects and buckets, and API-level (s3api and s3control) commands that expose direct access to all Amazon S3 API operations for advanced operations."
  },
  {
    "question": "What is Amazon S3 from a technical perspective?",
    "answer": "Amazon S3 is a REST service. You can send requests to Amazon S3 using the REST API or the AWS SDK libraries, which wrap the underlying Amazon S3 REST API, simplifying programming tasks."
  },
  {
    "question": "What is the scope of bucket naming in Amazon S3?",
    "answer": "Amazon S3 supports global general purpose buckets, which means that each bucket name must be unique across all AWS accounts in all the AWS Regions within a partition. A partition is a grouping of Regions."
  },
  {
    "question": "Do objects ever leave their original AWS Region?",
    "answer": "Objects that belong to a bucket created in a specific AWS Region never leave that Region, unless you explicitly transfer them to another Region."
  },
  {
    "question": "What are the four Block Public Access settings and their default state?",
    "answer": "To ensure that all Amazon S3 general purpose buckets and objects have their public access blocked, all four settings for Block Public Access are enabled by default when you create a new bucket. These settings block all public access for all current and future buckets."
  },
  {
    "question": "What permissions are needed to view bucket public access settings?",
    "answer": "To view bucket public access settings, you need the following permissions: s3:GetAccountPublicAccessBlock, s3:GetBucketPublicAccessBlock, s3:GetBucketPolicyStatus, s3:GetBucketLocation, s3:GetBucketAcl, s3:ListAccessPoints, and s3:ListAllMyBuckets."
  },
  {
    "question": "What are some examples of bucket subresources for configuration?",
    "answer": "Bucket subresources include: CORS for cross-origin requests, event notification for bucket event notifications, lifecycle for object lifecycle rules, location for storing bucket region information, logging for access request tracking, object locking for retention policies, and policy/ACL for permission management."
  },
  {
    "question": "What is the recommended alternative to S3 static websites for secure hosting?",
    "answer": "For secure static website hosting, it's recommended to use Amazon CloudFront origin access control (OAC) instead of S3 static websites. CloudFront provides additional security headers like HTTPS, while S3 static websites only support HTTP endpoints."
  },
  {
    "question": "What happens during the bucket emptying process?",
    "answer": "Emptying the bucket cannot be undone. Objects added to the bucket while the empty bucket action is in progress will be deleted. You must type 'permanently delete' to confirm the action."
  },
  {
    "question": "What is the relationship between AWS accounts and resource ownership in S3?",
    "answer": "The AWS account that creates a resource owns that resource. For example, if an IAM user creates a bucket, the user can create it, but the AWS account that the user belongs to owns the bucket. The user needs additional permission from the resource owner to perform other bucket operations."
  },
  {
    "question": "What is the purpose of verifying checksums after uploading or downloading objects in Amazon S3?",
    "answer": "To ensure data integrity by verifying that the data in S3 matches the original data uploaded or downloaded."
  },
  {
    "question": "Which REST API operations support using checksums for S3 object verification?",
    "answer": "PutObject for uploads, GetObject and HeadObject for retrieving the checksum value."
  },
  {
    "question": "How does Amazon S3 let you provide an MD5 checksum when uploading objects?",
    "answer": "By providing the Content-MD5 header with the PUT operation."
  },
  {
    "question": "When is the ETag of an S3 object an MD5 digest of its data?",
    "answer": "If uploaded as plaintext or with SSE-S3, and via PutObject, PostObject, CopyObject, or the S3 Console (for objects under 16 MB and not multipart)."
  },
  {
    "question": "Which headers do you need for a chunked upload with trailing checksums?",
    "answer": "x-amz-decoded-content-length, x-amz-content-sha256, and x-amz-trailer."
  },
  {
    "question": "What are examples of x-amz-trailer header values for checksums?",
    "answer": "x-amz-checksum-crc32, x-amz-checksum-crc32c, x-amz-checksum-crc64nvme, x-amz-checksum-sha1, x-amz-checksum-sha256."
  },
  {
    "question": "What does the Content-Encoding header do for chunked uploads?",
    "answer": "It indicates chunked encoding, which can minimize proxy issues, though it is not required."
  },
  {
    "question": "What is the required minimum chunk size for object body chunks in a chunked S3 upload?",
    "answer": "Each chunk must contain at least 8,192 bytes (8 KiB) of data, except for the final chunk."
  },
  {
    "question": "What is the completion chunk in S3 chunked uploads?",
    "answer": "A chunk of zero bytes that indicates all data has been uploaded; its format differs if payload signing is used."
  },
  {
    "question": "How is a trailer chunk formatted for unsigned payloads with a trailing checksum?",
    "answer": "x-amz-checksum-<algorithm>:base64-checksum-value\\r\\n\\r\\n"
  },
  {
    "question": "Describe the main steps in a chunked PutObject request with trailing CRC-32 checksum as shown in Example 1.",
    "answer": "Upload object in multiple chunks, each with chunk size, append a completion chunk, and finally a trailer chunk with the CRC-32 checksum."
  },
  {
    "question": "How does a SigV4-signed chunked PutObject request with a trailing CRC-32 checksum differ from unsigned?",
    "answer": "Each chunk and the trailer chunk are signed, with chunk-signature data appended."
  },
  {
    "question": "Which S3 API operation deletes a single object?",
    "answer": "The DELETE (DeleteObject) API operation."
  },
  {
    "question": "How do you delete up to 1,000 objects in a single S3 API request?",
    "answer": "By using the Multi-Object Delete (DeleteObjects) API operation."
  },
  {
    "question": "What is a delete marker in a versioning-enabled bucket?",
    "answer": "A delete marker is a placeholder that becomes the latest version of an object when it's deleted, making the object seem removed without erasing prior versions."
  },
  {
    "question": "What happens if you delete a current object in a versioning-enabled bucket without specifying a version ID?",
    "answer": "S3 adds a delete marker rather than permanently deleting the object."
  },
  {
    "question": "How can you automatically delete objects or incomplete multipart uploads in S3?",
    "answer": "By setting up an S3 Lifecycle rule with object expiration or abort incomplete multipart uploads."
  },
  {
    "question": "What S3 event notifies you of an object deletion by an S3 Lifecycle configuration?",
    "answer": "The s3:LifecycleExpiration:Delete event."
  },
  {
    "question": "What permissions should you deny in an S3 bucket policy to prevent users from deleting objects?",
    "answer": "s3:DeleteObject, s3:DeleteObjectVersion, and s3:PutLifecycleConfiguration."
  },
  {
    "question": "How do you delete a specific version of an object in a versioning-enabled bucket?",
    "answer": "Specify both the object's key and the version ID in your delete request."
  },
  {
    "question": "What happens if you delete the delete marker of an object in S3?",
    "answer": "The object reappears in your bucket as the latest version."
  },
  {
    "question": "What best practice helps protect data from accidental object deletions in S3?",
    "answer": "Enabling bucket versioning."
  },
  {
    "question": "How does object deletion behave in a versioning-suspended S3 bucket?",
    "answer": "Delete API operations behave like versioning-enabled buckets, except if the current version is null, in which case the object is permanently deleted; otherwise, a delete marker is created."
  },
  {
    "question": "What happens if you issue a delete operation in an unversioned S3 bucket?",
    "answer": "The object is permanently deleted and cannot be recovered, unless versioning is enabled."
  },
  {
    "question": "What is the effect of providing an invalid MFA token when deleting from an MFA-enabled S3 bucket?",
    "answer": "The deletion request always fails."
  },
  {
    "question": "If you make a versioned delete request without an MFA token to an MFA-enabled bucket, what happens?",
    "answer": "The request fails."
  },
  {
    "question": "When does a multi-object delete request to an MFA-enabled bucket fail completely?",
    "answer": "If any of the specified deletes are versioned and a valid MFA token is not provided, the entire request fails."
  },
  {
    "question": "Does non-versioned delete succeed on an MFA-enabled bucket without an MFA token?",
    "answer": "Yes, non-versioned deletes succeed without an MFA token."
  },
  {
    "question": "Where can you find additional information about MFA delete configuration?",
    "answer": "See Configuring MFA delete in the Amazon S3 documentation."
  },
  {
    "question": "What AWS SDK or service feature allows you to automatically delete objects after a time?",
    "answer": "S3 Lifecycle rules."
  },
  {
    "question": "What step is required to delete a single object using the AWS S3 Console?",
    "answer": "Select the object, choose Delete, and confirm deletion by typing 'delete' or 'permanently delete' as requested."
  },
  {
    "question": "What must you type to confirm permanent deletion of a specific version of an object in the S3 Console?",
    "answer": "You must type 'Permanently delete' in the confirmation box."
  },
  {
    "question": "What happens if you delete an object whose version id is NULL in a versioning-suspended bucket?",
    "answer": "The object is permanently deleted since no previous versions exist."
  },
  {
    "question": "If a valid version ID exists in a versioning-suspended bucket, what does deletion do?",
    "answer": "S3 creates a delete marker for the deleted object, retaining previous versions."
  },
  {
    "question": "Which CLI command deletes an object in S3?",
    "answer": "aws s3api delete-object --bucket <bucket> --key <key>"
  },
  {
    "question": "Which REST API is used to delete a single object in S3?",
    "answer": "The DELETE Object API."
  },
  {
    "question": "What are the two options when deleting from a versioning-enabled bucket via the API?",
    "answer": "You can specify a version id to delete a specific version or not specify it, which adds a delete marker."
  },
  {
    "question": "What is the purpose of the delete marker in S3 versioning?",
    "answer": "To make it appear the object has been deleted, while previous versions remain recoverable."
  },
  {
    "question": "Is deletion of objects in non-versioned buckets reversible?",
    "answer": "No, deletions in non-versioned buckets are permanent."
  },
  {
    "question": "How do you handle deletion issues in S3?",
    "answer": "Consult the 'I want to permanently delete versioned objects' troubleshooting guide."
  },
  {
    "question": "How can you delete multiple objects at once in S3?",
    "answer": "Use the Multi-Object Delete operation; for example, the aws s3api delete-objects command or via the REST API."
  },
  {
    "question": "In the Java SDK, what method is used to delete an object in a non-versioned bucket?",
    "answer": "s3Client.deleteObject(new DeleteObjectRequest(bucketName, keyName))"
  },
  {
    "question": "What two main exceptions must be handled when deleting an object in Java AWS SDK?",
    "answer": "AmazonServiceException and SdkClientException."
  },
  {
    "question": "In the Java versioned delete example, what happens after adding a sample object?",
    "answer": "Amazon S3 returns the version ID, and that version is then deleted by specifying key and version ID."
  },
  {
    "question": "How do you check if an S3 bucket has versioning enabled with the Java AWS SDK?",
    "answer": "Call s3Client.getBucketVersioningConfiguration(bucketName).getStatus() and compare to BucketVersioningConfiguration.ENABLED."
  },
  {
    "question": "How do you delete a specific object version in the Java AWS SDK?",
    "answer": "Use s3Client.deleteVersion(new DeleteVersionRequest(bucketName, keyName, versionId))."
  },
  {
    "question": "Which C# class and method deletes an object in a non-versioned bucket in AWS SDK for .NET?",
    "answer": "DeleteObjectRequest class and DeleteObjectAsync method."
  },
  {
    "question": "What is a best practice for collecting logs in S3 buckets?",
    "answer": "Set up a lifecycle rule to automatically delete log files when no longer needed."
  },
  {
    "question": "What does the 'Show versions' toggle in the S3 console allow you to do?",
    "answer": "It allows you to view and select specific object versions for deletion."
  },
  {
    "question": "If you want to delete a specific object version using AWS CLI, which parameters are necessary?",
    "answer": "You must provide both the --key and --version-id parameters."
  },
  {
    "question": "What method is used in the .NET example to get all versions of an object in a bucket?",
    "answer": "client.ListVersions(new ListVersionsRequest { BucketName = bucketName, Prefix = keyName })"
  },
  {
    "question": "In an MFA-enabled bucket, under which scenarios do deletions succeed without an MFA token?",
    "answer": "When deleting non-versioned objects or multi-object deletes that only specify non-versioned objects."
  },
  {
    "question": "Where in the AWS Management Console do you start the delete process for a single S3 object?",
    "answer": "Open the S3 console, select your bucket, find the object, select it, and choose Delete."
  },
  {
    "question": "What warning is given repeatedly in the documentation about object deletion?",
    "answer": "When you permanently delete an object or version, it cannot be undone."
  },
  {
    "question": "What AWS SDKs are shown for deleting a single S3 object in the guide?",
    "answer": "SDKs for .NET, PHP, Ruby, and JavaScript (Node.js) are shown."
  },
  {
    "question": "How do you delete an object in a non-versioned S3 bucket using PHP SDK v3?",
    "answer": "Call deleteObject with 'Bucket' and 'Key', omitting a version ID."
  },
  {
    "question": "What happens if the S3 PHP delete call result contains 'DeleteMarker' as true?",
    "answer": "It means the object was deleted or did not exist."
  },
  {
    "question": "How do you check if an object still exists after deletion with PHP SDK?",
    "answer": "Call getObject with 'Bucket' and 'Key', catching the exception if it doesn't exist."
  },
  {
    "question": "What command is used in JavaScript AWS SDK (v3) to delete an S3 object?",
    "answer": "DeleteObjectCommand from '@aws-sdk/client-s3'."
  },
  {
    "question": "What should you do before adding new objects to an S3 folder you've just deleted from?",
    "answer": "Wait for the delete action to finish, or new objects may be deleted too."
  },
  {
    "question": "What is the permanent effect of deleting objects in S3 buckets without versioning?",
    "answer": "The objects are permanently deleted and unrecoverable."
  },
  {
    "question": "What does S3 create when deleting objects in a versioned or versioning-suspended bucket?",
    "answer": "Delete markers are created, retaining previous object versions."
  },
  {
    "question": "How does S3 treat delete operations for objects with version ID 'NULL' in a versioning-suspended bucket?",
    "answer": "S3 permanently deletes the object since no previous versions exist."
  },
  {
    "question": "How can you delete specific versions of an object in a versioning-enabled bucket using the Console?",
    "answer": "Enable 'Show versions', select versions, and confirm permanent deletion with 'Permanently delete'."
  },
  {
    "question": "What text must you enter in the Console to confirm permanent deletion of specific object versions?",
    "answer": "'Permanently delete' (not case-sensitive)."
  },
  {
    "question": "If you want to delete multiple objects using the AWS Console, what step confirms the deletion?",
    "answer": "Type 'delete' in the confirmation box for general deletes, or 'permanently delete' for version deletes."
  },
  {
    "question": "Which AWS SDK methods/API features allow you to delete multiple objects at once?",
    "answer": "Multi-Object Delete via SDKs or the REST API's DeleteObjects operation."
  },
  {
    "question": "What is a prefix in S3 and how is it used?",
    "answer": "A prefix is a logical grouping (like a directory); it's a string at the start of the object key used to organize data hierarchically."
  },
  {
    "question": "How does the S3 Console represent prefixes?",
    "answer": "As folders in the interface."
  },
  {
    "question": "How can you organize and browse your S3 keys hierarchically?",
    "answer": "Use prefixes and a delimiter (like '/') in the object key names, with list operations specifying these parameters."
  },
  {
    "question": "Give an example of a prefix-structured S3 object key for a city in the USA.",
    "answer": "'North America/USA/Washington/Seattle'"
  },
  {
    "question": "How can you use the delimiter parameter in S3 list operations?",
    "answer": "To have the result return only one level of hierarchy, grouping deeper keys by their common prefix."
  },
  {
    "question": "In the ListBucketResult XML example, what does the <CommonPrefixes> element show?",
    "answer": "It shows all prefixes (folders) immediately under the current prefix, such as 'photos/'."
  },
  {
    "question": "How do you list only root-level objects in an S3 bucket?",
    "answer": "Send a GET request with the delimiter (e.g., '/') to return only objects that don't contain the delimiter."
  },
  {
    "question": "Can S3's list implementation be slowed down by a very large number of objects?",
    "answer": "No, list performance is not substantially affected by the total number of keys."
  },
  {
    "question": "How does S3 API handle very large result sets when listing objects?",
    "answer": "With pagination; each response returns up to 1,000 keys and an indicator if it's truncated."
  },
  {
    "question": "How do AWS CLI and SDKs handle paginated responses for S3 listings?",
    "answer": "They provide automatic handling for pagination to get all objects."
  },
  {
    "question": "What CLI command lists all object keys and sizes in a bucket called 'text-content'?",
    "answer": "aws s3api list-objects --bucket text-content --query 'Contents[].{Key: Key, Size: Size}'"
  },
  {
    "question": "Which permission do you need to list all objects in a bucket?",
    "answer": "s3:ListBucket"
  },
  {
    "question": "What does the --query parameter do in the AWS CLI list-objects example?",
    "answer": "It filters the output, showing only the key and size for each object."
  },
  {
    "question": "How do you list S3 keys programmatically by prefix?",
    "answer": "Call the list operation with the desired prefix and delimiter, and iterate through results."
  },
  {
    "question": "Are SOAP APIs for S3 still recommended?",
    "answer": "No, SOAP APIs are not available for new customers and are approaching End of Life; use REST API or SDKs."
  },
  {
    "question": "What S3 API lets you retrieve a list of objects matching a pattern like all words starting with 'q'?",
    "answer": "The list operation using a prefix corresponding to the pattern (e.g., 'q')."
  },
  {
    "question": "Which AWS CLI command lists all objects and prefixes in a specific S3 bucket?",
    "answer": "aws s3 ls s3://amzn-s3-demo-bucket"
  },
  {
    "question": "How do you list all items in an S3 bucket using AWS Tools for PowerShell?",
    "answer": "Get-S3Object -BucketName amzn-s3-demo-bucket"
  },
  {
    "question": "What PowerShell command lists a single object named 'sample.txt' in a bucket?",
    "answer": "Get-S3Object -BucketName amzn-s3-demo-bucket -Key sample.txt"
  },
  {
    "question": "Which PowerShell command lists items with the prefix 'sample' in a bucket?",
    "answer": "Get-S3Object -BucketName amzn-s3-demo-bucket -KeyPrefix sample"
  },
  {
    "question": "How does the S3 console implement folders for organizing objects?",
    "answer": "By using a shared key name prefix displayed as a folder in the console."
  },
  {
    "question": "What type of object does the S3 console create when you make a new folder?",
    "answer": "A 0-byte object with the key as the folder name plus a trailing slash ('/')."
  },
  {
    "question": "If an object in S3 is named with a trailing slash, how does it appear in the console?",
    "answer": "It appears as a folder."
  },
  {
    "question": "Can you upload an object with a trailing slash key from the S3 console?",
    "answer": "No; you must use the CLI, SDK, or REST API for this."
  },
  {
    "question": "What happens to content and metadata for folder objects when using the console?",
    "answer": "They are not displayed, and copying a folder object does not copy its data or metadata."
  },
  {
    "question": "Can you nest folders in S3?",
    "answer": "Yes, you can have folders within folders, but not buckets within buckets."
  },
  {
    "question": "What are the steps to create a folder using the S3 console?",
    "answer": "1. Sign in to AWS Console. 2. Open S3. 3. Choose bucket. 4. Choose 'Create folder'. 5. Enter name. 6. (Optional: choose encryption key.) 7. Confirm."
  },
  {
    "question": "What happens if a bucket policy requires tags or metadata for uploads?",
    "answer": "You can't create a folder via the console unless you upload an empty folder specifying these settings."
  },
  {
    "question": "What is the maximum length for an S3 object key name, including folder names?",
    "answer": "1,024 bytes."
  },
  {
    "question": "How can you make an S3 folder public?",
    "answer": "Via the S3 console or by a bucket policy restricting by prefix."
  },
  {
    "question": "Once a folder is made public in S3, how do you make it private again?",
    "answer": "You must remove public access on each object individually."
  },
  {
    "question": "How do you calculate the total size of an S3 folder?",
    "answer": "Select the folder, choose Actions, and then select 'Calculate total size' in the S3 console."
  },
  {
    "question": "If you navigate away after calculating folder size in the console, what happens?",
    "answer": "You must recalculate the size if you want to see it again."
  },
  {
    "question": "Does the S3 folder size calculation include incomplete multipart uploads?",
    "answer": "No, it only counts the latest version of each object and excludes incompletes and older versions."
  },
  {
    "question": "How do you delete folders in S3 using the console?",
    "answer": "Select checkboxes for folders/objects, choose Delete, verify selection, type 'delete', and confirm."
  },
  {
    "question": "What is the recommended practice after deleting a folder before adding new objects?",
    "answer": "Wait for the delete operation to fully complete, or new objects might get deleted."
  },
  {
    "question": "How do you view S3 object properties in the console?",
    "answer": "Choose the object, and scroll down on the Object overview page."
  },
  {
    "question": "In S3, what happens if you change an object's Storage Class, Encryption, or Metadata?",
    "answer": "A new object replaces the old; in versioned buckets, a new version is created."
  },
  {
    "question": "What permissions are required to change properties for large (over 16 MB) or tagged objects?",
    "answer": "You need s3:GetObjectTagging permission."
  },
  {
    "question": "If the destination bucket policy denies s3:GetObjectTagging, what happens?",
    "answer": "Object properties are updated, tags are removed, and you receive an error."
  },
  {
    "question": "Can you change the storage class of directory bucket objects?",
    "answer": "No, storage class change is not allowed for directory bucket objects."
  },
  {
    "question": "Which property prevents an object from being deleted in S3?",
    "answer": "Object lock legal hold or retention."
  },
  {
    "question": "What is an S3 object tag?",
    "answer": "A key-value pair attached to an object, used for categorization and management."
  },
  {
    "question": "How many tags can you assign to an S3 object?",
    "answer": "Up to 10 unique tags."
  },
  {
    "question": "What is the character limit for S3 tag keys and values?",
    "answer": "Keys: 128 Unicode characters, Values: 256 Unicode characters."
  },
  {
    "question": "Are S3 tag keys and values case sensitive?",
    "answer": "Yes, they are case sensitive."
  },
  {
    "question": "Give an example of a PHI object tagging scheme.",
    "answer": "PHI=True or Classification=PHI"
  },
  {
    "question": "How else can you categorize S3 objects besides tagging?",
    "answer": "By using key name prefixes (e.g., 'photos/')."
  },
  {
    "question": "How are prefixes different from tags for categorization?",
    "answer": "Prefixes provide one-dimensional grouping; tags enable multi-dimensional classification."
  },
  {
    "question": "What are benefits of tagging S3 objects?",
    "answer": "Fine-grained access control, lifecycle management filters, custom analytics, and CloudWatch metric customization."
  },
  {
    "question": "Should you put confidential information in S3 tags?",
    "answer": "No, while you can tag sensitive objects, tags themselves must not include confidential data."
  },
  {
    "question": "What S3 feature allows you to add object tag sets to multiple objects with a single request?",
    "answer": "S3 Batch Operations."
  },
  {
    "question": "How does S3 Batch Operations work?",
    "answer": "You provide a manifest of objects and an operation, and S3 executes the operation, sending progress notifications and a completion report."
  },
  {
    "question": "Which API operation replaces tags on an S3 object?",
    "answer": "PUT Object tagging."
  },
  {
    "question": "How do you add tags to an object that previously had no tags?",
    "answer": "Use the PUT Object tagging API to specify a new set of tags in the request body."
  },
  {
    "question": "What must you do before modifying an existing S3 object's tag set?",
    "answer": "You must retrieve the current tag set, modify it on the client side, and then use the API to replace the tag set."
  },
  {
    "question": "What happens if you send a PUT Object tagging request with an empty tag set?",
    "answer": "Amazon S3 deletes the existing tag set on the object and charges you for a Tier 1 PUT request."
  },
  {
    "question": "Why is the DELETE Object tagging request often preferred over a PUT with empty tag set?",
    "answer": "Because DELETE is free and achieves the same result without incurring charges."
  },
  {
    "question": "Which API call retrieves the set of tags attached to an S3 object?",
    "answer": "GET Object tagging"
  },
  {
    "question": "How can you remove all tags from an S3 object?",
    "answer": "By using the DELETE Object tagging API."
  },
  {
    "question": "When creating objects using PUT or InitiateMultipartUpload, how do you add tags?",
    "answer": "By specifying tags in the x-amz-tagging request header."
  },
  {
    "question": "What header shows the object tag count in a GET Object call?",
    "answer": "The x-amz-tag-count header."
  },
  {
    "question": "If tags for a new S3 object exceed 8K in the header, how do you upload them?",
    "answer": "Use the POST method and include the tags in the request body."
  },
  {
    "question": "When copying an object (PUT Object - Copy), how can you control tag behavior?",
    "answer": "By using the x-amz-tagging-directive header to copy existing tags or replace them with a new set."
  },
  {
    "question": "Is S3 object tagging strongly consistent?",
    "answer": "Yes, S3 object tagging is strongly consistent."
  },
  {
    "question": "How can lifecycle policies in S3 use tags?",
    "answer": "Lifecycle filters can select objects by object tags, allowing differentiated rules (e.g., archive only 'phototype=raw' photos)."
  },
  {
    "question": "Can S3 replication copy object tags to destination buckets?",
    "answer": "Yes, if Amazon S3 has permission to read the tags."
  },
  {
    "question": "Which S3 event types notify on tag changes?",
    "answer": "s3:ObjectTagging:Put (for addition/update), s3:ObjectTagging:Delete (for removal)."
  },
  {
    "question": "How can you limit user permissions to only manage certain S3 tags or tag values?",
    "answer": "Use condition keys such as s3:RequestObjectTagKeys and s3:RequestObjectTag/<tag-key> in policies."
  },
  {
    "question": "Which condition key checks if an existing tag has a specific key and value?",
    "answer": "s3:ExistingObjectTag/<tag-key>"
  },
  {
    "question": "Can you deny or grant object delete or overwrite based on existing tags using standard policy conditions?",
    "answer": "No, s3:ExistingObjectTag cannot be used with PUT or DELETE Object permissions."
  },
  {
    "question": "How would you restrict tag keys allowed in PutObjectTagging via IAM policy?",
    "answer": "Use the condition key s3:RequestObjectTagKeys."
  },
  {
    "question": "How would you restrict allowed tag key-value combinations via policy?",
    "answer": "Use the condition key s3:RequestObjectTag/<tag-key>."
  },
  {
    "question": "Give a policy use case for object tag-based access control.",
    "answer": "Allowing a user to read only objects tagged with environment:production, using s3:ExistingObjectTag condition key."
  },
  {
    "question": "What is the default payment model for S3 bucket downloads, and how can it be changed?",
    "answer": "By default, the AWS account that creates the bucket (the bucket owner) pays for downloads from the bucket. Using the requestPayment subresource, the bucket owner can specify that the person requesting the download will be charged for the download."
  },
  {
    "question": "What is the purpose of S3 tagging subresource?",
    "answer": "You can add cost allocation tags to your bucket to categorize and track your AWS costs. Using tags applied to your bucket, AWS generates a cost allocation report with usage and costs aggregated by your tags."
  },
  {
    "question": "How does S3 Transfer Acceleration work?",
    "answer": "Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of the globally distributed edge locations of Amazon CloudFront."
  },
  {
    "question": "Why is S3 Versioning recommended as a best practice?",
    "answer": "Versioning helps you recover accidental overwrites and deletes. It is recommended as a best practice to recover objects from being deleted or overwritten by mistake."
  },
  {
    "question": "What is the high availability engineering focus of Amazon S3?",
    "answer": "The high availability engineering of Amazon S3 is focused on get, put, list, and delete operations. It's recommended not to create, delete, or configure buckets on the high availability code path of your application, but rather in a separate initialization or setup routine run less often."
  },
  {
    "question": "What AWS services can be used to monitor S3 bucket performance?",
    "answer": "You can use CloudWatch alarms to monitor storage data and request metrics, Amazon S3 Storage Lens to plan storage usage and optimize costs, AWS CloudTrail logs to track API calls, Amazon S3 event notifications for bucket events, and S3 access logs for detailed request records."
  },
  {
    "question": "What is the multi-tenant general purpose bucket pattern?",
    "answer": "With multi-tenant buckets, you create a single general purpose bucket for a team or workload and use unique S3 prefixes to organize objects. A prefix is a string of characters at the beginning of the object key name and can be any length up to 1,024 bytes."
  },
  {
    "question": "What are the limitations of the multi-tenant bucket pattern?",
    "answer": "Many S3 bucket-level features like default bucket encryption, S3 Versioning, and S3 Requester Pays are set at the bucket-level and not the prefix-level. This makes it difficult to specify correct settings for each dataset. Additionally, cost allocation can become complex in multi-tenant buckets."
  },
  {
    "question": "What is the bucket-per-use pattern and its benefits?",
    "answer": "With the bucket-per-use pattern, you create a general purpose bucket for each distinct dataset, end user, or team. This allows you to configure unique bucket-level settings for each bucket and simplifies access management and cost allocation strategies."
  },
  {
    "question": "What is the default quota for general purpose buckets per AWS account?",
    "answer": "All AWS accounts have a default quota of 10,000 general purpose buckets. You can increase the bucket quota by submitting a quota increase request through the Service Quotas console."
  },
  {
    "question": "What are the character length requirements for general purpose bucket names?",
    "answer": "Bucket names must be between 3 (minimum) and 63 (maximum) characters long."
  },
  {
    "question": "What characters are allowed in general purpose bucket names?",
    "answer": "Bucket names can consist only of lowercase letters, numbers, periods (.), and hyphens (-). They must begin and end with a letter or number."
  },
  {
    "question": "What prefixes are prohibited for general purpose bucket names?",
    "answer": "Bucket names must not start with the prefixes 'xn--', 'sthree-', or 'amzn-s3-demo-'. They also must not end with suffixes like '-s3alias', '--ol-s3', '.mrap', '--x-s3', or '--table-s3'."
  },
  {
    "question": "Why should you avoid using periods in bucket names?",
    "answer": "For best compatibility, avoid using periods (.) in bucket names except for static website hosting. If you include periods in a bucket's name, you can't use virtual-host-style addressing over HTTPS unless you perform your own certificate validation."
  },
  {
    "question": "What is the recommended approach for creating unpredictable bucket names?",
    "answer": "Append a Globally Unique Identifier (GUID) to your bucket name, for example, 'amzn-s3-demo-bucket-a1b2c3d4-5678-90ab-cdef-EXAMPLE11111'. Don't write code assuming your chosen bucket name is available unless you have already created the bucket."
  },
  {
    "question": "What happens to bucket names after a bucket is deleted?",
    "answer": "After a bucket is deleted, the name becomes available for reuse. However, another AWS account might create a bucket with the same name before you can reuse it. If you want to prevent this, empty the bucket and keep it instead of deleting it."
  },
  {
    "question": "What are the limitations regarding ListBuckets requests for accounts with high bucket quotas?",
    "answer": "Unpaginated ListBuckets requests are only supported for AWS accounts set to the default quota of 10,000 buckets. If you have an approved quota above 10,000, you must send paginated ListBuckets requests, as all unpaginated requests will be rejected."
  },
  {
    "question": "Which AWS Regions are used to manage general purpose bucket quotas?",
    "answer": "General purpose bucket quotas for commercial Regions can only be viewed and managed from US East (N. Virginia), while quotas for AWS GovCloud (US) can only be viewed and managed from AWS GovCloud (US-West)."
  },
  {
    "question": "What are some common use cases for accessing S3 general purpose buckets?",
    "answer": "Common use cases include static websites, shared datasets using multi-tenant models with access points, high-throughput workloads using Mountpoint for Amazon S3, multi-Region applications with Multi-Region Access Points, and secure file transfers using SFTP."
  },
  {
    "question": "What is Mountpoint for Amazon S3?",
    "answer": "Mountpoint for Amazon S3 is a high-throughput open source file client for mounting an Amazon S3 general purpose bucket as a local file system. It allows applications to access objects through file-system operations like open and read, automatically translating them into S3 object API calls."
  },
  {
    "question": "What are the two tiers of AWS CLI commands for Amazon S3?",
    "answer": "The AWS CLI provides high-level (s3) commands that simplify common tasks like creating and manipulating objects and buckets, and API-level (s3api and s3control) commands that expose direct access to all Amazon S3 API operations for advanced operations."
  },
  {
    "question": "What is the difference between path-style and virtual-hosted-style URLs in S3?",
    "answer": "Path-style URLs use the format 'https://s3.region-code.amazonaws.com/bucket-name/key-name', while virtual-hosted-style URLs use 'https://bucket-name.s3.region-code.amazonaws.com/key-name' where the bucket name is part of the domain name."
  },
  {
    "question": "What is the future of path-style URLs in Amazon S3?",
    "answer": "Path-style URLs will be discontinued in the future. Amazon S3 currently supports both virtual-hosted-style and path-style URL access in all AWS Regions, but customers should transition to virtual-hosted-style URLs."
  },
  {
    "question": "What limitation exists with SSL and virtual-hosted-style buckets?",
    "answer": "When using virtual-hosted-style general purpose buckets with SSL, the SSL wildcard certificate matches only buckets that do not contain dots (.). To work around this limitation, use HTTP or write your own certificate-verification logic."
  },
  {
    "question": "How can you customize Amazon S3 URLs using CNAME records?",
    "answer": "You can use CNAME to map your domain name to an Amazon S3 hostname. Your bucket name must be the same as the CNAME. For example, you can map images.example.com to images.example.com.s3.us-east-1.amazonaws.com to use http://images.example.com/filename instead of the full S3 URL."
  },
  {
    "question": "What security consideration exists when using CNAMEs with S3?",
    "answer": "When using custom URLs with CNAMEs, ensure a matching bucket exists for any CNAME or alias record you configure. If you create DNS entries without matching buckets, any AWS user can create that bucket and publish content under the configured alias. Change or remove the CNAME when deleting a bucket."
  },
  {
    "question": "What are the benefits of using AWS SDKs over direct REST API calls?",
    "answer": "AWS SDKs provide libraries that wrap the underlying Amazon S3 REST API and simplify programming tasks. They handle tasks such as calculating signatures, cryptographically signing requests, managing errors, and retrying requests automatically."
  },
  {
    "question": "What is the recommended tool for managing bucket-per-use patterns at scale?",
    "answer": "AWS CloudFormation is recommended for managing bucket-per-use patterns. You can create a custom CloudFormation template that defines all desired settings for S3 general purpose buckets, allowing easy deployment and tracking of infrastructure changes."
  },
  {
    "question": "How do you associate a custom hostname with an Amazon S3 bucket using a CNAME record?",
    "answer": "To associate a custom hostname with an Amazon S3 bucket, create a bucket with a name that matches the hostname (e.g., images.example.com), then create a CNAME DNS record that points the hostname to the Amazon S3 endpoint (e.g., images.example.com CNAME images.example.com.s3.us-west-2.amazonaws.com)."
  },
  {
    "question": "What is the required DNS format for associating a CNAME with an S3 bucket?",
    "answer": "The CNAME DNS record must define the hostname as an alias for the exact Amazon S3 bucket endpoint (e.g., images.example.com CNAME images.example.com.s3.us-west-2.amazonaws.com), with the record created exactly as described to avoid unpredictable behavior."
  },
  {
    "question": "What are the limitations of using SOAP APIs with Amazon S3 buckets?",
    "answer": "SOAP APIs are not available for new customers and are being deprecated, with End of Life (EOL) scheduled for August 31, 2025. Amazon recommends using the REST API or AWS SDKs instead."
  },
  {
    "question": "Describe the format of virtual-hosted–style and path-style URLs for S3 buckets.",
    "answer": "Virtual-hosted–style URL: https://bucket-name.s3.region-code.amazonaws.com/key-name ; Path-style URL: https://s3.region-code.amazonaws.com/bucket-name/key-name."
  },
  {
    "question": "Why should you avoid using periods in S3 bucket names except for static website hosting?",
    "answer": "Using periods in bucket names can cause compatibility issues; for best compatibility, avoid using periods except when the bucket is intended for static website hosting."
  },
  {
    "question": "What happens if you access a bucket in a new AWS region (after March 20, 2019) using the legacy global endpoint?",
    "answer": "If you use the legacy global endpoint for a bucket in a region launched after March 20, 2019, you will receive an HTTP 400 Bad Request error, as the DNS server does not route your request to the correct region."
  },
  {
    "question": "How does Amazon S3 handle requests to a bucket using the wrong region-specific endpoint?",
    "answer": "If you use an endpoint from a different region than where the bucket resides, Amazon S3 returns an HTTP 301 Permanent Redirect error, providing the correct URI for your resource."
  },
  {
    "question": "What settings can you configure when creating a general purpose S3 bucket?",
    "answer": "You can configure S3 Object Ownership, S3 Object Lock, S3 Block Public Access, S3 Versioning, default encryption, tags, and object lock retention settings."
  },
  {
    "question": "After creating an Amazon S3 bucket, can you change its name, region, or owner?",
    "answer": "No, after creating an S3 bucket, you cannot change its name, region, or owner."
  },
  {
    "question": "What is the default bucket ownership and ACL setting when a new S3 bucket is created?",
    "answer": "By default, 'Bucket owner enforced' is enabled and ACLs are disabled. The bucket owner owns all objects and access is managed exclusively using policies."
  },
  {
    "question": "How does enabling S3 Object Lock affect a bucket?",
    "answer": "Enabling S3 Object Lock automatically enables versioning for the bucket and prevents objects from being deleted or overwritten during the retention period."
  },
  {
    "question": "What permissions are needed to create an S3 bucket with Object Lock enabled?",
    "answer": "You must have s3:CreateBucket, s3:PutBucketVersioning, and s3:PutBucketObjectLockConfiguration permissions to enable Object Lock."
  },
  {
    "question": "Describe the steps to create a general purpose bucket using the AWS Management Console.",
    "answer": "1. Sign in to the AWS Management Console and open S3. 2. Choose the region. 3. Select 'General purpose buckets', then 'Create bucket'. 4. Enter a unique bucket name. 5. (Optional) Copy settings from another bucket. 6. Configure Object Ownership and Block Public Access. 7. (Optional) Enable versioning, encryption, tagging, and Object Lock. 8. Click 'Create bucket'."
  },
  {
    "question": "What are the naming rules for Amazon S3 buckets?",
    "answer": "Bucket names must be unique within a partition, use only lowercase letters, numbers, periods, and hyphens, be 3–63 characters, and start and end with a letter or number."
  },
  {
    "question": "How do you create an S3 bucket using the AWS SDK for Java?",
    "answer": "Use AmazonS3ClientBuilder to create an S3 client in the desired region, then call createBucket with the bucket name. Optionally, check for existence and retrieve the location with getBucketLocation."
  },
  {
    "question": "Provide an AWS CLI example command to create an S3 bucket in a specific region.",
    "answer": "aws s3api create-bucket --bucket your-bucket-name --region your-region --create-bucket-configuration LocationConstraint=your-region"
  },
  {
    "question": "How do you retrieve the versioning state of an S3 bucket with the CLI?",
    "answer": "Use the command: aws s3api get-bucket-versioning --bucket your-bucket-name"
  },
  {
    "question": "What is S3 Bucket Key and what benefit does it provide?",
    "answer": "S3 Bucket Key reduces the cost of encryption for objects in S3 by decreasing request traffic from Amazon S3 to AWS KMS when using Server-Side Encryption with AWS KMS keys."
  },
  {
    "question": "Which server-side encryption options are available for S3 buckets?",
    "answer": "You can select server-side encryption with S3 managed keys (SSE-S3), AWS KMS keys (SSE-KMS), dual-layer KMS keys (DSSE-KMS), or customer-provided keys (SSE-C)."
  },
  {
    "question": "What is the effect of enabling all S3 Block Public Access settings?",
    "answer": "Enabling all S3 Block Public Access settings prevents public access to the bucket and its objects, overriding any bucket, access point, or object policies that might otherwise permit public access."
  },
  {
    "question": "What can you use tagging on an S3 bucket for?",
    "answer": "Bucket tags, as key-value pairs, can be used for cost allocation, resource organization, and applying specific policies or automation based on tags."
  },
  {
    "question": "What does enabling versioning on a bucket allow you to do?",
    "answer": "Enabling versioning allows you to retain, retrieve, and restore multiple versions of any object stored in your bucket, providing protection against accidental deletion and overwrites."
  },
  {
    "question": "Name three properties of an S3 bucket that you can view or configure via the AWS CLI.",
    "answer": "You can view or configure tags, versioning, default encryption, logging, and event notifications using the AWS CLI."
  },
  {
    "question": "How do you list all S3 buckets owned by your AWS account using the AWS CLI?",
    "answer": "Use: aws s3api list-buckets"
  },
  {
    "question": "What response and error do you get if you try to create a bucket with a non-unique name?",
    "answer": "If the bucket name is not unique, AWS returns an error indicating that the bucket already exists."
  },
  {
    "question": "What are the three AWS partitions mentioned for S3 bucket names?",
    "answer": "The three partitions are: aws (commercial regions), aws-cn (China regions), and aws-us-gov (GovCloud regions)."
  },
  {
    "question": "What happens if you do not specify a region when creating a bucket or S3 client?",
    "answer": "Amazon S3 uses the default region, which is US East (N. Virginia)."
  },
  {
    "question": "If ACLs are disabled on a bucket, how is access to objects controlled?",
    "answer": "With ACLs disabled, access to objects is managed exclusively through policies."
  },
  {
    "question": "Explain what 'Requester Pays' means for an S3 bucket.",
    "answer": "With 'Requester Pays' enabled, the requester of data (instead of the bucket owner) pays for the data transfer and request costs."
  },
  {
    "question": "What is S3 Transfer Acceleration and why would you enable it?",
    "answer": "S3 Transfer Acceleration speeds up and secures the transfer of files over long distances between your client and S3."
  },
  {
    "question": "How can you enable server access logging on an S3 bucket?",
    "answer": "You can enable server access logging via the S3 console, AWS CLI, or SDKs, specifying a target bucket to receive the logs."
  },
  {
    "question": "Describe how to create a new customer managed key for use with S3 server-side encryption.",
    "answer": "To create a customer managed key, use the AWS KMS console to generate a new key and ensure it is in the same region as your S3 bucket. Then, enter the AWS KMS key ARN when setting up encryption."
  },
  {
    "question": "What should you do if your required AWS KMS key is not listed in the S3 console?",
    "answer": "You should enter your AWS KMS key ARN manually. If the key is owned by a different account, you must have permission to use the key."
  },
  {
    "question": "What programming languages were examples provided for creating S3 buckets using AWS SDKs?",
    "answer": "Examples were given for Java, .NET, and Ruby."
  },
  {
    "question": "Can you copy all settings from one bucket to another using the AWS CLI?",
    "answer": "No, copying settings from an existing bucket is only available in the S3 console, and this does not copy bucket policy."
  },
  {
    "question": "Which permission is required to list all of your general purpose S3 buckets?",
    "answer": "You must have the s3:ListAllMyBuckets permission."
  },
  {
    "question": "What should you do if you encounter an HTTP 403 Forbidden error when listing S3 buckets?",
    "answer": "You should refer to the 'Troubleshoot access denied (403 Forbidden) errors in Amazon S3' documentation."
  },
  {
    "question": "Why does AWS recommend using paginated ListBuckets requests?",
    "answer": "Paginated ListBuckets requests are necessary for accounts with a bucket quota above 10,000, as unpaginated requests will be rejected for such accounts."
  },
  {
    "question": "How can you list your general purpose buckets using the AWS Management Console?",
    "answer": "Sign in to the AWS Management Console, open the Amazon S3 console, choose 'General purpose buckets' in the left navigation pane, and view the list on the tab. You can also search by name in the 'Find buckets by name' field."
  },
  {
    "question": "Provide the AWS CLI command to list all general purpose buckets in your account using an unpaginated call.",
    "answer": "aws s3 ls"
  },
  {
    "question": "What CLI command lists all buckets with results paginated to 100 buckets per page?",
    "answer": "aws s3 ls --page-size 100"
  },
  {
    "question": "How can you retrieve the next set of results in a paginated ListBuckets response using the AWS CLI?",
    "answer": "Use the --starting-token argument with the value of the continuation token returned by the previous call."
  },
  {
    "question": "How do you list up to 100 buckets in a specified AWS region using pagination in the CLI?",
    "answer": "aws s3api list-buckets --region us-east-2 --max-items 100 --page-size 100 --bucket-region us-east-2"
  },
  {
    "question": "How can you list S3 buckets beginning with a specific prefix using the CLI?",
    "answer": "Use aws s3api list-buckets with the --prefix argument set to your desired prefix. Example: --prefix amzn-s3-demo-bucket"
  },
  {
    "question": "Show an example of listing general purpose buckets using the AWS SDK for Python with pagination.",
    "answer": "import boto3; s3 = boto3.client('s3'); response = s3.list_buckets(MaxBuckets=100)"
  },
  {
    "question": "What is the purpose of the continuation token in a paginated request?",
    "answer": "It enables you to retrieve the next set of items by passing it to the next request after the previous call returned less items than available."
  },
  {
    "question": "Explain how to use the AWS SDK for Java to list Amazon S3 buckets with pagination.",
    "answer": "Use ListBucketsPaginatedRequest and ListBucketsPaginatedResult to fetch and process buckets in pages, printing names and regions and using the continuation token for subsequent pages."
  },
  {
    "question": "What should you remember to do after finishing with an S3Client in Java 2.x SDK?",
    "answer": "Close the S3 client to release resources."
  },
  {
    "question": "How can you list buckets with the AWS SDK for Go?",
    "answer": "Load the config, create a client, then call ListBuckets with the desired MaxBuckets value and process the returned buckets."
  },
  {
    "question": "What happens when you empty a general purpose S3 bucket?",
    "answer": "All objects are deleted, but the bucket remains. The operation cannot be undone, and objects added during the operation might be deleted."
  },
  {
    "question": "If S3 versioning is enabled or suspended and you empty a bucket, what happens?",
    "answer": "All versions of all objects (including delete markers) are deleted."
  },
  {
    "question": "Why should you remove incomplete multipart uploads when emptying a bucket?",
    "answer": "Incomplete multipart uploads take up storage and can incur additional costs. They should be removed to free up space and minimize costs."
  },
  {
    "question": "How can you automate the removal of incomplete multipart uploads and old objects?",
    "answer": "By configuring a bucket lifecycle rule to expire objects and incomplete multipart uploads older than a specified number of days (recommended: 7 days)."
  },
  {
    "question": "What is a good starting point for expiring incomplete multipart uploads using S3 Lifecycle rules?",
    "answer": "Seven days."
  },
  {
    "question": "Describe the process of emptying an S3 bucket using the AWS Console.",
    "answer": "Select the bucket, click 'Empty', enter the bucket name to confirm, and monitor progress on the status page."
  },
  {
    "question": "Can you use the AWS CLI to empty a versioned bucket?",
    "answer": "No, you cannot use aws s3 rm --recursive on buckets where versioning is enabled. This command only adds a delete marker."
  },
  {
    "question": "What is the command to recursively remove all objects from a non-versioned bucket?",
    "answer": "aws s3 rm s3://bucket-name --recursive"
  },
  {
    "question": "Why can't you remove objects from a bucket that has versioning enabled using the CLI's rm command?",
    "answer": "Because the rm command only adds a delete marker in versioned buckets, not actually deleting all versions."
  },
  {
    "question": "How can you empty a large S3 bucket asynchronously?",
    "answer": "By setting up an S3 Lifecycle configuration rule that expires (deletes) all objects asynchronously over time."
  },
  {
    "question": "What components should a Lifecycle configuration include to fully empty a bucket?",
    "answer": "Current versions, non-current versions, delete markers, and incomplete multipart uploads."
  },
  {
    "question": "How do you prevent CloudTrail from creating new objects in a bucket you're trying to empty?",
    "answer": "Stop the CloudTrail trails or add a deny s3:PutObject policy, then remove it when you need to store new objects again."
  },
  {
    "question": "What is an alternative to stopping CloudTrail logging when emptying a bucket?",
    "answer": "Add a deny s3:PutObject statement to the bucket policy."
  },
  {
    "question": "What must you do before deleting a general purpose S3 bucket?",
    "answer": "Ensure the bucket is empty and delete any same-account access points attached to the bucket."
  },
  {
    "question": "Why should you consider not deleting the bucket even after emptying it?",
    "answer": "Because bucket names are globally unique. If deleted, anyone can reuse that name and potentially receive requests intended for the old bucket."
  },
  {
    "question": "What should you do with an S3 bucket if it’s no longer in use but you want to avoid costs and keep the name?",
    "answer": "Empty the bucket and keep it, possibly blocking all requests, instead of deleting it."
  },
  {
    "question": "What happens technically when you delete an S3 bucket?",
    "answer": "The bucket is queued for deletion. Deletion may take time to propagate due to S3’s distributed regional architecture."
  },
  {
    "question": "What extra steps must be taken if your S3 bucket hosts a static website before deleting it?",
    "answer": "Clean up related Amazon Route 53 hosted zone settings."
  },
  {
    "question": "If your bucket receives load balancer logs, what must you do before deletion?",
    "answer": "Stop ELB logging to the bucket to avoid logs being delivered to a bucket reused by another account."
  },
  {
    "question": "What should you check if you’re unable to delete a bucket?",
    "answer": "Ensure the bucket is empty and that no access points are attached."
  },
  {
    "question": "What permission is required to delete an S3 bucket?",
    "answer": "You need the s3:DeleteBucket permission to delete an S3 bucket."
  },
  {
    "question": "If you can't delete a bucket, what IAM and policy checks should you perform?",
    "answer": "Check that your IAM user or role has s3:DeleteBucket permission, and verify there are no Deny statements for s3:DeleteBucket in AWS Organizations service control policies, resource control policies, or the bucket policy."
  },
  {
    "question": "What is a common default policy on buckets created by AWS Elastic Beanstalk that can prevent deletion?",
    "answer": "Buckets created by AWS Elastic Beanstalk have a policy that contains a Deny statement for s3:DeleteBucket by default."
  },
  {
    "question": "What must you do before deleting a general purpose S3 bucket?",
    "answer": "You must empty the bucket before you can delete it."
  },
  {
    "question": "How do you delete a bucket using the AWS S3 console?",
    "answer": "1. Sign in to the AWS Management Console and open the S3 console. 2. In the left navigation pane, choose General purpose buckets. 3. Select the bucket's option button and choose Delete. 4. Confirm deletion by typing the bucket's name, then choose Delete bucket."
  },
  {
    "question": "What should you do if you try to delete a non-empty bucket in the console?",
    "answer": "Use the Empty bucket button in the alert, follow instructions to empty the bucket, and then return to delete the bucket."
  },
  {
    "question": "How do you verify if an S3 bucket has been successfully deleted?",
    "answer": "Check the General purpose buckets list for the bucket's name. If it can't be found, deletion was successful."
  },
  {
    "question": "Describe the Java process for deleting a general purpose S3 bucket using the AWS SDK.",
    "answer": "First delete all objects in the bucket (and, if versioning is enabled, all object versions), then call deleteBucket with the bucket name."
  },
  {
    "question": "Why must all object versions be deleted before deleting a versioned S3 bucket?",
    "answer": "Amazon S3 requires you to delete all object versions before you can delete a versioned bucket, otherwise delete markers are left and deletion fails."
  },
  {
    "question": "What are the main Exceptions to handle when deleting buckets using the AWS SDK for Java?",
    "answer": "You must handle AmazonServiceException and SdkClientException."
  },
  {
    "question": "How can you delete a non-versioned S3 bucket containing objects with the AWS CLI?",
    "answer": "Use aws s3 rb s3://bucket-name --force to delete all objects and the bucket."
  },
  {
    "question": "What happens if you use aws s3 rb --force on a bucket that has versioning enabled?",
    "answer": "The command does not delete versioned objects, so the bucket deletion fails because the bucket isn’t empty."
  },
  {
    "question": "What should you do if your versioned bucket deletion fails with the CLI?",
    "answer": "Delete object versions using additional CLI commands or SDKs, then try to delete the bucket again."
  },
  {
    "question": "Which AWS CLI section contains further information on S3 high-level commands?",
    "answer": "The section 'Using High-Level S3 Commands with the AWS Command Line Interface' in the AWS CLI User Guide."
  },
  {
    "question": "What is Mountpoint for Amazon S3?",
    "answer": "It is a high-throughput open-source file client for mounting an Amazon S3 bucket as a local file system, translating file system operations into S3 object API calls."
  },
  {
    "question": "Which common applications can benefit from Mountpoint for Amazon S3?",
    "answer": "Data lakes, machine learning training, image rendering, autonomous vehicle simulation, and ETL workloads benefit from Mountpoint."
  },
  {
    "question": "What file operations does Mountpoint support and not support?",
    "answer": "Mountpoint supports listing, reading, and creating files up to 5 TB. It does not support modifying existing files, deleting directories, symbolic links, or file locking."
  },
  {
    "question": "On which operating systems is Mountpoint for Amazon S3 available?",
    "answer": "Mountpoint for Amazon S3 is available only for Linux operating systems."
  },
  {
    "question": "Which S3 storage classes cannot be accessed via Mountpoint?",
    "answer": "Mountpoint does not support S3 Glacier Flexible Retrieval, S3 Glacier Deep Archive, S3 Intelligent-Tiering Archive Access Tier, and S3 Intelligent-Tiering Deep Archive Access Tier."
  },
  {
    "question": "What are the four main steps to install Mountpoint for Amazon S3 on an RPM-based Linux distribution?",
    "answer": "1. Copy the download URL for your architecture. 2. Download the Mountpoint package. 3. (Optional) Verify authenticity with the signature. 4. Install the package using 'sudo yum install ./mount-s3.rpm'."
  },
  {
    "question": "How do you verify Mountpoint is installed on your system?",
    "answer": "Run 'mount-s3 --version' and look for output such as 'mount-s3 1.3.1'."
  },
  {
    "question": "Describe the process for installing Mountpoint for Amazon S3 on DEB-based distributions.",
    "answer": "1. Copy the download URL for your architecture. 2. Download the package using wget. 3. (Optional) Verify with the appropriate signature. 4. Install with 'sudo apt-get install ./mount-s3.deb'. 5. Verify installation with 'mount-s3 --version'."
  },
  {
    "question": "What additional packages are needed for Mountpoint on 'other' Linux distributions?",
    "answer": "FUSE and libfuse2 must first be installed."
  },
  {
    "question": "How do you add Mountpoint to your PATH after extracting on a non-RPM/DEB Linux system?",
    "answer": "Append 'export PATH=$PATH:/opt/aws/mountpoint-s3/bin' to your $HOME/.profile and run 'source $HOME/.profile'."
  },
  {
    "question": "What tool is required to verify the authenticity and integrity of a downloaded Mountpoint package?",
    "answer": "GnuPG (gpg) tool is required."
  },
  {
    "question": "How do you import the Mountpoint public key for signature verification?",
    "answer": "Download the KEYS public key file, then run 'gpg --import KEYS'."
  },
  {
    "question": "What is the required fingerprint for the Mountpoint public key?",
    "answer": "673F E406 1506 BB46 9A0E F857 BE39 7A52 B086 DA5A."
  },
  {
    "question": "What should you do if the GPG signature for a Mountpoint package does not match?",
    "answer": "Do not finish installing Mountpoint and contact AWS Support if the fingerprint string does not match."
  },
  {
    "question": "How do you verify the integrity of a Mountpoint package on an RPM-based Linux distribution?",
    "answer": "Use the command 'gpg --verify mount-s3.rpm.asc' after downloading the package signature file."
  },
  {
    "question": "What does the 'Good signature' phrase indicate in the GPG verification output?",
    "answer": "It indicates that the signature of the downloaded Mountpoint package is valid and has not been tampered with."
  },
  {
    "question": "What credentials are required to use Mountpoint for Amazon S3?",
    "answer": "Your host needs valid AWS credentials with access to the targeted Amazon S3 bucket(s)."
  },
  {
    "question": "How can you manually mount an Amazon S3 bucket using Mountpoint?",
    "answer": "Use the 'mount-s3' command with your bucket name and the target directory, e.g., 'mount-s3 amzn-s3-demo-bucket ~/mnt'."
  },
  {
    "question": "How do you configure an Amazon S3 bucket to mount automatically on EC2 start or reboot?",
    "answer": "Edit the '/etc/fstab' file on your Linux system and add an appropriate line for your S3 bucket and mountpoint using the 'mount-s3' filesystem type and desired options."
  },
  {
    "question": "What does the mount option '_netdev' mean when mounting an S3 bucket?",
    "answer": "It specifies that the filesystem requires networking to mount."
  },
  {
    "question": "How does Mountpoint interpret S3 object keys as file system paths?",
    "answer": "It splits S3 object keys on the forward slash '/' character, mapping keys to file system directories and files accordingly."
  },
  {
    "question": "What are limitations of Mountpoint for Amazon S3 regarding full POSIX compliance?",
    "answer": "Mountpoint does not implement the full POSIX standard and is optimized for high-throughput read/write workloads but not for features requiring full POSIX file semantics."
  },
  {
    "question": "How do you locally unmount an S3 bucket using Mountpoint?",
    "answer": "Use the 'umount' command with the mount directory, for example, 'umount ~/mnt'."
  },
  {
    "question": "List the three types of caching supported by Mountpoint.",
    "answer": "Local cache, shared cache, and combined local and shared cache."
  },
  {
    "question": "When should you use a local cache with Mountpoint?",
    "answer": "If you repeatedly read the same data from the same compute instance and have unused local storage for the dataset."
  },
  {
    "question": "What is an S3 Express One Zone shared cache used for in Mountpoint?",
    "answer": "To accelerate repeated reads of small objects from multiple compute instances and leverage cache elasticity."
  },
  {
    "question": "What are the implications of enabling local caching with Mountpoint?",
    "answer": "Mountpoint will persist unencrypted S3 object content at the local cache location, so file system access control should be used to protect data."
  },
  {
    "question": "Which flag enables a local cache in Mountpoint and how do you use it?",
    "answer": "The '--cache CACHE_PATH' flag enables a local cache. For example: 'mount-s3 --cache /tmp/cache amzn-s3-demo-bucket ~/mnt'."
  },
  {
    "question": "What should you do to control storage costs when using a shared cache in S3 Express One Zone?",
    "answer": "Set up a Lifecycle policy on your directory bucket so that Amazon S3 expires cached data after a specified period."
  },
  {
    "question": "What command line option enables S3 Express One Zone shared caching with Mountpoint?",
    "answer": "Use '--cache-xz' followed by the directory bucket as your cache location."
  },
  {
    "question": "What security best practice is recommended when enabling shared cache with Mountpoint?",
    "answer": "Use a dedicated directory bucket for Mountpoint shared caching, grant access only to Mountpoint clients, and ensure the bucket uses correct policies and is not publicly accessible."
  },
  {
    "question": "How does Mountpoint log information by default?",
    "answer": "Mountpoint emits high-severity log information to syslog."
  },
  {
    "question": "How can you view Mountpoint logs on Amazon Linux?",
    "answer": "Use 'journalctl -e SYSLOG_IDENTIFIER=mount-s3' command."
  },
  {
    "question": "Where are Mountpoint syslog entries typically written on other Linux systems?",
    "answer": "They are likely written to a file such as '/var/log/syslog'."
  },
  {
    "question": "Where can you report a security issue found in Mountpoint?",
    "answer": "Notify AWS Security through the vulnerability reporting page. Do not create a public GitHub issue."
  },
  {
    "question": "What is Storage Browser for Amazon S3?",
    "answer": "It is an open source component that provides a simple graphical interface for accessing, browsing, downloading, uploading, copying, and deleting data in S3 from your web applications."
  },
  {
    "question": "Which file operations are supported by Storage Browser for S3?",
    "answer": "LIST, GET, PUT, COPY, UPLOAD, and DELETE."
  },
  {
    "question": "What AWS frameworks and environments support Storage Browser for S3?",
    "answer": "It is available for web and intranet applications on the React framework, using the AWS Amplify React library."
  },
  {
    "question": "Which S3 storage classes are NOT accessible using Storage Browser for S3?",
    "answer": "S3 Glacier Flexible Retrieval, S3 Glacier Deep Archive, S3 Intelligent-Tiering Archive Access tier, and S3 Intelligent-Tiering Deep Archive Access tier."
  },
  {
    "question": "List the four main views in the Storage Browser for S3 user interface.",
    "answer": "Home page, location details, location action, and vertical ellipsis (action drop-down)."
  },
  {
    "question": "What limitations should you be aware of when using Storage Browser for S3?",
    "answer": "Folders starting or ending with dots are not supported, WRITE only S3 Access Grants are not supported, PUT is supported up to 160 GB, and COPY only for files smaller than 5 GB."
  },
  {
    "question": "How can you quickly get started with Storage Browser for S3?",
    "answer": "Clone a sample project from GitHub or install the aws-amplify/ui-react-storage and aws-amplify packages from the aws-amplify GitHub repository, and follow setup instructions."
  },
  {
    "question": "What is the NPM command to install Storage Browser for S3?",
    "answer": "npm i --save @aws-amplify/ui-react-storage aws-amplify"
  },
  {
    "question": "Which authentication and authorization methods can be set up for Storage Browser?",
    "answer": "You can use AWS Amplify Auth for end users and third parties, manage IAM principals, or manage data access at scale."
  },
  {
    "question": "How does AWS Amplify Auth manage access control for Storage Browser for S3?",
    "answer": "AWS Amplify Auth, based on Amazon Cognito, can authenticate users from user or enterprise directories or consumer identity providers, issuing IAM credentials and authorizing access using the Amplify Storage model."
  },
  {
    "question": "How do you initialize the Storage Browser component with Amplify authentication and storage methods?",
    "answer": "Import createAmplifyAuthAdapter and createStorageBrowser from '@aws-amplify/ui-react-storage/browser', configure Amplify with your config file, and use createStorageBrowser with createAmplifyAuthAdapter as the config."
  },
  {
    "question": "How can you manage S3 data access using IAM principals?",
    "answer": "Create an IAM role with s3:GetDataAccess permission, set up an S3 Access Grants instance, and use the Storage Browser component with managed auth, providing IAM credentials upon acquisition."
  },
  {
    "question": "Which API operation does the Storage Browser component invoke to fetch available grants for an IAM identity?",
    "answer": "ListCallerAccessGrants S3 API operation."
  },
  {
    "question": "What permissions must be assigned to IAM principals to enable Storage Browser access to S3 data?",
    "answer": "The IAM principal must have the s3:GetDataAccess permission."
  },
  {
    "question": "Describe the use of a 'credentialsProvider' in the Storage Browser's managed authentication setup.",
    "answer": "The 'credentialsProvider' is an async function returning an object that includes accessKeyId, secretAccessKey, sessionToken, and expiration details for temporary credentials."
  },
  {
    "question": "What is the recommended approach for managing S3 data access at scale for large organizations?",
    "answer": "Associate an S3 Access Grants instance with your IAM Identity Center, using trusted identity propagation so you can centrally manage group-based access, including integration with external IdPs like Entra or Okta."
  },
  {
    "question": "What is 'trusted identity propagation' in the context of AWS IAM Identity Center?",
    "answer": "It is a feature that lets customer managed applications request access on behalf of authenticated users, passing the user's identity in requests to AWS services."
  },
  {
    "question": "What benefits does trusted identity propagation offer for organizations with large user bases?",
    "answer": "It allows use of existing user directories, simplifies permission management, connects S3 data access directly to end-user identities, and enables audit trails referencing the actual user."
  },
  {
    "question": "Outline the main workflow steps to set up Storage Browser authentication with S3 Access Grants and IAM Identity Center.",
    "answer": "1. Enable IAM Identity Center for your AWS Organizations. 2. Configure AWS IAM Identity Center federation. 3. Add a trusted token issuer. 4. Create IAM roles for bootstrap application and identity bearer. 5. Create and configure your application. 6. Add S3 Access Grants as a trusted application. 7. Create grants for users or groups. 8. Create your Storage Browser for S3 component."
  },
  {
    "question": "What is the function of the System for Cross-domain Identity Management (SCIM) in this setup?",
    "answer": "SCIM keeps IAM Identity Center's identities in sync with those from your identity provider, enabling proper grant assignments."
  },
  {
    "question": "How do you enable IAM Identity Center for your AWS Organizations?",
    "answer": "Sign in to the AWS Management Console, open the IAM Identity Center console, choose Enable, select Enable with AWS Organizations, and follow the setup wizard."
  },
  {
    "question": "Why is it important to verify your management account email address after IAM Identity Center setup?",
    "answer": "Because AWS Organizations sends a verification email to this address, and the address must be verified within 24 hours to complete the setup."
  },
  {
    "question": "What is the benefit of configuring delegated administration in IAM Identity Center?",
    "answer": "It limits the number of users who need access to the root management account in multi-account environments, enhancing security."
  },
  {
    "question": "When configuring IAM Identity Center federation, what information do you need from your identity provider (IdP)?",
    "answer": "You need the issuer URL and audience attributes from the external IdP."
  },
  {
    "question": "Briefly explain the process to add a trusted token issuer in AWS IAM Identity Center.",
    "answer": "In the IAM Identity Center console, under Settings and Authentication, enter the issuer URL and a display name for the token issuer, map attributes as required, and click Create trusted token issuer."
  },
  {
    "question": "Why might you need to contact an application administrator after adding a new trusted token issuer?",
    "answer": "So they can check the trusted token issuer's appearance in the application console and configure it for identity propagation."
  },
  {
    "question": "Which two IAM roles are typically created for Storage Browser with S3 Access Grants and what are their purposes?",
    "answer": "A 'bootstrap' role (enables token exchange with sso-oauth:CreateTokenWithIAM) and an 'identity bearer' role (grants S3 data access via s3:GetDataAccess and s3:ListCallerAccessGrants)."
  },
  {
    "question": "What permissions should the identity bearer IAM role have?",
    "answer": "It should allow s3:GetDataAccess and s3:ListCallerAccessGrants on the relevant S3 Access Grants resource."
  },
  {
    "question": "What is the purpose of a trust policy for the identity bearer role?",
    "answer": "To allow the bootstrap application's IAM role to assume the identity bearer role using sts:AssumeRole and sts:SetContext."
  },
  {
    "question": "Summarize the steps to create and configure a managed application in AWS IAM Identity Center for Storage Browser.",
    "answer": "1. Open IAM Identity Center console. 2. Add an application with OAuth 2.0 type. 3. Enter display name and description. 4. Set assignment method to not require assignments. 5. Specify application URL. 6. Select trusted token issuer and configure its audience claim. 7. Enter the IAM role ARN for identity bearer. 8. Submit and save."
  },
  {
    "question": "What value is typically set for the 'aud' claim when configuring a trusted token issuer in IAM Identity Center?",
    "answer": "The 'aud' claim should be the application name or identifier as recognized by the identity provider issuing the JWT."
  },
  {
    "question": "What do you do after creating a customer managed app to enable S3 Access Grants for identity propagation?",
    "answer": "Specify S3 Access Grants as a trusted application for identity propagation in your application configuration."
  },
  {
    "question": "What permission must be granted to users to enable them to retrieve credentials for accessing Amazon S3 through S3 Access Grants?",
    "answer": "The s3:access_grants:read_write permission."
  },
  {
    "question": "How do you configure access levels for S3 Access Grants when adding trusted applications?",
    "answer": "You can select access per application (and set levels for each), or apply the same level of access to all applications during the configuration steps in the console."
  },
  {
    "question": "What is the main purpose of using SCIM with IAM Identity Center when integrating with S3 Access Grants?",
    "answer": "SCIM ensures that IAM Identity Center identities stay in sync with identities from your external identity provider, allowing provisioning, updates, and deprovisioning of users automatically."
  },
  {
    "question": "Why are local IAM Identity Center users not used when S3 Access Grants is used with IAM Identity Center?",
    "answer": "Because user synchronization from the external identity provider (IdP) is required for S3 Access Grants to work, meaning only users managed by the IdP and synced via SCIM are valid."
  },
  {
    "question": "List the steps to synchronize users from an identity provider with IAM Identity Center.",
    "answer": "1. Enable automatic provisioning. 2. Generate an access token."
  },
  {
    "question": "How do you set up the Storage Browser for S3 component in a React application?",
    "answer": "Use the 'createManagedAuthAdapter' for configuring authorization and 'createStorageBrowser' for initializing the Storage Browser component in your React application."
  },
  {
    "question": "What must your Storage Browser credentials provider return?",
    "answer": "It must return an object containing accessKeyId, secretAccessKey, sessionToken, and expiration fields for AWS credentials."
  },
  {
    "question": "What tasks must a mechanism for exchanging JWTs with IAM credentials be able to handle for a web application?",
    "answer": "It must exchange JSON web tokens (JWT) from the web app for IAM credentials using the IAM Identity Center."
  },
  {
    "question": "What are the steps to validate and exchange an IdP's JWT for AWS IAM credentials?",
    "answer": "1. Retrieve the JWT from the request. 2. Validate the JWT using the JWKS URL. 3. Verify claims like expiration, issuer, subject, and audience. 4. Verify required permission and scope. 5. Call the CreateTokenWithIAM API. 6. Use AssumeRole. 7. Return IAM credentials to the app."
  },
  {
    "question": "According to best practices, what should you avoid when handling JWTs for AWS credential exchange?",
    "answer": "Do not log sensitive information and handle errors properly for missing authorization and expired tokens."
  },
  {
    "question": "What operation allows the IAM Identity Center token to be used for getting temporary AWS credentials?",
    "answer": "The AssumeRole API operation."
  },
  {
    "question": "Why must you use a new IdP JSON web token for each request to IAM Identity Center?",
    "answer": "Because a used IdP JSON web token cannot be used again; a new token is needed for every exchange."
  },
  {
    "question": "What should you ensure about API response logging and formatting when building a JWT-to-AWS-credentials endpoint?",
    "answer": "Set up proper logging and ensure responses are in standardized JSON format with correct HTTP status codes."
  },
  {
    "question": "Why is CORS configuration important for S3 buckets accessed via Storage Browser for S3?",
    "answer": "Because REST API calls from the Storage Browser won't work unless CORS is enabled for the S3 buckets involved."
  },
  {
    "question": "Give an example of the CORS policy required for Storage Browser for S3.",
    "answer": "A CORS policy allowing headers '*', methods [GET, HEAD, PUT, POST, DELETE], origins '*', and certain exposed headers (like etag, x-amz-version-id) with MaxAgeSeconds set."
  },
  {
    "question": "What are common troubleshooting tips for the Storage Browser for S3 component?",
    "answer": "1. Do not reuse tokens in multiple requests. 2. Make sure provided IAM credentials include s3:GetDataAccess permission."
  },
  {
    "question": "What resources can you consult if you have issues with Storage Browser for S3?",
    "answer": "AWS Support Center, Amplify GitHub page, and AWS Security Vulnerability Reporting page."
  },
  {
    "question": "What is Amazon S3 Transfer Acceleration and what does it do?",
    "answer": "It is a bucket-level feature that speeds up file transfers over long distances by routing data through Amazon CloudFront edge locations into S3."
  },
  {
    "question": "In what circumstances should you use S3 Transfer Acceleration?",
    "answer": "If you need fast uploads from users worldwide, transfer large amounts of data across continents, or can't use all available bandwidth when uploading to S3."
  },
  {
    "question": "What are the requirements to use Amazon S3 Transfer Acceleration on a bucket?",
    "answer": "The bucket must support virtual-hosted style requests, have a DNS-compliant name without periods (.), and have Transfer Acceleration enabled."
  },
  {
    "question": "Why is the bucket name format important for S3 Transfer Acceleration?",
    "answer": "Because the bucket name must be DNS-compliant, and Transfer Acceleration does not support buckets with periods in the name."
  },
  {
    "question": "List at least five regions where S3 Transfer Acceleration is supported.",
    "answer": "Examples: ap-northeast-1 (Tokyo), ap-south-1 (Mumbai), eu-west-1 (Ireland), us-east-1 (N. Virginia), sa-east-1 (São Paulo)."
  },
  {
    "question": "After enabling S3 Transfer Acceleration on a bucket, how soon do you see performance improvements?",
    "answer": "It may take up to 20 minutes before performance increases are realized."
  },
  {
    "question": "What endpoints must you use for data transfer with an acceleration-enabled S3 bucket?",
    "answer": "Use bucket-name.s3-accelerate.amazonaws.com or bucket-name.s3-accelerate.dualstack.amazonaws.com for IPv6."
  },
  {
    "question": "Who can change the transfer acceleration state of an S3 bucket?",
    "answer": "The bucket owner, or other users granted s3:PutAccelerateConfiguration or s3:GetAccelerateConfiguration permissions."
  },
  {
    "question": "How do you enable S3 Transfer Acceleration on a bucket with the AWS console?",
    "answer": "Open S3 console, go to the bucket's Properties > Transfer acceleration, choose Edit, and set to Enable."
  },
  {
    "question": "What AWS CLI command can enable S3 Transfer Acceleration?",
    "answer": "aws s3api put-bucket-accelerate-configuration --bucket BUCKET --accelerate-configuration Status=Enabled"
  },
  {
    "question": "How can you compare upload speeds with and without Transfer Acceleration?",
    "answer": "Use the Amazon S3 Transfer Acceleration Speed Comparison tool in the AWS console."
  },
  {
    "question": "What happens if you suspend transfer acceleration for your bucket?",
    "answer": "The accelerate endpoint will no longer work for that bucket."
  },
  {
    "question": "Which S3 operations cannot be performed using a transfer acceleration endpoint?",
    "answer": "ListBuckets, CreateBucket, DeleteBucket, and cross-Region CopyObject operations."
  },
  {
    "question": "What is the minimum required by your data transfer application to benefit from Transfer Acceleration?",
    "answer": "You must use the .s3-accelerate.amazonaws.com or .s3-accelerate.dualstack.amazonaws.com endpoint."
  },
  {
    "question": "What should you do to access an acceleration-enabled bucket over IPv6?",
    "answer": "Use the dual-stack endpoint bucket-name.s3-accelerate.dualstack.amazonaws.com."
  }
]